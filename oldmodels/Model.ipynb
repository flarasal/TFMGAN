{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946ab409-0c2b-442c-8497-1d847be17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c3e50c-ac00-4334-9f09-484fdf78991b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster.id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjects</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject496</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject498</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject499</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject500</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cluster.id\n",
       "subjects              \n",
       "subject1             1\n",
       "subject2             2\n",
       "subject3             1\n",
       "subject4             1\n",
       "subject5             1\n",
       "...                ...\n",
       "subject496           1\n",
       "subject497           1\n",
       "subject498           1\n",
       "subject499           2\n",
       "subject500           2\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methyldata = pd.read_csv(\"methyl.csv\",index_col=0)\n",
    "exprdata = pd.read_csv(\"expr.csv\",index_col=0)\n",
    "assigndata = pd.read_csv(\"assign.csv\",usecols=[1,2],index_col=0)\n",
    "assigndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d14c143d-8fb3-4bd7-8436-0015a40f7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACACA</th>\n",
       "      <th>ACVRL1</th>\n",
       "      <th>AKT1</th>\n",
       "      <th>AKT1S1</th>\n",
       "      <th>ANXA1</th>\n",
       "      <th>AR</th>\n",
       "      <th>ARAF</th>\n",
       "      <th>ASNS</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAD</th>\n",
       "      <th>...</th>\n",
       "      <th>VHL</th>\n",
       "      <th>WWTR1</th>\n",
       "      <th>XBP1</th>\n",
       "      <th>XRCC1</th>\n",
       "      <th>XRCC5</th>\n",
       "      <th>YAP1</th>\n",
       "      <th>YBX1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject1</th>\n",
       "      <td>-0.778773</td>\n",
       "      <td>6.661350</td>\n",
       "      <td>0.601915</td>\n",
       "      <td>-1.381072</td>\n",
       "      <td>0.276366</td>\n",
       "      <td>6.826198</td>\n",
       "      <td>5.217787</td>\n",
       "      <td>-0.531270</td>\n",
       "      <td>-0.685574</td>\n",
       "      <td>4.965979</td>\n",
       "      <td>...</td>\n",
       "      <td>4.997389</td>\n",
       "      <td>5.227471</td>\n",
       "      <td>-1.173942</td>\n",
       "      <td>4.990185</td>\n",
       "      <td>4.739730</td>\n",
       "      <td>0.752404</td>\n",
       "      <td>4.925204</td>\n",
       "      <td>-0.332289</td>\n",
       "      <td>4.740385</td>\n",
       "      <td>1.588527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2</th>\n",
       "      <td>-1.202237</td>\n",
       "      <td>6.599935</td>\n",
       "      <td>-1.059473</td>\n",
       "      <td>-1.343091</td>\n",
       "      <td>0.567053</td>\n",
       "      <td>2.980298</td>\n",
       "      <td>-1.060058</td>\n",
       "      <td>-0.543807</td>\n",
       "      <td>4.112408</td>\n",
       "      <td>-0.377627</td>\n",
       "      <td>...</td>\n",
       "      <td>5.324622</td>\n",
       "      <td>-0.575675</td>\n",
       "      <td>-0.521891</td>\n",
       "      <td>1.270019</td>\n",
       "      <td>5.396299</td>\n",
       "      <td>1.926090</td>\n",
       "      <td>0.432328</td>\n",
       "      <td>0.453741</td>\n",
       "      <td>0.124431</td>\n",
       "      <td>1.516044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3</th>\n",
       "      <td>-0.351933</td>\n",
       "      <td>7.037106</td>\n",
       "      <td>-0.169862</td>\n",
       "      <td>-0.600338</td>\n",
       "      <td>0.467504</td>\n",
       "      <td>4.409149</td>\n",
       "      <td>5.523396</td>\n",
       "      <td>-0.820251</td>\n",
       "      <td>-0.896702</td>\n",
       "      <td>6.273091</td>\n",
       "      <td>...</td>\n",
       "      <td>6.548016</td>\n",
       "      <td>6.983861</td>\n",
       "      <td>1.174447</td>\n",
       "      <td>5.292834</td>\n",
       "      <td>5.405190</td>\n",
       "      <td>1.720668</td>\n",
       "      <td>5.243290</td>\n",
       "      <td>0.383033</td>\n",
       "      <td>7.387739</td>\n",
       "      <td>1.114908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4</th>\n",
       "      <td>-0.560752</td>\n",
       "      <td>6.302922</td>\n",
       "      <td>-1.095674</td>\n",
       "      <td>-0.335570</td>\n",
       "      <td>-1.799302</td>\n",
       "      <td>4.858635</td>\n",
       "      <td>4.789491</td>\n",
       "      <td>-0.335323</td>\n",
       "      <td>-1.509982</td>\n",
       "      <td>5.800878</td>\n",
       "      <td>...</td>\n",
       "      <td>4.780794</td>\n",
       "      <td>6.124618</td>\n",
       "      <td>-1.631147</td>\n",
       "      <td>5.213470</td>\n",
       "      <td>5.813819</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>4.457551</td>\n",
       "      <td>-0.836184</td>\n",
       "      <td>5.890996</td>\n",
       "      <td>-0.108535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5</th>\n",
       "      <td>-0.260576</td>\n",
       "      <td>5.974995</td>\n",
       "      <td>-1.597704</td>\n",
       "      <td>-1.239989</td>\n",
       "      <td>-0.848312</td>\n",
       "      <td>6.202194</td>\n",
       "      <td>3.625550</td>\n",
       "      <td>-0.896659</td>\n",
       "      <td>-0.772299</td>\n",
       "      <td>5.427887</td>\n",
       "      <td>...</td>\n",
       "      <td>5.509940</td>\n",
       "      <td>5.235778</td>\n",
       "      <td>-1.466720</td>\n",
       "      <td>5.391635</td>\n",
       "      <td>5.309465</td>\n",
       "      <td>1.113457</td>\n",
       "      <td>5.062244</td>\n",
       "      <td>-1.151481</td>\n",
       "      <td>5.118148</td>\n",
       "      <td>-0.504209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject496</th>\n",
       "      <td>-0.933188</td>\n",
       "      <td>6.758515</td>\n",
       "      <td>0.141599</td>\n",
       "      <td>1.175414</td>\n",
       "      <td>-0.199922</td>\n",
       "      <td>5.404964</td>\n",
       "      <td>4.587526</td>\n",
       "      <td>-2.250117</td>\n",
       "      <td>-0.043880</td>\n",
       "      <td>4.791108</td>\n",
       "      <td>...</td>\n",
       "      <td>4.960378</td>\n",
       "      <td>4.375743</td>\n",
       "      <td>-0.822462</td>\n",
       "      <td>6.340625</td>\n",
       "      <td>5.713636</td>\n",
       "      <td>1.483030</td>\n",
       "      <td>5.739724</td>\n",
       "      <td>0.294931</td>\n",
       "      <td>5.237734</td>\n",
       "      <td>1.493332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject497</th>\n",
       "      <td>-0.351743</td>\n",
       "      <td>6.307647</td>\n",
       "      <td>-0.395698</td>\n",
       "      <td>-0.410632</td>\n",
       "      <td>-0.335842</td>\n",
       "      <td>5.095289</td>\n",
       "      <td>5.294062</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>-0.599051</td>\n",
       "      <td>4.903005</td>\n",
       "      <td>...</td>\n",
       "      <td>6.760614</td>\n",
       "      <td>6.283541</td>\n",
       "      <td>0.793421</td>\n",
       "      <td>5.089189</td>\n",
       "      <td>4.882482</td>\n",
       "      <td>0.584146</td>\n",
       "      <td>5.546557</td>\n",
       "      <td>-0.167474</td>\n",
       "      <td>5.152645</td>\n",
       "      <td>1.066775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject498</th>\n",
       "      <td>-0.555563</td>\n",
       "      <td>6.201945</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>-0.527820</td>\n",
       "      <td>1.141942</td>\n",
       "      <td>4.788988</td>\n",
       "      <td>4.594360</td>\n",
       "      <td>-1.039993</td>\n",
       "      <td>-0.339906</td>\n",
       "      <td>6.553227</td>\n",
       "      <td>...</td>\n",
       "      <td>4.372792</td>\n",
       "      <td>5.008954</td>\n",
       "      <td>-0.950863</td>\n",
       "      <td>4.932677</td>\n",
       "      <td>5.589072</td>\n",
       "      <td>1.423199</td>\n",
       "      <td>4.820329</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>6.264235</td>\n",
       "      <td>0.935781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject499</th>\n",
       "      <td>-0.496976</td>\n",
       "      <td>6.736355</td>\n",
       "      <td>0.309428</td>\n",
       "      <td>1.002468</td>\n",
       "      <td>-1.421916</td>\n",
       "      <td>0.913188</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>-2.264783</td>\n",
       "      <td>4.140739</td>\n",
       "      <td>1.007924</td>\n",
       "      <td>...</td>\n",
       "      <td>4.496152</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>-0.513729</td>\n",
       "      <td>1.362927</td>\n",
       "      <td>5.841151</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>-0.240316</td>\n",
       "      <td>0.078770</td>\n",
       "      <td>0.380601</td>\n",
       "      <td>1.431541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject500</th>\n",
       "      <td>-1.368186</td>\n",
       "      <td>6.686480</td>\n",
       "      <td>-1.006249</td>\n",
       "      <td>-1.156804</td>\n",
       "      <td>0.353833</td>\n",
       "      <td>0.752743</td>\n",
       "      <td>0.222256</td>\n",
       "      <td>-0.858002</td>\n",
       "      <td>4.597509</td>\n",
       "      <td>0.781072</td>\n",
       "      <td>...</td>\n",
       "      <td>5.391630</td>\n",
       "      <td>0.374457</td>\n",
       "      <td>1.049327</td>\n",
       "      <td>-0.022351</td>\n",
       "      <td>5.551886</td>\n",
       "      <td>1.589490</td>\n",
       "      <td>1.385238</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.945422</td>\n",
       "      <td>0.832131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ACACA    ACVRL1      AKT1    AKT1S1     ANXA1        AR  \\\n",
       "subject1   -0.778773  6.661350  0.601915 -1.381072  0.276366  6.826198   \n",
       "subject2   -1.202237  6.599935 -1.059473 -1.343091  0.567053  2.980298   \n",
       "subject3   -0.351933  7.037106 -0.169862 -0.600338  0.467504  4.409149   \n",
       "subject4   -0.560752  6.302922 -1.095674 -0.335570 -1.799302  4.858635   \n",
       "subject5   -0.260576  5.974995 -1.597704 -1.239989 -0.848312  6.202194   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "subject496 -0.933188  6.758515  0.141599  1.175414 -0.199922  5.404964   \n",
       "subject497 -0.351743  6.307647 -0.395698 -0.410632 -0.335842  5.095289   \n",
       "subject498 -0.555563  6.201945  0.369510 -0.527820  1.141942  4.788988   \n",
       "subject499 -0.496976  6.736355  0.309428  1.002468 -1.421916  0.913188   \n",
       "subject500 -1.368186  6.686480 -1.006249 -1.156804  0.353833  0.752743   \n",
       "\n",
       "                ARAF      ASNS       ATM       BAD  ...       VHL     WWTR1  \\\n",
       "subject1    5.217787 -0.531270 -0.685574  4.965979  ...  4.997389  5.227471   \n",
       "subject2   -1.060058 -0.543807  4.112408 -0.377627  ...  5.324622 -0.575675   \n",
       "subject3    5.523396 -0.820251 -0.896702  6.273091  ...  6.548016  6.983861   \n",
       "subject4    4.789491 -0.335323 -1.509982  5.800878  ...  4.780794  6.124618   \n",
       "subject5    3.625550 -0.896659 -0.772299  5.427887  ...  5.509940  5.235778   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "subject496  4.587526 -2.250117 -0.043880  4.791108  ...  4.960378  4.375743   \n",
       "subject497  5.294062  0.064668 -0.599051  4.903005  ...  6.760614  6.283541   \n",
       "subject498  4.594360 -1.039993 -0.339906  6.553227  ...  4.372792  5.008954   \n",
       "subject499  0.544914 -2.264783  4.140739  1.007924  ...  4.496152  0.002271   \n",
       "subject500  0.222256 -0.858002  4.597509  0.781072  ...  5.391630  0.374457   \n",
       "\n",
       "                XBP1     XRCC1     XRCC5      YAP1      YBX1     YWHAB  \\\n",
       "subject1   -1.173942  4.990185  4.739730  0.752404  4.925204 -0.332289   \n",
       "subject2   -0.521891  1.270019  5.396299  1.926090  0.432328  0.453741   \n",
       "subject3    1.174447  5.292834  5.405190  1.720668  5.243290  0.383033   \n",
       "subject4   -1.631147  5.213470  5.813819  0.552651  4.457551 -0.836184   \n",
       "subject5   -1.466720  5.391635  5.309465  1.113457  5.062244 -1.151481   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "subject496 -0.822462  6.340625  5.713636  1.483030  5.739724  0.294931   \n",
       "subject497  0.793421  5.089189  4.882482  0.584146  5.546557 -0.167474   \n",
       "subject498 -0.950863  4.932677  5.589072  1.423199  4.820329  0.054766   \n",
       "subject499 -0.513729  1.362927  5.841151  0.990882 -0.240316  0.078770   \n",
       "subject500  1.049327 -0.022351  5.551886  1.589490  1.385238 -0.000684   \n",
       "\n",
       "               YWHAE     YWHAZ  \n",
       "subject1    4.740385  1.588527  \n",
       "subject2    0.124431  1.516044  \n",
       "subject3    7.387739  1.114908  \n",
       "subject4    5.890996 -0.108535  \n",
       "subject5    5.118148 -0.504209  \n",
       "...              ...       ...  \n",
       "subject496  5.237734  1.493332  \n",
       "subject497  5.152645  1.066775  \n",
       "subject498  6.264235  0.935781  \n",
       "subject499  0.380601  1.431541  \n",
       "subject500  0.945422  0.832131  \n",
       "\n",
       "[500 rows x 131 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exprdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000 | Disc Loss: 0.11042317748069763 | Gen Loss: 2.607304096221924\n",
      "Epoch 200/10000 | Disc Loss: 0.03652320057153702 | Gen Loss: 6.1525044441223145\n",
      "Epoch 300/10000 | Disc Loss: 0.003343258984386921 | Gen Loss: 7.043646812438965\n",
      "Epoch 400/10000 | Disc Loss: 0.014414506033062935 | Gen Loss: 8.356715202331543\n",
      "Epoch 500/10000 | Disc Loss: 0.002773153129965067 | Gen Loss: 10.663679122924805\n",
      "Epoch 600/10000 | Disc Loss: 0.004810021724551916 | Gen Loss: 9.026785850524902\n",
      "Epoch 700/10000 | Disc Loss: 0.000711931730620563 | Gen Loss: 9.217474937438965\n",
      "Epoch 800/10000 | Disc Loss: 0.001228748937137425 | Gen Loss: 10.089400291442871\n",
      "Epoch 900/10000 | Disc Loss: 0.009620037861168385 | Gen Loss: 12.503063201904297\n",
      "Epoch 1000/10000 | Disc Loss: 6.939322702237405e-06 | Gen Loss: 12.602293968200684\n",
      "Epoch 1100/10000 | Disc Loss: 6.691618182230741e-06 | Gen Loss: 12.196562767028809\n",
      "Epoch 1200/10000 | Disc Loss: 2.1861840195924742e-06 | Gen Loss: 13.224803924560547\n",
      "Epoch 1300/10000 | Disc Loss: 2.6304660423193127e-06 | Gen Loss: 13.269500732421875\n",
      "Epoch 1400/10000 | Disc Loss: 4.263182290742407e-06 | Gen Loss: 13.835149765014648\n",
      "Epoch 1500/10000 | Disc Loss: 6.1293685575947165e-06 | Gen Loss: 12.719283103942871\n",
      "Epoch 1600/10000 | Disc Loss: 1.5455469792868826e-06 | Gen Loss: 14.034231185913086\n",
      "Epoch 1700/10000 | Disc Loss: 9.522740583634004e-06 | Gen Loss: 12.652009963989258\n",
      "Epoch 1800/10000 | Disc Loss: 8.65623750742073e-21 | Gen Loss: 49.62908935546875\n",
      "Epoch 1900/10000 | Disc Loss: 1.5427361094988653e-20 | Gen Loss: 49.82163619995117\n",
      "Epoch 2000/10000 | Disc Loss: 1.252100627502747e-20 | Gen Loss: 50.59546661376953\n",
      "Epoch 2100/10000 | Disc Loss: 1.4506880325453326e-20 | Gen Loss: 51.45676803588867\n",
      "Epoch 2200/10000 | Disc Loss: 2.229229132610535e-20 | Gen Loss: 51.222999572753906\n",
      "Epoch 2300/10000 | Disc Loss: 1.5888204739992112e-19 | Gen Loss: 50.523868560791016\n",
      "Epoch 2400/10000 | Disc Loss: 7.900982834068602e-21 | Gen Loss: 50.68320083618164\n",
      "Epoch 2500/10000 | Disc Loss: 8.872305607012126e-30 | Gen Loss: 77.50302124023438\n",
      "Epoch 2600/10000 | Disc Loss: 1.1602809487090866e-28 | Gen Loss: 77.55632019042969\n",
      "Epoch 2700/10000 | Disc Loss: 1.663685392583194e-29 | Gen Loss: 78.30409240722656\n",
      "Epoch 2800/10000 | Disc Loss: 8.404242188266112e-30 | Gen Loss: 78.07910919189453\n",
      "Epoch 2900/10000 | Disc Loss: 1.7161241391496475e-30 | Gen Loss: 78.29957580566406\n",
      "Epoch 3000/10000 | Disc Loss: 5.538420439040487e-30 | Gen Loss: 77.74757385253906\n",
      "Epoch 3100/10000 | Disc Loss: 3.878098427317594e-30 | Gen Loss: 77.13220977783203\n",
      "Epoch 3200/10000 | Disc Loss: 2.549191719362711e-30 | Gen Loss: 78.18672180175781\n",
      "Epoch 3300/10000 | Disc Loss: 5.664589539098425e-30 | Gen Loss: 77.67505645751953\n",
      "Epoch 3400/10000 | Disc Loss: 3.284582339406626e-29 | Gen Loss: 78.15636444091797\n",
      "Epoch 3500/10000 | Disc Loss: 2.098312561008769e-29 | Gen Loss: 78.64158630371094\n",
      "Epoch 3600/10000 | Disc Loss: 3.138791248175835e-29 | Gen Loss: 78.60292053222656\n",
      "Epoch 3700/10000 | Disc Loss: 2.206856678690252e-28 | Gen Loss: 78.90193939208984\n",
      "Epoch 3800/10000 | Disc Loss: 1.1999875905849475e-29 | Gen Loss: 77.83937072753906\n",
      "Epoch 3900/10000 | Disc Loss: 3.3593382882093166e-30 | Gen Loss: 78.62915802001953\n",
      "Epoch 4000/10000 | Disc Loss: 4.407811352589094e-31 | Gen Loss: 79.67623901367188\n",
      "Epoch 4100/10000 | Disc Loss: 1.886478849340871e-30 | Gen Loss: 77.52267456054688\n",
      "Epoch 4200/10000 | Disc Loss: 4.180377645987478e-30 | Gen Loss: 77.40072631835938\n",
      "Epoch 4300/10000 | Disc Loss: 1.3916914222887995e-29 | Gen Loss: 78.09368133544922\n",
      "Epoch 4400/10000 | Disc Loss: 2.348789379926051e-29 | Gen Loss: 77.75533294677734\n",
      "Epoch 4500/10000 | Disc Loss: 4.491428497542746e-30 | Gen Loss: 78.05564880371094\n",
      "Epoch 4600/10000 | Disc Loss: 2.4480179550224654e-29 | Gen Loss: 77.94544982910156\n",
      "Epoch 4700/10000 | Disc Loss: 1.7978808056843663e-30 | Gen Loss: 78.21672821044922\n",
      "Epoch 4800/10000 | Disc Loss: 8.573764046603846e-29 | Gen Loss: 78.67120361328125\n",
      "Epoch 4900/10000 | Disc Loss: 1.234404711007532e-29 | Gen Loss: 77.66394805908203\n",
      "Epoch 5000/10000 | Disc Loss: 6.426749983516215e-29 | Gen Loss: 77.99992370605469\n",
      "Epoch 5100/10000 | Disc Loss: 4.605000059741792e-30 | Gen Loss: 77.4354248046875\n",
      "Epoch 5200/10000 | Disc Loss: 7.407559618232152e-31 | Gen Loss: 77.66889953613281\n",
      "Epoch 5300/10000 | Disc Loss: 3.4620827236508484e-28 | Gen Loss: 78.00175476074219\n",
      "Epoch 5400/10000 | Disc Loss: 6.332448629348617e-29 | Gen Loss: 77.73519134521484\n",
      "Epoch 5500/10000 | Disc Loss: 1.1624135249179755e-29 | Gen Loss: 78.61727905273438\n",
      "Epoch 5600/10000 | Disc Loss: 1.0564798548128323e-29 | Gen Loss: 78.82591247558594\n",
      "Epoch 5700/10000 | Disc Loss: 6.399405351808743e-31 | Gen Loss: 77.70968627929688\n",
      "Epoch 5800/10000 | Disc Loss: 2.1512680736997535e-30 | Gen Loss: 78.51102447509766\n",
      "Epoch 5900/10000 | Disc Loss: 2.2338277627433156e-29 | Gen Loss: 78.1772232055664\n",
      "Epoch 6000/10000 | Disc Loss: 9.054060731615364e-30 | Gen Loss: 78.47325134277344\n",
      "Epoch 6100/10000 | Disc Loss: 4.678914768363305e-30 | Gen Loss: 78.37487030029297\n",
      "Epoch 6200/10000 | Disc Loss: 2.0833152542538594e-28 | Gen Loss: 77.30320739746094\n",
      "Epoch 6300/10000 | Disc Loss: 1.1652105620040057e-30 | Gen Loss: 77.40113067626953\n",
      "Epoch 6400/10000 | Disc Loss: 3.0840909257100382e-30 | Gen Loss: 76.92664337158203\n",
      "Epoch 6500/10000 | Disc Loss: 1.105979282457032e-30 | Gen Loss: 77.13483428955078\n",
      "Epoch 6600/10000 | Disc Loss: 4.833201693292913e-30 | Gen Loss: 79.28374481201172\n",
      "Epoch 6700/10000 | Disc Loss: 2.385619329457088e-28 | Gen Loss: 78.32791137695312\n",
      "Epoch 6800/10000 | Disc Loss: 4.385914890343869e-32 | Gen Loss: 78.18586730957031\n",
      "Epoch 6900/10000 | Disc Loss: 8.230713643327669e-30 | Gen Loss: 78.30168914794922\n",
      "Epoch 7000/10000 | Disc Loss: 1.0521600541328825e-29 | Gen Loss: 77.86902618408203\n",
      "Epoch 7100/10000 | Disc Loss: 3.7307219045380527e-29 | Gen Loss: 78.12478637695312\n",
      "Epoch 7200/10000 | Disc Loss: 2.1693089967588855e-31 | Gen Loss: 78.32311248779297\n",
      "Epoch 7300/10000 | Disc Loss: 2.3669498460586842e-30 | Gen Loss: 77.43861389160156\n",
      "Epoch 7400/10000 | Disc Loss: 3.226775552125843e-29 | Gen Loss: 78.83663940429688\n",
      "Epoch 7500/10000 | Disc Loss: 4.2186697976434804e-29 | Gen Loss: 75.79547119140625\n",
      "Epoch 7600/10000 | Disc Loss: 9.345547783670123e-31 | Gen Loss: 76.67920684814453\n",
      "Epoch 7700/10000 | Disc Loss: 4.747061843422848e-30 | Gen Loss: 77.25856018066406\n",
      "Epoch 7800/10000 | Disc Loss: 1.0429097975636629e-29 | Gen Loss: 77.84461975097656\n",
      "Epoch 7900/10000 | Disc Loss: 3.2061889860237343e-28 | Gen Loss: 78.1782455444336\n",
      "Epoch 8000/10000 | Disc Loss: 2.5459304278357896e-29 | Gen Loss: 78.23863220214844\n",
      "Epoch 8100/10000 | Disc Loss: 2.3092926945067836e-30 | Gen Loss: 77.34833526611328\n",
      "Epoch 8200/10000 | Disc Loss: 1.6441465323708318e-29 | Gen Loss: 78.76103973388672\n",
      "Epoch 8300/10000 | Disc Loss: 1.3471989987259952e-30 | Gen Loss: 78.24040222167969\n",
      "Epoch 8400/10000 | Disc Loss: 9.661287635171047e-30 | Gen Loss: 78.13021850585938\n",
      "Epoch 8500/10000 | Disc Loss: 1.0118891411143896e-30 | Gen Loss: 76.80121612548828\n",
      "Epoch 8600/10000 | Disc Loss: 5.494984474568563e-29 | Gen Loss: 77.53620910644531\n",
      "Epoch 8700/10000 | Disc Loss: 2.3299996763694e-28 | Gen Loss: 77.25959014892578\n",
      "Epoch 8800/10000 | Disc Loss: 2.795893800224587e-31 | Gen Loss: 77.94540405273438\n",
      "Epoch 8900/10000 | Disc Loss: 3.472195526001255e-30 | Gen Loss: 78.08067321777344\n",
      "Epoch 9000/10000 | Disc Loss: 3.411275954447856e-30 | Gen Loss: 76.66158294677734\n",
      "Epoch 9100/10000 | Disc Loss: 1.8994181269174342e-30 | Gen Loss: 77.96047973632812\n",
      "Epoch 9200/10000 | Disc Loss: 8.893680871519117e-29 | Gen Loss: 77.00160217285156\n",
      "Epoch 9300/10000 | Disc Loss: 7.291513593404851e-30 | Gen Loss: 77.9603271484375\n",
      "Epoch 9400/10000 | Disc Loss: 1.395009510693487e-27 | Gen Loss: 78.03728485107422\n",
      "Epoch 9500/10000 | Disc Loss: 1.33498909188423e-30 | Gen Loss: 78.51000213623047\n",
      "Epoch 9600/10000 | Disc Loss: 2.5156495052795113e-30 | Gen Loss: 78.16817474365234\n",
      "Epoch 9700/10000 | Disc Loss: 6.276166937842546e-29 | Gen Loss: 77.21379852294922\n",
      "Epoch 9800/10000 | Disc Loss: 1.4718141892916946e-30 | Gen Loss: 77.8378677368164\n",
      "Epoch 9900/10000 | Disc Loss: 1.3953688952396408e-29 | Gen Loss: 77.09627532958984\n",
      "Epoch 10000/10000 | Disc Loss: 4.888295778154371e-29 | Gen Loss: 78.85871887207031\n",
      "tensor([[ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -0.6978,  0.9108, -1.0000,\n",
      "          1.0000,  0.3193,  0.9988, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -0.9998,  1.0000,  1.0000,  0.9951, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -0.8765,  1.0000, -1.0000, -0.9999, -1.0000,  1.0000,  1.0000,\n",
      "         -0.9991,  1.0000,  1.0000, -1.0000,  1.0000,  0.8995, -1.0000,  1.0000,\n",
      "         -1.0000, -1.0000, -0.9759, -1.0000,  1.0000,  1.0000,  0.9998,  1.0000,\n",
      "         -0.7170,  1.0000,  1.0000,  1.0000,  0.5257, -1.0000,  1.0000,  0.9997,\n",
      "         -1.0000,  0.9951,  0.9998,  0.9997, -1.0000,  1.0000,  0.9746,  1.0000,\n",
      "         -0.9988, -1.0000,  0.8251,  0.9676,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000, -1.0000,  1.0000, -0.9822,  0.9997,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  0.0928, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000,  1.0000,  0.9998,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  0.9999, -1.0000, -0.9983,  1.0000, -1.0000,  0.9885, -0.9999,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -0.9999,  1.0000,\n",
      "          1.0000,  1.0000, -0.0274, -1.0000,  0.9988,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, -1.0000]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Leer datos\n",
    "df = pd.read_csv('expr.csv')\n",
    "\n",
    "# Suponiendo que la primera columna es una columna de etiquetas (por ejemplo, nombres de genes)\n",
    "# y todas las demÃ¡s columnas son datos numÃ©ricos. Ajusta segÃºn tu CSV.\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Lidiar con valores NaN (si los hay). AquÃ­ simplemente los lleno con 0, pero podrÃ­as manejarlo de otra manera.\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "data = df.values\n",
    "n_samples, n_genes = data.shape\n",
    "\n",
    "# Normalizar los datos (esto es crucial para un buen entrenamiento)\n",
    "data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "# Convertir a tensores\n",
    "tensor_data = torch.FloatTensor(data)\n",
    "\n",
    "# Definir el generador y el discriminador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()  # Tanh funciona bien para datos normalizados\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instanciar modelos y optimizadores\n",
    "gen = Generator(100, n_genes)\n",
    "disc = Discriminator(n_genes)\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.Adam(disc.parameters(), lr=0.0002)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Entrenamiento\n",
    "n_epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for idx in range(0, n_samples, batch_size):\n",
    "        # Datos reales y etiquetas reales\n",
    "        real_data = tensor_data[idx:idx+batch_size]\n",
    "        current_batch_size = real_data.size(0)\n",
    "        real_labels = torch.ones(current_batch_size, 1)\n",
    "        \n",
    "        # Datos falsos y etiquetas falsas\n",
    "        noise = torch.randn(current_batch_size, 100)\n",
    "        fake_data = gen(noise)\n",
    "        fake_labels = torch.zeros(current_batch_size, 1)\n",
    "        \n",
    "        # Entrenar discriminador\n",
    "        disc_optimizer.zero_grad()\n",
    "        \n",
    "        real_preds = disc(real_data)\n",
    "        real_loss = criterion(real_preds, real_labels)\n",
    "        \n",
    "        fake_preds = disc(fake_data)\n",
    "        fake_loss = criterion(fake_preds, fake_labels)\n",
    "        \n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "        # Entrenar generador\n",
    "        gen_optimizer.zero_grad()\n",
    "        \n",
    "        noise = torch.randn(current_batch_size, 100)\n",
    "        fake_data = gen(noise)\n",
    "        fake_preds = disc(fake_data)\n",
    "        \n",
    "        gen_loss = criterion(fake_preds, real_labels)  # Queremos que el discriminador piense que estos datos son reales\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Imprimir estado del entrenamiento\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Disc Loss: {disc_loss.item()} | Gen Loss: {gen_loss.item()}\")\n",
    "\n",
    "# Una vez entrenada, puedes generar datos sintÃ©ticos\n",
    "noise = torch.randn(1, 100)\n",
    "synthetic_data = gen(noise)\n",
    "print(synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d7865-fa40-496c-944d-a230c3a87b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
