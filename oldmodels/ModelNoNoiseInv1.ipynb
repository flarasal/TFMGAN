{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefe2b6d-ee4c-44a3-b1f2-2dda2ca33fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade19a53-9154-49af-a8d1-6136ad278adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Definir el generador y el discriminador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #nn.Linear(input_dim, 128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, 256),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(256, output_dim),\n",
    "            #nn.Tanh()\n",
    "                    \n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(768, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #nn.Linear(1024, 768),\n",
    "            #nn.BatchNorm1d(768),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Linear(768, 512),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)*10\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #nn.Linear(input_dim, 256),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.Linear(256, 256),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.Linear(256, 128),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.Linear(128, 1),\n",
    "            #nn.Sigmoid()\n",
    "            \n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        \n",
    "            #nn.Linear(256, 128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(0.3),\n",
    "            \n",
    "            #nn.Linear(128, 64),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(0.3),\n",
    "    \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "# Leer datos\n",
    "expression_data = pd.read_csv('exprTRAIN.csv')\n",
    "methylation_data = pd.read_csv('methylTRAIN.csv')\n",
    "assign_data = pd.read_csv('assignTRAIN.csv')\n",
    "\n",
    "expression_data = expression_data.iloc[:, 1:]\n",
    "methylation_data = methylation_data.iloc[:, 1:]\n",
    "assign_data = assign_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data = methylation_data.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data = assign_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expression_data.fillna(0, inplace=True)\n",
    "methylation_data.fillna(0, inplace=True)\n",
    "assign_data.fillna(0, inplace=True)\n",
    "\n",
    "expression_data = expression_data.values\n",
    "methylation_data = methylation_data.values\n",
    "assign_data=assign_data.values\n",
    "# Asegurar que tienes el mismo número de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data.shape[0] == methylation_data.shape[0], \"Los datos de expresión y metilación deben tener el mismo número de muestras.\"\n",
    "assert assign_data.shape[0] == methylation_data.shape[0], \"Los datos de asignación y metilación deben tener el mismo número de muestras.\"\n",
    "\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "expression_data = torch.FloatTensor(expression_data).to(device)\n",
    "methylation_data = torch.FloatTensor(methylation_data).to(device)\n",
    "assign_data = torch.FloatTensor(assign_data).to(device)\n",
    "combined_data = torch.cat((expression_data,methylation_data,assign_data), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000 | Disc Loss: 0.0928548201918602 | Gen Loss: 4.791635036468506| Gan Loss: 4.1876220703125| Mse Loss: 0.20133768022060394\n",
      "El código tardó 29.93214 segundos en ejecutarse.\n",
      "Epoch 200/10000 | Disc Loss: 0.050168998539447784 | Gen Loss: 8.463813781738281| Gan Loss: 7.831985950469971| Mse Loss: 0.21060924232006073\n",
      "El código tardó 28.93259 segundos en ejecutarse.\n",
      "Epoch 300/10000 | Disc Loss: 0.0045082950964570045 | Gen Loss: 10.418811798095703| Gan Loss: 9.642791748046875| Mse Loss: 0.2586735188961029\n",
      "El código tardó 27.23135 segundos en ejecutarse.\n",
      "Epoch 400/10000 | Disc Loss: 0.0017557195387780666 | Gen Loss: 11.473146438598633| Gan Loss: 11.200204849243164| Mse Loss: 0.09098054468631744\n",
      "El código tardó 27.48838 segundos en ejecutarse.\n",
      "Epoch 500/10000 | Disc Loss: 0.0017733562272042036 | Gen Loss: 11.811843872070312| Gan Loss: 11.648148536682129| Mse Loss: 0.0545651949942112\n",
      "El código tardó 22.87618 segundos en ejecutarse.\n",
      "Epoch 600/10000 | Disc Loss: 0.0010975315235555172 | Gen Loss: 13.735002517700195| Gan Loss: 13.593445777893066| Mse Loss: 0.04718542844057083\n",
      "El código tardó 24.02223 segundos en ejecutarse.\n",
      "Epoch 700/10000 | Disc Loss: 0.0009737698128446937 | Gen Loss: 14.397544860839844| Gan Loss: 14.297886848449707| Mse Loss: 0.03321932628750801\n",
      "El código tardó 28.43170 segundos en ejecutarse.\n",
      "Epoch 800/10000 | Disc Loss: 0.00059540692018345 | Gen Loss: 15.010221481323242| Gan Loss: 14.928424835205078| Mse Loss: 0.027265435084700584\n",
      "El código tardó 26.16062 segundos en ejecutarse.\n",
      "Epoch 900/10000 | Disc Loss: 0.0006944468477740884 | Gen Loss: 15.148765563964844| Gan Loss: 15.086541175842285| Mse Loss: 0.020741380751132965\n",
      "El código tardó 21.40438 segundos en ejecutarse.\n",
      "Epoch 1000/10000 | Disc Loss: 0.0008109230548143387 | Gen Loss: 15.739020347595215| Gan Loss: 15.654319763183594| Mse Loss: 0.028233395889401436\n",
      "El código tardó 22.48849 segundos en ejecutarse.\n",
      "Epoch 1100/10000 | Disc Loss: 0.0004159078234806657 | Gen Loss: 16.942079544067383| Gan Loss: 16.890201568603516| Mse Loss: 0.017292704433202744\n",
      "El código tardó 28.46493 segundos en ejecutarse.\n",
      "Epoch 1200/10000 | Disc Loss: 0.0007432519341818988 | Gen Loss: 17.507160186767578| Gan Loss: 17.466594696044922| Mse Loss: 0.013521673157811165\n",
      "El código tardó 31.32479 segundos en ejecutarse.\n",
      "Epoch 1300/10000 | Disc Loss: 0.00043716764776036143 | Gen Loss: 17.364179611206055| Gan Loss: 17.32085418701172| Mse Loss: 0.01444190926849842\n",
      "El código tardó 22.29941 segundos en ejecutarse.\n",
      "Epoch 1400/10000 | Disc Loss: 0.00015845760935917497 | Gen Loss: 18.43340492248535| Gan Loss: 18.390722274780273| Mse Loss: 0.014227384701371193\n",
      "El código tardó 30.74079 segundos en ejecutarse.\n",
      "Epoch 1500/10000 | Disc Loss: 0.000834493082948029 | Gen Loss: 19.20737648010254| Gan Loss: 19.1744384765625| Mse Loss: 0.010979332029819489\n",
      "El código tardó 27.20899 segundos en ejecutarse.\n",
      "Epoch 1600/10000 | Disc Loss: 0.00024364171258639544 | Gen Loss: 18.304428100585938| Gan Loss: 18.269105911254883| Mse Loss: 0.011773861013352871\n",
      "El código tardó 27.01485 segundos en ejecutarse.\n",
      "Epoch 1700/10000 | Disc Loss: 0.0002173296525143087 | Gen Loss: 18.718093872070312| Gan Loss: 18.682050704956055| Mse Loss: 0.01201453898102045\n",
      "El código tardó 33.29867 segundos en ejecutarse.\n",
      "Epoch 1800/10000 | Disc Loss: 0.00018292466120328754 | Gen Loss: 19.196882247924805| Gan Loss: 19.163501739501953| Mse Loss: 0.011126887053251266\n",
      "El código tardó 39.93312 segundos en ejecutarse.\n",
      "Epoch 1900/10000 | Disc Loss: 0.00017593844677321613 | Gen Loss: 21.0126953125| Gan Loss: 20.975727081298828| Mse Loss: 0.012322543188929558\n",
      "El código tardó 34.51256 segundos en ejecutarse.\n",
      "Epoch 2000/10000 | Disc Loss: 0.00017369781562592834 | Gen Loss: 20.54619598388672| Gan Loss: 20.519180297851562| Mse Loss: 0.009005002677440643\n",
      "El código tardó 26.43973 segundos en ejecutarse.\n",
      "Epoch 2100/10000 | Disc Loss: 0.000362956925528124 | Gen Loss: 20.077604293823242| Gan Loss: 20.05166244506836| Mse Loss: 0.008647003211081028\n",
      "El código tardó 25.82577 segundos en ejecutarse.\n",
      "Epoch 2200/10000 | Disc Loss: 0.0003137326566502452 | Gen Loss: 21.314151763916016| Gan Loss: 21.2871150970459| Mse Loss: 0.009012149646878242\n",
      "El código tardó 32.61693 segundos en ejecutarse.\n",
      "Epoch 2300/10000 | Disc Loss: 0.00015289139992091805 | Gen Loss: 19.954328536987305| Gan Loss: 19.93132781982422| Mse Loss: 0.0076668355613946915\n",
      "El código tardó 32.65730 segundos en ejecutarse.\n",
      "Epoch 2400/10000 | Disc Loss: 0.00017406747792847455 | Gen Loss: 23.026426315307617| Gan Loss: 23.00295639038086| Mse Loss: 0.007823294959962368\n",
      "El código tardó 30.04813 segundos en ejecutarse.\n",
      "Epoch 2500/10000 | Disc Loss: 6.547664816025645e-05 | Gen Loss: 21.466703414916992| Gan Loss: 21.443904876708984| Mse Loss: 0.0075997523963451385\n",
      "El código tardó 25.49615 segundos en ejecutarse.\n",
      "Epoch 2600/10000 | Disc Loss: 0.00013928450061939657 | Gen Loss: 21.751771926879883| Gan Loss: 21.730195999145508| Mse Loss: 0.007192274555563927\n",
      "El código tardó 25.10937 segundos en ejecutarse.\n",
      "Epoch 2700/10000 | Disc Loss: 0.00017025945999193937 | Gen Loss: 21.046871185302734| Gan Loss: 21.027061462402344| Mse Loss: 0.00660321768373251\n",
      "El código tardó 17.87083 segundos en ejecutarse.\n",
      "Epoch 2800/10000 | Disc Loss: 0.0002986639447044581 | Gen Loss: 21.711130142211914| Gan Loss: 21.69167709350586| Mse Loss: 0.006484153680503368\n",
      "El código tardó 28.03360 segundos en ejecutarse.\n",
      "Epoch 2900/10000 | Disc Loss: 0.0001370128884445876 | Gen Loss: 23.254886627197266| Gan Loss: 23.237876892089844| Mse Loss: 0.005669710226356983\n",
      "El código tardó 29.79946 segundos en ejecutarse.\n",
      "Epoch 3000/10000 | Disc Loss: 7.338682189583778e-05 | Gen Loss: 23.571786880493164| Gan Loss: 23.546207427978516| Mse Loss: 0.008526727557182312\n",
      "El código tardó 25.06208 segundos en ejecutarse.\n",
      "Epoch 3100/10000 | Disc Loss: 8.339030318893492e-05 | Gen Loss: 21.595949172973633| Gan Loss: 21.574277877807617| Mse Loss: 0.007223872467875481\n",
      "El código tardó 21.26297 segundos en ejecutarse.\n",
      "Epoch 3200/10000 | Disc Loss: 0.00019561652152333409 | Gen Loss: 24.08808708190918| Gan Loss: 24.071565628051758| Mse Loss: 0.005506857298314571\n",
      "El código tardó 21.33398 segundos en ejecutarse.\n",
      "Epoch 3300/10000 | Disc Loss: 0.0002991337387356907 | Gen Loss: 23.717008590698242| Gan Loss: 23.69774627685547| Mse Loss: 0.006421064957976341\n",
      "El código tardó 25.49849 segundos en ejecutarse.\n",
      "Epoch 3400/10000 | Disc Loss: 7.33502529328689e-05 | Gen Loss: 26.916873931884766| Gan Loss: 26.89365577697754| Mse Loss: 0.007739095017313957\n",
      "El código tardó 24.49212 segundos en ejecutarse.\n",
      "Epoch 3500/10000 | Disc Loss: 9.023807797348127e-05 | Gen Loss: 23.15072250366211| Gan Loss: 23.13129997253418| Mse Loss: 0.006474217399954796\n",
      "El código tardó 31.02094 segundos en ejecutarse.\n",
      "Epoch 3600/10000 | Disc Loss: 7.232857751660049e-05 | Gen Loss: 25.159391403198242| Gan Loss: 25.14242935180664| Mse Loss: 0.00565427727997303\n",
      "El código tardó 28.80767 segundos en ejecutarse.\n",
      "Epoch 3700/10000 | Disc Loss: 3.851157089229673e-05 | Gen Loss: 24.979774475097656| Gan Loss: 24.963977813720703| Mse Loss: 0.005265796557068825\n",
      "El código tardó 25.25931 segundos en ejecutarse.\n",
      "Epoch 3800/10000 | Disc Loss: 3.811333954217844e-05 | Gen Loss: 23.99311637878418| Gan Loss: 23.977500915527344| Mse Loss: 0.005204951390624046\n",
      "El código tardó 25.97737 segundos en ejecutarse.\n",
      "Epoch 3900/10000 | Disc Loss: 6.251724698813632e-05 | Gen Loss: 24.923051834106445| Gan Loss: 24.903730392456055| Mse Loss: 0.0064407032914459705\n",
      "El código tardó 38.12901 segundos en ejecutarse.\n",
      "Epoch 4000/10000 | Disc Loss: 5.605731712421402e-05 | Gen Loss: 25.47763442993164| Gan Loss: 25.463518142700195| Mse Loss: 0.004705457016825676\n",
      "El código tardó 31.01265 segundos en ejecutarse.\n",
      "Epoch 4100/10000 | Disc Loss: 4.9829992349259555e-05 | Gen Loss: 25.42438507080078| Gan Loss: 25.411128997802734| Mse Loss: 0.0044187684543430805\n",
      "El código tardó 29.81635 segundos en ejecutarse.\n",
      "Epoch 4200/10000 | Disc Loss: 8.938051178120077e-05 | Gen Loss: 24.768421173095703| Gan Loss: 24.75443458557129| Mse Loss: 0.004662107676267624\n",
      "El código tardó 28.44570 segundos en ejecutarse.\n",
      "Epoch 4300/10000 | Disc Loss: 5.952068750048056e-05 | Gen Loss: 24.021595001220703| Gan Loss: 24.006465911865234| Mse Loss: 0.005043004173785448\n",
      "El código tardó 25.19464 segundos en ejecutarse.\n",
      "Epoch 4400/10000 | Disc Loss: 0.0001183850909001194 | Gen Loss: 25.642885208129883| Gan Loss: 25.61555290222168| Mse Loss: 0.009110539220273495\n",
      "El código tardó 21.60283 segundos en ejecutarse.\n",
      "Epoch 4500/10000 | Disc Loss: 6.315849896054715e-05 | Gen Loss: 25.24019432067871| Gan Loss: 25.220584869384766| Mse Loss: 0.006536280270665884\n",
      "El código tardó 23.68738 segundos en ejecutarse.\n",
      "Epoch 4600/10000 | Disc Loss: 2.275025144626852e-05 | Gen Loss: 28.239938735961914| Gan Loss: 28.223230361938477| Mse Loss: 0.0055694663897156715\n",
      "El código tardó 36.14485 segundos en ejecutarse.\n",
      "Epoch 4700/10000 | Disc Loss: 3.3775868359953165e-05 | Gen Loss: 25.45811653137207| Gan Loss: 25.444862365722656| Mse Loss: 0.004418213851749897\n",
      "El código tardó 32.29221 segundos en ejecutarse.\n",
      "Epoch 4800/10000 | Disc Loss: 3.684670809889212e-05 | Gen Loss: 26.94890022277832| Gan Loss: 26.936973571777344| Mse Loss: 0.003975508734583855\n",
      "El código tardó 24.87593 segundos en ejecutarse.\n",
      "Epoch 4900/10000 | Disc Loss: 4.604499918059446e-05 | Gen Loss: 25.634279251098633| Gan Loss: 25.62148094177246| Mse Loss: 0.004266372881829739\n",
      "El código tardó 23.80858 segundos en ejecutarse.\n",
      "Epoch 5000/10000 | Disc Loss: 0.00013582194515038282 | Gen Loss: 26.645524978637695| Gan Loss: 26.630184173583984| Mse Loss: 0.005113352555781603\n",
      "El código tardó 26.55032 segundos en ejecutarse.\n",
      "Epoch 5100/10000 | Disc Loss: 4.6103108616080135e-05 | Gen Loss: 26.33101463317871| Gan Loss: 26.319297790527344| Mse Loss: 0.0039059086702764034\n",
      "El código tardó 25.22065 segundos en ejecutarse.\n",
      "Epoch 5200/10000 | Disc Loss: 7.72055791458115e-05 | Gen Loss: 26.221223831176758| Gan Loss: 26.206796646118164| Mse Loss: 0.0048091174103319645\n",
      "El código tardó 36.63465 segundos en ejecutarse.\n",
      "Epoch 5300/10000 | Disc Loss: 1.651635830057785e-05 | Gen Loss: 27.187746047973633| Gan Loss: 27.17521858215332| Mse Loss: 0.004175710957497358\n",
      "El código tardó 38.86576 segundos en ejecutarse.\n",
      "Epoch 5400/10000 | Disc Loss: 5.986859468976036e-05 | Gen Loss: 26.781497955322266| Gan Loss: 26.766916275024414| Mse Loss: 0.00486036017537117\n",
      "El código tardó 40.24587 segundos en ejecutarse.\n",
      "Epoch 5500/10000 | Disc Loss: 3.7184461689321324e-05 | Gen Loss: 26.98885154724121| Gan Loss: 26.974102020263672| Mse Loss: 0.004916476085782051\n",
      "El código tardó 24.40195 segundos en ejecutarse.\n",
      "Epoch 5600/10000 | Disc Loss: 3.8574373320443556e-05 | Gen Loss: 28.77583122253418| Gan Loss: 28.763931274414062| Mse Loss: 0.0039666881784796715\n",
      "El código tardó 38.79529 segundos en ejecutarse.\n",
      "Epoch 5700/10000 | Disc Loss: 1.9887773305526935e-05 | Gen Loss: 28.345491409301758| Gan Loss: 28.332813262939453| Mse Loss: 0.004225919954478741\n",
      "El código tardó 40.67675 segundos en ejecutarse.\n",
      "Epoch 5800/10000 | Disc Loss: 6.0151764046167955e-05 | Gen Loss: 29.94586944580078| Gan Loss: 29.935029983520508| Mse Loss: 0.0036133339162915945\n",
      "El código tardó 28.91788 segundos en ejecutarse.\n",
      "Epoch 5900/10000 | Disc Loss: 0.00021695093892049044 | Gen Loss: 26.409603118896484| Gan Loss: 26.397571563720703| Mse Loss: 0.0040105669759213924\n",
      "El código tardó 30.48044 segundos en ejecutarse.\n",
      "Epoch 6000/10000 | Disc Loss: 1.1415722838137299e-05 | Gen Loss: 28.385074615478516| Gan Loss: 28.3729305267334| Mse Loss: 0.004048093222081661\n",
      "El código tardó 31.00311 segundos en ejecutarse.\n",
      "Epoch 6100/10000 | Disc Loss: 3.864198151859455e-05 | Gen Loss: 28.827974319458008| Gan Loss: 28.8181209564209| Mse Loss: 0.0032842380460351706\n",
      "El código tardó 19.05191 segundos en ejecutarse.\n",
      "Epoch 6200/10000 | Disc Loss: 2.3389839043375105e-05 | Gen Loss: 26.689706802368164| Gan Loss: 26.67808723449707| Mse Loss: 0.0038728879299014807\n",
      "El código tardó 30.95057 segundos en ejecutarse.\n",
      "Epoch 6300/10000 | Disc Loss: 3.2513620681129396e-05 | Gen Loss: 29.360742568969727| Gan Loss: 29.34857749938965| Mse Loss: 0.004054809454828501\n",
      "El código tardó 37.34819 segundos en ejecutarse.\n",
      "Epoch 6400/10000 | Disc Loss: 5.099443660583347e-05 | Gen Loss: 28.333595275878906| Gan Loss: 28.321630477905273| Mse Loss: 0.003988317679613829\n",
      "El código tardó 35.52921 segundos en ejecutarse.\n",
      "Epoch 6500/10000 | Disc Loss: 3.104344796156511e-05 | Gen Loss: 28.318185806274414| Gan Loss: 28.306602478027344| Mse Loss: 0.0038611481431871653\n",
      "El código tardó 28.82313 segundos en ejecutarse.\n",
      "Epoch 6600/10000 | Disc Loss: 5.0760088925017044e-05 | Gen Loss: 30.06732749938965| Gan Loss: 30.054840087890625| Mse Loss: 0.004162168130278587\n",
      "El código tardó 31.26725 segundos en ejecutarse.\n",
      "Epoch 6700/10000 | Disc Loss: 4.1639905248302966e-05 | Gen Loss: 29.243539810180664| Gan Loss: 29.23259162902832| Mse Loss: 0.0036492536310106516\n",
      "El código tardó 34.81228 segundos en ejecutarse.\n",
      "Epoch 6800/10000 | Disc Loss: 3.1691641197539866e-05 | Gen Loss: 28.080345153808594| Gan Loss: 28.065887451171875| Mse Loss: 0.0048194024711847305\n",
      "El código tardó 28.39813 segundos en ejecutarse.\n",
      "Epoch 6900/10000 | Disc Loss: 2.5592002202756703e-05 | Gen Loss: 28.804622650146484| Gan Loss: 28.794267654418945| Mse Loss: 0.0034515310544520617\n",
      "El código tardó 26.45057 segundos en ejecutarse.\n",
      "Epoch 7000/10000 | Disc Loss: 3.4469492675270885e-05 | Gen Loss: 28.210155487060547| Gan Loss: 28.199281692504883| Mse Loss: 0.003624388249590993\n",
      "El código tardó 32.01721 segundos en ejecutarse.\n",
      "Epoch 7100/10000 | Disc Loss: 2.1323648979887366e-05 | Gen Loss: 30.820924758911133| Gan Loss: 30.81076431274414| Mse Loss: 0.003386648604646325\n",
      "El código tardó 39.62227 segundos en ejecutarse.\n",
      "Epoch 7200/10000 | Disc Loss: 2.7549245714908466e-05 | Gen Loss: 27.984872817993164| Gan Loss: 27.973453521728516| Mse Loss: 0.0038065151311457157\n",
      "El código tardó 35.50677 segundos en ejecutarse.\n",
      "Epoch 7300/10000 | Disc Loss: 0.00010470156121300533 | Gen Loss: 28.095809936523438| Gan Loss: 28.08402442932129| Mse Loss: 0.003928718622773886\n",
      "El código tardó 33.82559 segundos en ejecutarse.\n",
      "Epoch 7400/10000 | Disc Loss: 2.325748027942609e-05 | Gen Loss: 30.029659271240234| Gan Loss: 30.019275665283203| Mse Loss: 0.003460943466052413\n",
      "El código tardó 35.58858 segundos en ejecutarse.\n",
      "Epoch 7500/10000 | Disc Loss: 2.949628105852753e-05 | Gen Loss: 30.290122985839844| Gan Loss: 30.279638290405273| Mse Loss: 0.0034947472158819437\n",
      "El código tardó 39.99170 segundos en ejecutarse.\n",
      "Epoch 7600/10000 | Disc Loss: 2.5144019673462026e-05 | Gen Loss: 29.886642456054688| Gan Loss: 29.874868392944336| Mse Loss: 0.003924911841750145\n",
      "El código tardó 37.88020 segundos en ejecutarse.\n",
      "Epoch 7700/10000 | Disc Loss: 2.426512583042495e-05 | Gen Loss: 29.940587997436523| Gan Loss: 29.929275512695312| Mse Loss: 0.003770739072933793\n",
      "El código tardó 36.18139 segundos en ejecutarse.\n",
      "Epoch 7800/10000 | Disc Loss: 2.156579284928739e-05 | Gen Loss: 29.99968719482422| Gan Loss: 29.987680435180664| Mse Loss: 0.004002431407570839\n",
      "El código tardó 37.85209 segundos en ejecutarse.\n",
      "Epoch 7900/10000 | Disc Loss: 0.00010828840458998457 | Gen Loss: 29.78960418701172| Gan Loss: 29.778610229492188| Mse Loss: 0.0036647452507168055\n",
      "El código tardó 40.82707 segundos en ejecutarse.\n",
      "Epoch 8000/10000 | Disc Loss: 2.749789200606756e-05 | Gen Loss: 29.9092960357666| Gan Loss: 29.897846221923828| Mse Loss: 0.0038164276629686356\n",
      "El código tardó 34.68002 segundos en ejecutarse.\n",
      "Epoch 8100/10000 | Disc Loss: 1.673347105679568e-05 | Gen Loss: 31.186065673828125| Gan Loss: 31.17599105834961| Mse Loss: 0.003358021378517151\n",
      "El código tardó 35.66511 segundos en ejecutarse.\n",
      "Epoch 8200/10000 | Disc Loss: 1.122269532061182e-05 | Gen Loss: 30.190383911132812| Gan Loss: 30.1805477142334| Mse Loss: 0.0032789723481982946\n",
      "El código tardó 33.92134 segundos en ejecutarse.\n",
      "Epoch 8300/10000 | Disc Loss: 1.7575728634255938e-05 | Gen Loss: 30.097740173339844| Gan Loss: 30.087223052978516| Mse Loss: 0.0035056332126259804\n",
      "El código tardó 32.69426 segundos en ejecutarse.\n",
      "Epoch 8400/10000 | Disc Loss: 1.9197688743588515e-05 | Gen Loss: 31.30439567565918| Gan Loss: 31.29459571838379| Mse Loss: 0.00326640740968287\n",
      "El código tardó 28.22287 segundos en ejecutarse.\n",
      "Epoch 8500/10000 | Disc Loss: 1.1121823263238184e-05 | Gen Loss: 28.936567306518555| Gan Loss: 28.924869537353516| Mse Loss: 0.0038991831243038177\n",
      "El código tardó 27.57811 segundos en ejecutarse.\n",
      "Epoch 8600/10000 | Disc Loss: 2.556457184255123e-05 | Gen Loss: 30.238544464111328| Gan Loss: 30.228166580200195| Mse Loss: 0.0034593171440064907\n",
      "El código tardó 40.33347 segundos en ejecutarse.\n",
      "Epoch 8700/10000 | Disc Loss: 6.280928573687561e-06 | Gen Loss: 31.20954704284668| Gan Loss: 31.199289321899414| Mse Loss: 0.0034192667808383703\n",
      "El código tardó 40.25901 segundos en ejecutarse.\n",
      "Epoch 8800/10000 | Disc Loss: 1.651534330449067e-05 | Gen Loss: 29.60407257080078| Gan Loss: 29.59347152709961| Mse Loss: 0.00353386253118515\n",
      "El código tardó 40.40244 segundos en ejecutarse.\n",
      "Epoch 8900/10000 | Disc Loss: 3.156318416586146e-05 | Gen Loss: 30.508203506469727| Gan Loss: 30.498165130615234| Mse Loss: 0.0033459593541920185\n",
      "El código tardó 31.72632 segundos en ejecutarse.\n",
      "Epoch 9000/10000 | Disc Loss: 2.603683060442563e-05 | Gen Loss: 30.22113800048828| Gan Loss: 30.211021423339844| Mse Loss: 0.0033721933141350746\n",
      "El código tardó 29.22734 segundos en ejecutarse.\n",
      "Epoch 9100/10000 | Disc Loss: 2.2528507543029264e-05 | Gen Loss: 28.979215621948242| Gan Loss: 28.969017028808594| Mse Loss: 0.003399320412427187\n",
      "El código tardó 28.06978 segundos en ejecutarse.\n",
      "Epoch 9200/10000 | Disc Loss: 1.8927014025393873e-05 | Gen Loss: 29.404462814331055| Gan Loss: 29.39391326904297| Mse Loss: 0.0035165692679584026\n",
      "El código tardó 39.07246 segundos en ejecutarse.\n",
      "Epoch 9300/10000 | Disc Loss: 1.722909655654803e-05 | Gen Loss: 30.96279525756836| Gan Loss: 30.95287322998047| Mse Loss: 0.0033074261154979467\n",
      "El código tardó 34.86822 segundos en ejecutarse.\n",
      "Epoch 9400/10000 | Disc Loss: 5.260999387246557e-06 | Gen Loss: 31.134973526000977| Gan Loss: 31.123409271240234| Mse Loss: 0.0038549192249774933\n",
      "El código tardó 39.66245 segundos en ejecutarse.\n",
      "Epoch 9500/10000 | Disc Loss: 1.3046833373664413e-05 | Gen Loss: 28.386253356933594| Gan Loss: 28.37689781188965| Mse Loss: 0.0031183979008346796\n",
      "El código tardó 41.15239 segundos en ejecutarse.\n",
      "Epoch 9600/10000 | Disc Loss: 2.0088311430299655e-05 | Gen Loss: 30.663314819335938| Gan Loss: 30.65379524230957| Mse Loss: 0.0031732753850519657\n",
      "El código tardó 35.05455 segundos en ejecutarse.\n",
      "Epoch 9700/10000 | Disc Loss: 8.445364983344916e-06 | Gen Loss: 30.474445343017578| Gan Loss: 30.464736938476562| Mse Loss: 0.0032358579337596893\n",
      "El código tardó 38.60147 segundos en ejecutarse.\n",
      "Epoch 9800/10000 | Disc Loss: 2.587282142485492e-05 | Gen Loss: 31.46297836303711| Gan Loss: 31.451080322265625| Mse Loss: 0.003966020420193672\n",
      "El código tardó 38.62720 segundos en ejecutarse.\n",
      "Epoch 9900/10000 | Disc Loss: 1.5880536011536606e-05 | Gen Loss: 29.477827072143555| Gan Loss: 29.468036651611328| Mse Loss: 0.0032635554671287537\n",
      "El código tardó 31.57055 segundos en ejecutarse.\n",
      "Epoch 10000/10000 | Disc Loss: 1.0630444194248412e-05 | Gen Loss: 30.136890411376953| Gan Loss: 30.126754760742188| Mse Loss: 0.0033783691469579935\n",
      "El código tardó 41.00952 segundos en ejecutarse.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modelos y optimizadores\n",
    "gen = Generator(methylation_data.shape[1], expression_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "disc = Discriminator(expression_data.shape[1] + methylation_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.Adam(disc.parameters(), lr=0.0002)\n",
    "criterion = nn.BCELoss()\n",
    "mse_criterion = nn.MSELoss()\n",
    "# Entrenamiento\n",
    "n_samples = combined_data.size(0)\n",
    "batch_size = 64\n",
    "n_epochs = 10000\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for idx in range(0, n_samples, batch_size):\n",
    "        real_data = combined_data[idx:idx+batch_size].to(device)\n",
    "        current_batch_size = real_data.size(0)\n",
    "        real_labels = torch.ones(current_batch_size, 1).to(device)\n",
    "\n",
    "        #noise = torch.randn(current_batch_size, 100).to(device)\n",
    "        noise = methylation_data[idx:idx+batch_size]\n",
    "        \n",
    "        fake_expr = gen(noise).to(device)\n",
    "        #fake_data = torch.cat((expression_data[idx:idx+batch_size],fake_methyl), 1)\n",
    "\n",
    "        fake_data = torch.cat((fake_expr,methylation_data[idx:idx+batch_size] ), 1).to(device)\n",
    "\n",
    "        fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
    "        \n",
    "        # Entrenar discriminador\n",
    "        disc_optimizer.zero_grad()\n",
    "\n",
    "        real_preds = disc(real_data).to(device)\n",
    "        real_loss = criterion(real_preds, real_labels)\n",
    "        \n",
    "        fake_preds = disc(fake_data).to(device)\n",
    "        fake_loss = criterion(fake_preds, fake_labels)\n",
    "\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Entrenar generador\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        #noise = torch.randn(current_batch_size, 100).to(device)\n",
    "        noise = methylation_data[idx:idx+batch_size]\n",
    "        fake_expr = gen(noise).to(device)\n",
    "        fake_data = torch.cat((fake_expr,methylation_data[idx:idx+batch_size] ), 1).to(device)\n",
    "        fake_preds = disc(fake_data).to(device)\n",
    "\n",
    "\n",
    "        # Loss basado en la capacidad de engañar al discriminador\n",
    "        gan_loss = criterion(fake_preds, real_labels)\n",
    "\n",
    "        # MSE loss entre el metilación generado y el metilación real\n",
    "        mse_loss = mse_criterion(fake_expr,  torch.cat((expression_data[idx:idx+batch_size],assign_data[idx:idx+batch_size]),1).to(device))\n",
    "\n",
    "        # Combina ambos losses. El coeficiente 'alpha' permite ponderar la importancia relativa de cada loss.\n",
    "        alpha = 3\n",
    "        gen_loss = gan_loss + alpha * mse_loss\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Disc Loss: {disc_loss.item()} | Gen Loss: {gen_loss.item()}| Gan Loss: {gan_loss.item()}| Mse Loss: {mse_loss.item()}\")\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calcular la diferencia de tiempo\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"El código tardó {elapsed_time:.5f} segundos en ejecutarse.\")\n",
    "        start_time = time.time()\n",
    "\n",
    "torch.save(gen.state_dict(), 'generator_model2.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be72300-c585-4101-b1f9-9ff6cb527c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Error cuadrático medio entre datos generados y reales: 10.495379447937012\n",
      "tensor(2.1460, device='cuda:0')\n",
      "Fila 1: MSE = 11.581097602844238\n",
      "Fila 2: MSE = 10.028812408447266\n",
      "Fila 3: MSE = 11.99780559539795\n",
      "Fila 4: MSE = 10.645430564880371\n",
      "Fila 5: MSE = 12.25088119506836\n",
      "Fila 6: MSE = 10.169303894042969\n",
      "Fila 7: MSE = 11.072985649108887\n",
      "Fila 8: MSE = 10.984508514404297\n",
      "Fila 9: MSE = 10.076899528503418\n",
      "Fila 10: MSE = 11.520282745361328\n",
      "Fila 11: MSE = 10.783255577087402\n",
      "Fila 12: MSE = 10.950822830200195\n",
      "Fila 13: MSE = 10.302997589111328\n",
      "Fila 14: MSE = 10.051380157470703\n",
      "Fila 15: MSE = 10.744034767150879\n",
      "Fila 16: MSE = 10.102511405944824\n",
      "Fila 17: MSE = 11.275416374206543\n",
      "Fila 18: MSE = 10.679347038269043\n",
      "Fila 19: MSE = 10.502598762512207\n",
      "Fila 20: MSE = 11.78080940246582\n",
      "Fila 21: MSE = 9.829861640930176\n",
      "Fila 22: MSE = 10.311623573303223\n",
      "Fila 23: MSE = 9.769621849060059\n",
      "Fila 24: MSE = 9.961358070373535\n",
      "Fila 25: MSE = 10.184032440185547\n",
      "Fila 26: MSE = 9.201407432556152\n",
      "Fila 27: MSE = 11.309487342834473\n",
      "Fila 28: MSE = 11.02479362487793\n",
      "Fila 29: MSE = 10.75346565246582\n",
      "Fila 30: MSE = 10.498064041137695\n",
      "Fila 31: MSE = 10.54364013671875\n",
      "Fila 32: MSE = 10.523576736450195\n",
      "Fila 33: MSE = 10.293956756591797\n",
      "Fila 34: MSE = 10.407363891601562\n",
      "Fila 35: MSE = 10.853740692138672\n",
      "Fila 36: MSE = 10.109162330627441\n",
      "Fila 37: MSE = 10.327730178833008\n",
      "Fila 38: MSE = 9.170857429504395\n",
      "Fila 39: MSE = 10.603808403015137\n",
      "Fila 40: MSE = 10.795210838317871\n",
      "Fila 41: MSE = 11.130295753479004\n",
      "Fila 42: MSE = 10.49722957611084\n",
      "Fila 43: MSE = 10.076807022094727\n",
      "Fila 44: MSE = 10.269302368164062\n",
      "Fila 45: MSE = 11.414107322692871\n",
      "Fila 46: MSE = 10.355441093444824\n",
      "Fila 47: MSE = 10.561737060546875\n",
      "Fila 48: MSE = 10.53836441040039\n",
      "Fila 49: MSE = 9.468705177307129\n",
      "Fila 50: MSE = 10.997146606445312\n",
      "Fila 51: MSE = 10.676572799682617\n",
      "Fila 52: MSE = 9.968545913696289\n",
      "Fila 53: MSE = 10.314136505126953\n",
      "Fila 54: MSE = 11.470206260681152\n",
      "Fila 55: MSE = 10.547102928161621\n",
      "Fila 56: MSE = 11.070033073425293\n",
      "Fila 57: MSE = 10.565035820007324\n",
      "Fila 58: MSE = 10.672234535217285\n",
      "Fila 59: MSE = 10.656947135925293\n",
      "Fila 60: MSE = 9.742980003356934\n",
      "Fila 61: MSE = 10.417804718017578\n",
      "Fila 62: MSE = 10.343756675720215\n",
      "Fila 63: MSE = 11.07059383392334\n",
      "Fila 64: MSE = 11.099002838134766\n",
      "Fila 65: MSE = 10.788763046264648\n",
      "Fila 66: MSE = 10.115506172180176\n",
      "Fila 67: MSE = 10.053277015686035\n",
      "Fila 68: MSE = 10.661855697631836\n",
      "Fila 69: MSE = 10.088284492492676\n",
      "Fila 70: MSE = 9.983552932739258\n",
      "Fila 71: MSE = 10.82511043548584\n",
      "Fila 72: MSE = 8.32619571685791\n",
      "Fila 73: MSE = 10.225131034851074\n",
      "Fila 74: MSE = 9.769440650939941\n",
      "Fila 75: MSE = 10.424530029296875\n",
      "Fila 76: MSE = 10.825098991394043\n",
      "Fila 77: MSE = 10.933283805847168\n",
      "Fila 78: MSE = 11.851839065551758\n",
      "Fila 79: MSE = 10.431180000305176\n",
      "Fila 80: MSE = 9.708174705505371\n",
      "Fila 81: MSE = 10.247185707092285\n",
      "Fila 82: MSE = 10.096105575561523\n",
      "Fila 83: MSE = 10.5140380859375\n",
      "Fila 84: MSE = 9.023662567138672\n",
      "Fila 85: MSE = 9.410970687866211\n",
      "Fila 86: MSE = 10.47872543334961\n",
      "Fila 87: MSE = 10.806130409240723\n",
      "Fila 88: MSE = 10.18381404876709\n",
      "Fila 89: MSE = 10.251436233520508\n",
      "Fila 90: MSE = 10.169763565063477\n",
      "Fila 91: MSE = 10.550405502319336\n",
      "Fila 92: MSE = 10.300167083740234\n",
      "Fila 93: MSE = 9.023833274841309\n",
      "Fila 94: MSE = 10.412358283996582\n",
      "Fila 95: MSE = 11.214397430419922\n",
      "Fila 96: MSE = 9.79205322265625\n",
      "Fila 97: MSE = 9.667428970336914\n",
      "Fila 98: MSE = 9.644454002380371\n",
      "Fila 99: MSE = 10.809199333190918\n",
      "Fila 100: MSE = 11.689146041870117\n",
      "Fila 101: MSE = 10.794951438903809\n",
      "Fila 102: MSE = 10.738529205322266\n",
      "Fila 103: MSE = 11.11531925201416\n",
      "Fila 104: MSE = 9.749591827392578\n",
      "Fila 105: MSE = 10.397274017333984\n",
      "Fila 106: MSE = 9.012449264526367\n",
      "Fila 107: MSE = 10.743353843688965\n",
      "Fila 108: MSE = 10.633636474609375\n",
      "Fila 109: MSE = 10.211390495300293\n",
      "Fila 110: MSE = 10.482194900512695\n",
      "Fila 111: MSE = 10.601859092712402\n",
      "Fila 112: MSE = 10.98640251159668\n",
      "Fila 113: MSE = 9.741363525390625\n",
      "Fila 114: MSE = 10.450429916381836\n",
      "Fila 115: MSE = 9.524057388305664\n",
      "Fila 116: MSE = 10.141742706298828\n",
      "Fila 117: MSE = 12.309089660644531\n",
      "Fila 118: MSE = 11.1887788772583\n",
      "Fila 119: MSE = 9.28386402130127\n",
      "Fila 120: MSE = 10.043466567993164\n",
      "Fila 121: MSE = 9.944757461547852\n",
      "Fila 122: MSE = 11.108189582824707\n",
      "Fila 123: MSE = 10.457582473754883\n",
      "Fila 124: MSE = 10.966643333435059\n",
      "Fila 125: MSE = 10.974822044372559\n",
      "Fila 126: MSE = 12.337382316589355\n",
      "Fila 127: MSE = 10.501252174377441\n",
      "Fila 128: MSE = 11.204792976379395\n",
      "Fila 129: MSE = 10.036648750305176\n",
      "Fila 130: MSE = 11.06177806854248\n",
      "Fila 131: MSE = 10.674052238464355\n",
      "Fila 132: MSE = 9.571039199829102\n",
      "Fila 133: MSE = 11.112951278686523\n",
      "Fila 134: MSE = 9.47980785369873\n",
      "Fila 135: MSE = 10.650529861450195\n",
      "Fila 136: MSE = 11.365026473999023\n",
      "Fila 137: MSE = 9.641885757446289\n",
      "Fila 138: MSE = 11.178587913513184\n",
      "Fila 139: MSE = 10.615403175354004\n",
      "Fila 140: MSE = 9.872410774230957\n",
      "Fila 141: MSE = 10.291535377502441\n",
      "Fila 142: MSE = 9.983973503112793\n",
      "Fila 143: MSE = 10.588055610656738\n",
      "Fila 144: MSE = 10.832818984985352\n",
      "Fila 145: MSE = 9.050505638122559\n",
      "Fila 146: MSE = 10.518604278564453\n",
      "Fila 147: MSE = 9.20308780670166\n",
      "Fila 148: MSE = 10.28754997253418\n",
      "Fila 149: MSE = 10.311017990112305\n",
      "Fila 150: MSE = 9.595760345458984\n",
      "Fila 151: MSE = 10.093785285949707\n",
      "Fila 152: MSE = 10.80121898651123\n",
      "Fila 153: MSE = 10.749542236328125\n",
      "Fila 154: MSE = 10.916913032531738\n",
      "Fila 155: MSE = 9.312299728393555\n",
      "Fila 156: MSE = 11.104011535644531\n",
      "Fila 157: MSE = 10.157594680786133\n",
      "Fila 158: MSE = 11.009860038757324\n",
      "Fila 159: MSE = 9.919794082641602\n",
      "Fila 160: MSE = 10.368206977844238\n",
      "Fila 161: MSE = 10.57673454284668\n",
      "Fila 162: MSE = 11.455810546875\n",
      "Fila 163: MSE = 9.673699378967285\n",
      "Fila 164: MSE = 9.410552024841309\n",
      "Fila 165: MSE = 9.93814468383789\n",
      "Fila 166: MSE = 9.922245025634766\n",
      "Fila 167: MSE = 10.594807624816895\n",
      "Fila 168: MSE = 10.863640785217285\n",
      "Fila 169: MSE = 10.719954490661621\n",
      "Fila 170: MSE = 10.579581260681152\n",
      "Fila 171: MSE = 11.769883155822754\n",
      "Fila 172: MSE = 10.94343376159668\n",
      "Fila 173: MSE = 10.885527610778809\n",
      "Fila 174: MSE = 10.80366325378418\n",
      "Fila 175: MSE = 11.408195495605469\n",
      "Fila 176: MSE = 11.199115753173828\n",
      "Fila 177: MSE = 9.798404693603516\n",
      "Fila 178: MSE = 10.922371864318848\n",
      "Fila 179: MSE = 9.503260612487793\n",
      "Fila 180: MSE = 9.953680038452148\n",
      "Fila 181: MSE = 11.094139099121094\n",
      "Fila 182: MSE = 10.229268074035645\n",
      "Fila 183: MSE = 9.561135292053223\n",
      "Fila 184: MSE = 10.920218467712402\n",
      "Fila 185: MSE = 10.898027420043945\n",
      "Fila 186: MSE = 10.070606231689453\n",
      "Fila 187: MSE = 9.622784614562988\n",
      "Fila 188: MSE = 9.930989265441895\n",
      "Fila 189: MSE = 10.331862449645996\n",
      "Fila 190: MSE = 10.742202758789062\n",
      "Fila 191: MSE = 10.080022811889648\n",
      "Fila 192: MSE = 9.645227432250977\n",
      "Fila 193: MSE = 10.29082202911377\n",
      "Fila 194: MSE = 11.144950866699219\n",
      "Fila 195: MSE = 9.325793266296387\n",
      "Fila 196: MSE = 10.409340858459473\n",
      "Fila 197: MSE = 9.997966766357422\n",
      "Fila 198: MSE = 10.071484565734863\n",
      "Fila 199: MSE = 11.144283294677734\n",
      "Fila 200: MSE = 10.719560623168945\n",
      "Fila 201: MSE = 9.599061965942383\n",
      "Fila 202: MSE = 10.469070434570312\n",
      "Fila 203: MSE = 10.070483207702637\n",
      "Fila 204: MSE = 10.751276969909668\n",
      "Fila 205: MSE = 9.339113235473633\n",
      "Fila 206: MSE = 10.794340133666992\n",
      "Fila 207: MSE = 9.494499206542969\n",
      "Fila 208: MSE = 11.230253219604492\n",
      "Fila 209: MSE = 10.958755493164062\n",
      "Fila 210: MSE = 10.64018726348877\n",
      "Fila 211: MSE = 10.68349552154541\n",
      "Fila 212: MSE = 11.736948013305664\n",
      "Fila 213: MSE = 11.357892990112305\n",
      "Fila 214: MSE = 10.72746753692627\n",
      "Fila 215: MSE = 10.496079444885254\n",
      "Fila 216: MSE = 10.042535781860352\n",
      "Fila 217: MSE = 9.976808547973633\n",
      "Fila 218: MSE = 9.962482452392578\n",
      "Fila 219: MSE = 10.375163078308105\n",
      "Fila 220: MSE = 10.34097671508789\n",
      "Fila 221: MSE = 11.624990463256836\n",
      "Fila 222: MSE = 11.439665794372559\n",
      "Fila 223: MSE = 9.600810050964355\n",
      "Fila 224: MSE = 10.712770462036133\n",
      "Fila 225: MSE = 10.306716918945312\n",
      "Fila 226: MSE = 10.67158317565918\n",
      "Fila 227: MSE = 10.94257640838623\n",
      "Fila 228: MSE = 11.5089693069458\n",
      "Fila 229: MSE = 10.5903959274292\n",
      "Fila 230: MSE = 10.854377746582031\n",
      "Fila 231: MSE = 10.43393611907959\n",
      "Fila 232: MSE = 9.492435455322266\n",
      "Fila 233: MSE = 10.40068531036377\n",
      "Fila 234: MSE = 9.547392845153809\n",
      "Fila 235: MSE = 11.795341491699219\n",
      "Fila 236: MSE = 9.448894500732422\n",
      "Fila 237: MSE = 10.854462623596191\n",
      "Fila 238: MSE = 10.7985200881958\n",
      "Fila 239: MSE = 10.756155967712402\n",
      "Fila 240: MSE = 11.171120643615723\n",
      "Fila 241: MSE = 10.1286039352417\n",
      "Fila 242: MSE = 9.977341651916504\n",
      "Fila 243: MSE = 11.218313217163086\n",
      "Fila 244: MSE = 9.721635818481445\n",
      "Fila 245: MSE = 10.699610710144043\n",
      "Fila 246: MSE = 10.81379508972168\n",
      "Fila 247: MSE = 10.693047523498535\n",
      "Fila 248: MSE = 10.518817901611328\n",
      "Fila 249: MSE = 10.48612117767334\n",
      "Fila 250: MSE = 9.285727500915527\n",
      "Fila 251: MSE = 9.794615745544434\n",
      "Fila 252: MSE = 9.857049942016602\n",
      "Fila 253: MSE = 10.038888931274414\n",
      "Fila 254: MSE = 11.071741104125977\n",
      "Fila 255: MSE = 10.681888580322266\n",
      "Fila 256: MSE = 10.572806358337402\n",
      "Fila 257: MSE = 11.1104736328125\n",
      "Fila 258: MSE = 10.117631912231445\n",
      "Fila 259: MSE = 9.195341110229492\n",
      "Fila 260: MSE = 10.917994499206543\n",
      "Fila 261: MSE = 10.023996353149414\n",
      "Fila 262: MSE = 10.362420082092285\n",
      "Fila 263: MSE = 8.567294120788574\n",
      "Fila 264: MSE = 9.444759368896484\n",
      "Fila 265: MSE = 9.269172668457031\n",
      "Fila 266: MSE = 9.308947563171387\n",
      "Fila 267: MSE = 10.484543800354004\n",
      "Fila 268: MSE = 10.838238716125488\n",
      "Fila 269: MSE = 10.521988868713379\n",
      "Fila 270: MSE = 9.82247257232666\n",
      "Fila 271: MSE = 9.780664443969727\n",
      "Fila 272: MSE = 10.341597557067871\n",
      "Fila 273: MSE = 10.065353393554688\n",
      "Fila 274: MSE = 10.410616874694824\n",
      "Fila 275: MSE = 10.564851760864258\n",
      "Fila 276: MSE = 11.254429817199707\n",
      "Fila 277: MSE = 10.32297134399414\n",
      "Fila 278: MSE = 11.066508293151855\n",
      "Fila 279: MSE = 10.101827621459961\n",
      "Fila 280: MSE = 9.879117012023926\n",
      "Fila 281: MSE = 10.281291007995605\n",
      "Fila 282: MSE = 10.811538696289062\n",
      "Fila 283: MSE = 10.058926582336426\n",
      "Fila 284: MSE = 10.559139251708984\n",
      "Fila 285: MSE = 10.16094970703125\n",
      "Fila 286: MSE = 10.015079498291016\n",
      "Fila 287: MSE = 10.63543701171875\n",
      "Fila 288: MSE = 11.347216606140137\n",
      "Fila 289: MSE = 9.46720027923584\n",
      "Fila 290: MSE = 10.191847801208496\n",
      "Fila 291: MSE = 9.871292114257812\n",
      "Fila 292: MSE = 10.856429100036621\n",
      "Fila 293: MSE = 11.088497161865234\n",
      "Fila 294: MSE = 8.887299537658691\n",
      "Fila 295: MSE = 8.31339168548584\n",
      "Fila 296: MSE = 10.064207077026367\n",
      "Fila 297: MSE = 9.976993560791016\n",
      "Fila 298: MSE = 9.779509544372559\n",
      "Fila 299: MSE = 10.153046607971191\n",
      "Fila 300: MSE = 10.269160270690918\n",
      "Fila 301: MSE = 11.334074020385742\n",
      "Fila 302: MSE = 10.131893157958984\n",
      "Fila 303: MSE = 10.190998077392578\n",
      "Fila 304: MSE = 10.938608169555664\n",
      "Fila 305: MSE = 10.562000274658203\n",
      "Fila 306: MSE = 11.12244987487793\n",
      "Fila 307: MSE = 10.412379264831543\n",
      "Fila 308: MSE = 10.81685733795166\n",
      "Fila 309: MSE = 9.884722709655762\n",
      "Fila 310: MSE = 11.772199630737305\n",
      "Fila 311: MSE = 10.174240112304688\n",
      "Fila 312: MSE = 12.043336868286133\n",
      "Fila 313: MSE = 10.014123916625977\n",
      "Fila 314: MSE = 10.18094539642334\n",
      "Fila 315: MSE = 9.182219505310059\n",
      "Fila 316: MSE = 10.032772064208984\n",
      "Fila 317: MSE = 11.240859985351562\n",
      "Fila 318: MSE = 10.117853164672852\n",
      "Fila 319: MSE = 11.251004219055176\n",
      "Fila 320: MSE = 10.89428424835205\n",
      "Fila 321: MSE = 10.603899002075195\n",
      "Fila 322: MSE = 10.105950355529785\n",
      "Fila 323: MSE = 9.595640182495117\n",
      "Fila 324: MSE = 8.276237487792969\n",
      "Fila 325: MSE = 10.985761642456055\n",
      "Fila 326: MSE = 11.105738639831543\n",
      "Fila 327: MSE = 10.860577583312988\n",
      "Fila 328: MSE = 9.659676551818848\n",
      "Fila 329: MSE = 9.556059837341309\n",
      "Fila 330: MSE = 9.58891773223877\n",
      "Fila 331: MSE = 10.464366912841797\n",
      "Fila 332: MSE = 9.768089294433594\n",
      "Fila 333: MSE = 11.231637954711914\n",
      "Fila 334: MSE = 10.38325309753418\n",
      "Fila 335: MSE = 10.624462127685547\n",
      "Fila 336: MSE = 10.84751033782959\n",
      "Fila 337: MSE = 10.700810432434082\n",
      "Fila 338: MSE = 10.305261611938477\n",
      "Fila 339: MSE = 9.516566276550293\n",
      "Fila 340: MSE = 10.812291145324707\n",
      "Fila 341: MSE = 11.426480293273926\n",
      "Fila 342: MSE = 11.044367790222168\n",
      "Fila 343: MSE = 10.832706451416016\n",
      "Fila 344: MSE = 10.720490455627441\n",
      "Fila 345: MSE = 11.663703918457031\n",
      "Fila 346: MSE = 9.850915908813477\n",
      "Fila 347: MSE = 11.145608901977539\n",
      "Fila 348: MSE = 10.220787048339844\n",
      "Fila 349: MSE = 10.968358039855957\n",
      "Fila 350: MSE = 10.615928649902344\n",
      "Fila 351: MSE = 10.953575134277344\n",
      "Fila 352: MSE = 10.654597282409668\n",
      "Fila 353: MSE = 10.489624977111816\n",
      "Fila 354: MSE = 10.692768096923828\n",
      "Fila 355: MSE = 9.237597465515137\n",
      "Fila 356: MSE = 10.864055633544922\n",
      "Fila 357: MSE = 9.377338409423828\n",
      "Fila 358: MSE = 11.225044250488281\n",
      "Fila 359: MSE = 11.78791618347168\n",
      "Fila 360: MSE = 10.878016471862793\n",
      "Fila 361: MSE = 10.253974914550781\n",
      "Fila 362: MSE = 8.25045108795166\n",
      "Fila 363: MSE = 10.059820175170898\n",
      "Fila 364: MSE = 9.908677101135254\n",
      "Fila 365: MSE = 11.671975135803223\n",
      "Fila 366: MSE = 10.73591423034668\n",
      "Fila 367: MSE = 12.118732452392578\n",
      "Fila 368: MSE = 9.830286026000977\n",
      "Fila 369: MSE = 10.02530574798584\n",
      "Fila 370: MSE = 10.403921127319336\n",
      "Fila 371: MSE = 10.270769119262695\n",
      "Fila 372: MSE = 10.580292701721191\n",
      "Fila 373: MSE = 11.156558990478516\n",
      "Fila 374: MSE = 10.61298942565918\n",
      "Fila 375: MSE = 9.701050758361816\n",
      "Fila 376: MSE = 10.404635429382324\n",
      "Fila 377: MSE = 9.991573333740234\n",
      "Fila 378: MSE = 9.964902877807617\n",
      "Fila 379: MSE = 10.725349426269531\n",
      "Fila 380: MSE = 12.046527862548828\n",
      "Fila 381: MSE = 10.39415454864502\n",
      "Fila 382: MSE = 10.543272018432617\n",
      "Fila 383: MSE = 10.884902954101562\n",
      "Fila 384: MSE = 10.780122756958008\n",
      "Fila 385: MSE = 10.995782852172852\n",
      "Fila 386: MSE = 11.551279067993164\n",
      "Fila 387: MSE = 10.21059799194336\n",
      "Fila 388: MSE = 9.680249214172363\n",
      "Fila 389: MSE = 10.131973266601562\n",
      "Fila 390: MSE = 10.758893013000488\n",
      "Fila 391: MSE = 10.380906105041504\n",
      "Fila 392: MSE = 10.251222610473633\n",
      "Fila 393: MSE = 10.066518783569336\n",
      "Fila 394: MSE = 11.057120323181152\n",
      "Fila 395: MSE = 10.225323677062988\n",
      "Fila 396: MSE = 9.732102394104004\n",
      "Fila 397: MSE = 10.828678131103516\n",
      "Fila 398: MSE = 9.173432350158691\n",
      "Fila 399: MSE = 10.608867645263672\n",
      "Fila 400: MSE = 11.309347152709961\n",
      "Fila 401: MSE = 10.096988677978516\n",
      "Fila 402: MSE = 8.457536697387695\n",
      "Fila 403: MSE = 10.646482467651367\n",
      "Fila 404: MSE = 9.111886024475098\n",
      "Fila 405: MSE = 10.439423561096191\n",
      "Fila 406: MSE = 11.377494812011719\n",
      "Fila 407: MSE = 9.640015602111816\n",
      "Fila 408: MSE = 10.335854530334473\n",
      "Fila 409: MSE = 10.025583267211914\n",
      "Fila 410: MSE = 10.456071853637695\n",
      "Fila 411: MSE = 9.932273864746094\n",
      "Fila 412: MSE = 10.431443214416504\n",
      "Fila 413: MSE = 9.876235008239746\n",
      "Fila 414: MSE = 9.597975730895996\n",
      "Fila 415: MSE = 10.0758056640625\n",
      "Fila 416: MSE = 10.997199058532715\n",
      "Fila 417: MSE = 10.514625549316406\n",
      "Fila 418: MSE = 9.489422798156738\n",
      "Fila 419: MSE = 10.029223442077637\n",
      "Fila 420: MSE = 10.235305786132812\n",
      "Fila 421: MSE = 10.745849609375\n",
      "Fila 422: MSE = 10.323680877685547\n",
      "Fila 423: MSE = 10.397984504699707\n",
      "Fila 424: MSE = 9.81157398223877\n",
      "Fila 425: MSE = 9.606173515319824\n",
      "Fila 426: MSE = 10.990700721740723\n",
      "Fila 427: MSE = 10.615250587463379\n",
      "Fila 428: MSE = 10.596007347106934\n",
      "Fila 429: MSE = 10.587428092956543\n",
      "Fila 430: MSE = 10.399532318115234\n",
      "Fila 431: MSE = 11.475643157958984\n",
      "Fila 432: MSE = 10.266246795654297\n",
      "Fila 433: MSE = 11.286267280578613\n",
      "Fila 434: MSE = 10.695784568786621\n",
      "Fila 435: MSE = 10.626128196716309\n",
      "Fila 436: MSE = 10.48782730102539\n",
      "Fila 437: MSE = 10.622751235961914\n",
      "Fila 438: MSE = 9.975809097290039\n",
      "Fila 439: MSE = 10.932208061218262\n",
      "Fila 440: MSE = 11.304656028747559\n",
      "Fila 441: MSE = 10.826323509216309\n",
      "Fila 442: MSE = 10.230733871459961\n",
      "Fila 443: MSE = 10.547119140625\n",
      "Fila 444: MSE = 10.849838256835938\n",
      "Fila 445: MSE = 9.664942741394043\n",
      "Fila 446: MSE = 10.141656875610352\n",
      "Fila 447: MSE = 10.374053955078125\n",
      "Fila 448: MSE = 10.640073776245117\n",
      "Fila 449: MSE = 10.173417091369629\n",
      "Fila 450: MSE = 10.904241561889648\n",
      "Fila 451: MSE = 10.938645362854004\n",
      "Fila 452: MSE = 9.79096794128418\n",
      "Fila 453: MSE = 11.268845558166504\n",
      "Fila 454: MSE = 10.078428268432617\n",
      "Fila 455: MSE = 9.32091999053955\n",
      "Fila 456: MSE = 9.489692687988281\n",
      "Fila 457: MSE = 9.696470260620117\n",
      "Fila 458: MSE = 10.957590103149414\n",
      "Fila 459: MSE = 10.542551040649414\n",
      "Fila 460: MSE = 8.761295318603516\n",
      "Fila 461: MSE = 11.019779205322266\n",
      "Fila 462: MSE = 10.737212181091309\n",
      "Fila 463: MSE = 12.36473560333252\n",
      "Fila 464: MSE = 9.762151718139648\n",
      "Fila 465: MSE = 9.052511215209961\n",
      "Fila 466: MSE = 8.490744590759277\n",
      "Fila 467: MSE = 9.766668319702148\n",
      "Fila 468: MSE = 10.48566722869873\n",
      "Fila 469: MSE = 10.812950134277344\n",
      "Fila 470: MSE = 10.832951545715332\n",
      "Fila 471: MSE = 9.703286170959473\n",
      "Fila 472: MSE = 11.212379455566406\n",
      "Fila 473: MSE = 10.388920783996582\n",
      "Fila 474: MSE = 9.954073905944824\n",
      "Fila 475: MSE = 9.809947967529297\n",
      "Fila 476: MSE = 10.919002532958984\n",
      "Fila 477: MSE = 9.723177909851074\n",
      "Fila 478: MSE = 10.423833847045898\n",
      "Fila 479: MSE = 12.401185989379883\n",
      "Fila 480: MSE = 10.322832107543945\n",
      "Fila 481: MSE = 9.683246612548828\n",
      "Fila 482: MSE = 10.120870590209961\n",
      "Fila 483: MSE = 10.571762084960938\n",
      "Fila 484: MSE = 10.78895378112793\n",
      "Fila 485: MSE = 10.668356895446777\n",
      "Fila 486: MSE = 10.934159278869629\n",
      "Fila 487: MSE = 9.967550277709961\n",
      "Fila 488: MSE = 10.098052024841309\n",
      "Fila 489: MSE = 11.459399223327637\n",
      "Fila 490: MSE = 9.82439136505127\n",
      "Fila 491: MSE = 9.802258491516113\n",
      "Fila 492: MSE = 10.878363609313965\n",
      "Fila 493: MSE = 10.647832870483398\n",
      "Fila 494: MSE = 11.114563941955566\n",
      "Fila 495: MSE = 10.36980152130127\n",
      "Fila 496: MSE = 9.720704078674316\n",
      "Fila 497: MSE = 11.32905101776123\n",
      "Fila 498: MSE = 9.644696235656738\n",
      "Fila 499: MSE = 10.684733390808105\n",
      "Fila 500: MSE = 9.228096008300781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Leer datos\n",
    "expression_data2 = pd.read_csv('exprTEST.csv')\n",
    "methylation_data2 = pd.read_csv('methylTEST.csv')\n",
    "assign_data2 = pd.read_csv('assignTEST.csv')\n",
    "\n",
    "expression_data2 = expression_data2.iloc[:, 1:]\n",
    "methylation_data2 = methylation_data2.iloc[:, 1:]\n",
    "assign_data2 = assign_data2.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data2 = expression_data2.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data2 = methylation_data2.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data2 = assign_data2.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expression_data2.fillna(0, inplace=True)\n",
    "methylation_data2.fillna(0, inplace=True)\n",
    "assign_data2.fillna(0, inplace=True)\n",
    "\n",
    "expression_data2 = expression_data2.values\n",
    "methylation_data2 = methylation_data2.values\n",
    "assign_data2=assign_data2.values\n",
    "# Asegurar que tienes el mismo número de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data2.shape[0] == methylation_data2.shape[0], \"Los datos de expresión y metilación deben tener el mismo número de muestras.\"\n",
    "assert assign_data2.shape[0] == methylation_data2.shape[0], \"Los datos de asignación y metilación deben tener el mismo número de muestras.\"\n",
    "\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "expression_data2 = torch.FloatTensor(expression_data2).to(device)\n",
    "methylation_data2 = torch.FloatTensor(methylation_data2).to(device)\n",
    "assign_data2 = torch.FloatTensor(assign_data2).to(device)\n",
    "combined_data2= torch.cat((expression_data2,methylation_data2,assign_data2), 1).to(device)\n",
    "# Carga del modelo previamente entrenado\n",
    "gen = Generator(methylation_data2.shape[1], expression_data2.shape[1]+assign_data2.shape[1])\n",
    "gen.load_state_dict(torch.load('generator_model2.pth'))\n",
    "gen.eval()\n",
    "gen.to(device)\n",
    "\n",
    "# Pasar todos los datos de expr.csv a través del generador\n",
    "with torch.no_grad():\n",
    "    generated_expr = gen(methylation_data2)\n",
    "\n",
    "# Asegúrate de que los datos generados y los datos reales estén en la misma forma\n",
    "generated_expr = generated_expr.cpu().numpy()\n",
    "\n",
    "# Aquí estamos asumiendo que la primera parte de la salida generada corresponde a methyl.csv\n",
    "# Si tu salida generada incluye más datos además de methyl.csv, necesitas ajustar esto\n",
    "generated_expr_data = generated_expr[:, :expression_data2.shape[1]]\n",
    "\n",
    "# Calcula el error cuadrático medio entre los datos generados y los reales\n",
    "mse = mean_squared_error(expression_data2.cpu().numpy(), generated_expr_data)\n",
    "print(f\"Error cuadrático medio entre datos generados y reales: {mse}\")\n",
    "print(expression_data2.mean())\n",
    "# Si el MSE es bajo, significa que los datos generados y los datos reales son muy similares.\n",
    "\n",
    "individual_mse_errors = []\n",
    "\n",
    "# Procesar datos\n",
    "with torch.no_grad():\n",
    "    for i in range(methylation_data2.shape[0]):\n",
    "        input_expr = methylation_data2[i].unsqueeze(0)  # Añadir dimensión de batch\n",
    "        real_output = torch.cat([expression_data2[i], assign_data2[i]]).unsqueeze(0)  # Añadir dimensión de batch\n",
    "        generated_output = gen(input_expr)\n",
    "        \n",
    "        error = mse_loss(generated_output, real_output)\n",
    "        #print(generated_output)\n",
    "        #print(real_output)\n",
    "        individual_mse_errors.append(error.item())\n",
    "\n",
    "# Si deseas, puedes imprimir el error MSE para cada fila\n",
    "for i, error in enumerate(individual_mse_errors):\n",
    "    print(f\"Fila {i + 1}: MSE = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4389ef-27fa-4d77-b5cb-ac1662916c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo SVC\u001b[39;00m\n\u001b[1;32m     29\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Predicciones\u001b[39;00m\n\u001b[1;32m     33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:747\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    750\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Generar datos de expresión sintética\n",
    "gen2 = Generator(expression_data.shape[1], methylation_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "gen2.load_state_dict(torch.load('generator_model2.pth'))\n",
    "gen2.eval()\n",
    "expressions_list = []\n",
    "for i in range(100000):\n",
    "    noise = torch.randn(1, expression_data.shape[1]).to(device)\n",
    "    synthetic_expression = gen2(noise).to(device)\n",
    "    #print(synthetic_expression)\n",
    "    expressions_list.append(synthetic_expression.detach().cpu().numpy().squeeze())\n",
    "\n",
    "X = pd.DataFrame(expressions_list)\n",
    "#X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = X.iloc[:, -1].values.astype(int)  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "X = X.iloc[:, :-1]\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a54d7865-fa40-496c-944d-a230c3a87b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     23934\n",
      "           2       1.00      1.00      1.00      6066\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprMID.csv')\n",
    "meth_data = pd.read_csv('methylMID.csv')\n",
    "assig_data = pd.read_csv('assignMID.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = pd.concat([expr_data, meth_data], axis=1)\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4684f20f-50aa-4729-b7f8-fa924b20592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     23934\n",
      "           2       1.00      1.00      1.00      6066\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprMID.csv')\n",
    "meth_data = pd.read_csv('methylMID.csv')\n",
    "assig_data = pd.read_csv('assignMID.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = meth_data\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8b7b5-c373-4a86-9b69-eb5ddebd998a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
