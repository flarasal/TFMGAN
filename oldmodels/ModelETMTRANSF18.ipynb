{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefe2b6d-ee4c-44a3-b1f2-2dda2ca33fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade19a53-9154-49af-a8d1-6136ad278adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from torch.utils.data import TensorDataset,DataLoader, random_split\n",
    "\n",
    "fnexpr='exprLOW.csv'\n",
    "fnmet='methylLOW.csv'\n",
    "fnassig='assignLOW.csv'\n",
    "nsamp=10000\n",
    "fnmodel='ETMTRANSF18-10K.pth'\n",
    "\n",
    "\n",
    "#Función para contar número de parámetros del modelo\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "# Definir el modelo Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, nhid)\n",
    "        self.transformer_encoder = TransformerEncoder(TransformerEncoderLayer(nhid, nhead, nhid, dropout), nlayers)\n",
    "        self.decoder = nn.Linear(nhid, output_dim)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "        \n",
    "# Leer datos\n",
    "expression_data = pd.read_csv(fnexpr)\n",
    "methylation_data = pd.read_csv(fnmet)\n",
    "assign_data = pd.read_csv(fnassig)\n",
    "\n",
    "expression_data = expression_data.iloc[:, 1:]\n",
    "methylation_data = methylation_data.iloc[:, 1:]\n",
    "assign_data = assign_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data = methylation_data.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data = assign_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "#expression_data.fillna(0, inplace=True)\n",
    "#methylation_data.fillna(0, inplace=True)\n",
    "#assign_data.fillna(0, inplace=True)\n",
    "\n",
    "#Normalizamos\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Asumiendo que expression_data, methylation_data y assign_data son tus DataFrames\n",
    "expression_data_scaled = scaler.fit_transform(expression_data)\n",
    "methylation_data_scaled = scaler.fit_transform(methylation_data)\n",
    "assign_data_scaled =assign_data\n",
    "\n",
    "# Convertir a DataFrame\n",
    "expression_data = pd.DataFrame(expression_data_scaled, index=expression_data.index, columns=expression_data.columns)\n",
    "methylation_data = pd.DataFrame(methylation_data_scaled, index=methylation_data.index, columns=methylation_data.columns)\n",
    "assign_data = pd.DataFrame(assign_data_scaled, index=assign_data.index, columns=assign_data.columns)\n",
    "\n",
    "#print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "\n",
    "expression_data_tensor = torch.FloatTensor(expression_data.values).to(device)\n",
    "methylation_data_tensor = torch.FloatTensor(methylation_data.values).to(device)\n",
    "assign_data_tensor = torch.FloatTensor(assign_data.values).to(device)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "dataset = TensorDataset(expression_data_tensor, methylation_data_tensor)\n",
    "train_size = int(0.9 * len(dataset))  # Ajusta esto según tu necesidad\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Convertir los Subset en tensores para el entrenamiento\n",
    "expression_train, methylation_train= zip(*[(e, m) for e, m in train_dataset])\n",
    "expression_train = torch.stack(expression_train)\n",
    "methylation_train = torch.stack(methylation_train)\n",
    "#assign_train = torch.stack(assign_train)\n",
    "\n",
    "# Convertir los Subset en tensores para la validación/prueba\n",
    "expression_test, methylation_test = zip(*[(e, m) for e, m in test_dataset])\n",
    "expression_test = torch.stack(expression_test)\n",
    "methylation_test = torch.stack(methylation_test)\n",
    "#assign_test = torch.stack(assign_test)\n",
    "input_dim = expression_data.shape[1]\n",
    "output_dim = methylation_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/PyEnv/PB/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0399\n",
      "Saved model with Train Loss: 0.039882\n",
      "Epoch 2, Train Loss: 0.0190\n",
      "Saved model with Train Loss: 0.019040\n",
      "Epoch 3, Train Loss: 0.0153\n",
      "Saved model with Train Loss: 0.015290\n",
      "Epoch 4, Train Loss: 0.0147\n",
      "Saved model with Train Loss: 0.014726\n",
      "Epoch 5, Train Loss: 0.0139\n",
      "Saved model with Train Loss: 0.013913\n",
      "Epoch 6, Train Loss: 0.0140\n",
      "Epoch 7, Train Loss: 0.0137\n",
      "Saved model with Train Loss: 0.013656\n",
      "Epoch 8, Train Loss: 0.0136\n",
      "Saved model with Train Loss: 0.013588\n",
      "Epoch 9, Train Loss: 0.0136\n",
      "Saved model with Train Loss: 0.013550\n",
      "Epoch 10, Train Loss: 0.0135\n",
      "Saved model with Train Loss: 0.013490\n",
      "Epoch 11, Train Loss: 0.0135\n",
      "Saved model with Train Loss: 0.013463\n",
      "Epoch 12, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013445\n",
      "Epoch 13, Train Loss: 0.0135\n",
      "Epoch 14, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013403\n",
      "Epoch 15, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013396\n",
      "Epoch 16, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013372\n",
      "Epoch 17, Train Loss: 0.0134\n",
      "Epoch 18, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013355\n",
      "Epoch 19, Train Loss: 0.0134\n",
      "Saved model with Train Loss: 0.013351\n",
      "Epoch 20, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013349\n",
      "Epoch 21, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013333\n",
      "Epoch 22, Train Loss: 0.0133\n",
      "Epoch 23, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013332\n",
      "Epoch 24, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013317\n",
      "Epoch 25, Train Loss: 0.0133\n",
      "Epoch 26, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013316\n",
      "Epoch 27, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013313\n",
      "Epoch 28, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013305\n",
      "Epoch 29, Train Loss: 0.0133\n",
      "Epoch 30, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013303\n",
      "Epoch 31, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013302\n",
      "Epoch 32, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013296\n",
      "Epoch 33, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013294\n",
      "Epoch 34, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013287\n",
      "Epoch 35, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013281\n",
      "Epoch 36, Train Loss: 0.0133\n",
      "Epoch 37, Train Loss: 0.0133\n",
      "Epoch 38, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013269\n",
      "Epoch 39, Train Loss: 0.0133\n",
      "Epoch 40, Train Loss: 0.0133\n",
      "Epoch 41, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013265\n",
      "Epoch 42, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013264\n",
      "Epoch 43, Train Loss: 0.0133\n",
      "Epoch 44, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013258\n",
      "Epoch 45, Train Loss: 0.0133\n",
      "Saved model with Train Loss: 0.013252\n",
      "Epoch 46, Train Loss: 0.0133\n",
      "Epoch 47, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013244\n",
      "Epoch 48, Train Loss: 0.0132\n",
      "Epoch 49, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013229\n",
      "Epoch 50, Train Loss: 0.0132\n",
      "Epoch 51, Train Loss: 0.0132\n",
      "Epoch 52, Train Loss: 0.0132\n",
      "Epoch 53, Train Loss: 0.0132\n",
      "Epoch 54, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013221\n",
      "Epoch 55, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013215\n",
      "Epoch 56, Train Loss: 0.0132\n",
      "Epoch 57, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013188\n",
      "Epoch 58, Train Loss: 0.0132\n",
      "Epoch 59, Train Loss: 0.0132\n",
      "Epoch 60, Train Loss: 0.0132\n",
      "Epoch 61, Train Loss: 0.0132\n",
      "Epoch 62, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013183\n",
      "Epoch 63, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013167\n",
      "Epoch 64, Train Loss: 0.0132\n",
      "Saved model with Train Loss: 0.013152\n",
      "Epoch 65, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013142\n",
      "Epoch 66, Train Loss: 0.0131\n",
      "Epoch 67, Train Loss: 0.0132\n",
      "Epoch 68, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013129\n",
      "Epoch 69, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013129\n",
      "Epoch 70, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013111\n",
      "Epoch 71, Train Loss: 0.0131\n",
      "Epoch 72, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013087\n",
      "Epoch 73, Train Loss: 0.0131\n",
      "Epoch 74, Train Loss: 0.0131\n",
      "Epoch 75, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013070\n",
      "Epoch 76, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013066\n",
      "Epoch 77, Train Loss: 0.0131\n",
      "Saved model with Train Loss: 0.013057\n",
      "Epoch 78, Train Loss: 0.0130\n",
      "Saved model with Train Loss: 0.013024\n",
      "Epoch 79, Train Loss: 0.0130\n",
      "Saved model with Train Loss: 0.013017\n",
      "Epoch 80, Train Loss: 0.0130\n",
      "Saved model with Train Loss: 0.012976\n",
      "Epoch 81, Train Loss: 0.0130\n",
      "Saved model with Train Loss: 0.012970\n",
      "Epoch 82, Train Loss: 0.0130\n",
      "Saved model with Train Loss: 0.012964\n",
      "Epoch 83, Train Loss: 0.0130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parámetros del Transformer\n",
    "\n",
    "nhead = 8  # Número de cabezas en el multiheadattention\n",
    "nhid = 512  # Dimensión oculta\n",
    "nlayers = 4  # Número de capas TransformerEncoderLayer\n",
    "dropout = 0.2  # Probabilidad de dropout\n",
    "\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model = TransformerModel(input_dim, output_dim, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "# Hiperparámetros y optimizador\n",
    "lr = 0.0002  # Tasa de aprendizaje\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Entrenamiento\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:\n",
    "        expression_data, methylation_data = batch\n",
    "        expression_data = expression_data.to(device)\n",
    "        methylation_data = methylation_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = generate_square_subsequent_mask(expression_data.size(0)).to(device)\n",
    "        output = model(expression_data, src_mask)\n",
    "        loss = criterion(output, methylation_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            expression_data, methylation_data = batch\n",
    "            expression_data = expression_data.to(device)\n",
    "            methylation_data = methylation_data.to(device)\n",
    "            src_mask = generate_square_subsequent_mask(expression_data.size(0)).to(device)\n",
    "            output = model(expression_data, src_mask)\n",
    "            loss = criterion(output, methylation_data)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "n_epochs = 100  # Número de épocas\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    #test_loss = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Guardar el modelo si es el mejor hasta ahora\n",
    "    if train_loss < best_val_loss:\n",
    "        best_val_loss = train_loss\n",
    "        torch.save(model.state_dict(), fnmodel)\n",
    "        print(f\"Saved model with Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be72300-c585-4101-b1f9-9ff6cb527c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn.functional import mse_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "import sys\n",
    "import umap\n",
    "# Leer datos\n",
    "#expression_data_test_tensor, methylation_data_test_tensor = zip(*[(e, m) for e, m in test_dataset])\n",
    "\n",
    "\n",
    "# Convertir los tensores a numpy y luego a DataFrame de pandas\n",
    "expression_data_test_np = expression_test.cpu().numpy()\n",
    "methylation_data_test_np = methylation_test.cpu().numpy()\n",
    "\n",
    "#expression_data_test_tensor = torch.FloatTensor(expression_data_test_np).to(device)\n",
    "expression_data_test_df = pd.DataFrame(expression_data_test_np)\n",
    "methylation_data_test_df = pd.DataFrame(methylation_data_test_np)\n",
    "\n",
    "\n",
    "\n",
    "X_real = pd.concat([expression_data_test_df, methylation_data_test_df], axis=1)\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "\n",
    "# Cargar el modelo Transformer previamente entrenado\n",
    "model = TransformerModel(input_dim, output_dim, nhead, nhid, nlayers, dropout).to(device)  # Asegúrate de proporcionar los parámetros correctos aquí\n",
    "model.load_state_dict(torch.load(fnmodel))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Preparar los datos de expresión para la inferencia\n",
    "#expression_data_test_tensor = torch.FloatTensor(expression_data_test.values).to(device)\n",
    "\n",
    "# Generar datos de metilación utilizando el modelo Transformer\n",
    "with torch.no_grad():\n",
    "    # Necesitamos crear una máscara de secuencia para la inferencia\n",
    "    src_mask = generate_square_subsequent_mask(expression_test.size(0)).to(device)\n",
    "    generated_methyl = model(expression_test, src_mask)\n",
    "\n",
    "# Convertir los datos generados a formato numpy\n",
    "generated_methyl = generated_methyl.cpu().numpy()\n",
    "\n",
    "# Suponiendo que la primera parte de la salida generada corresponde a los datos de metilación\n",
    "generated_methyl_data = generated_methyl[:, :methylation_test.shape[1]]\n",
    "\n",
    "# Convertir a DataFrame de pandas\n",
    "generated_methyl_data_df = pd.DataFrame(generated_methyl_data)\n",
    "\n",
    "X_gan = pd.concat([expression_data_test_df, generated_methyl_data_df], axis=1)\n",
    "#last_column = X_gan.columns[-1]\n",
    "#X_gan = X_gan.drop(columns=[last_column])\n",
    "X_gan.columns = X_real.columns\n",
    "# Concatena los datos reales con los generados\n",
    "X_combined = np.vstack([X_real, X_gan])\n",
    "\n",
    "#generated_methyl_data_np = generated_methyl_data.cpu().numpy()\n",
    "\n",
    "# Calcular el MSE\n",
    "mse = mean_squared_error(methylation_data_test_np, generated_methyl_data)\n",
    "\n",
    "print(f\"MSE entre los datos de metilación reales y los generados: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8b7b5-c373-4a86-9b69-eb5ddebd998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate([np.ones(X_real.shape[0]), np.zeros(X_gan.shape[0])])\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_combined)\n",
    "\n",
    "plt.scatter(X_tsne[labels==1, 0], X_tsne[labels==1, 1], c='blue', label='Real', s=3)\n",
    "plt.scatter(X_tsne[labels==0, 0], X_tsne[labels==0, 1], c='red', label='Generated', s=3)\n",
    "plt.legend()\n",
    "plt.title('t-SNE visualization')\n",
    "#plt.savefig(filename+\"tsne.jpg\")\n",
    "plt.show()\n",
    "\n",
    "# Configurando y entrenando UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_combined)\n",
    "\n",
    "# Dibujando la visualización\n",
    "plt.scatter(X_umap[labels==1, 0], X_umap[labels==1, 1], c='blue', label='Real', s=3)\n",
    "plt.scatter(X_umap[labels==0, 0], X_umap[labels==0, 1], c='red', label='Generated', s=3)\n",
    "plt.legend()\n",
    "plt.title('UMAP visualization')\n",
    "#plt.savefig(filename+\"umap.jpg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "plt.scatter(X_pca[labels==1, 0], X_pca[labels==1, 1], c='blue', label='Real', s=3)\n",
    "plt.scatter(X_pca[labels==0, 0], X_pca[labels==0, 1], c='red', label='Generated', s=3)\n",
    "plt.legend()\n",
    "plt.title('PCA visualization')\n",
    "#plt.savefig(filename+\"pca.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaef2b3-ab07-4354-a354-d1a8434bdec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
