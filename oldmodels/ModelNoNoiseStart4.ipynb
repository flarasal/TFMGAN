{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefe2b6d-ee4c-44a3-b1f2-2dda2ca33fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade19a53-9154-49af-a8d1-6136ad278adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Definir el generador y el discriminador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "                    \n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "# Leer datos\n",
    "expression_data = pd.read_csv('exprTRAIN.csv')\n",
    "methylation_data = pd.read_csv('methylTRAIN.csv')\n",
    "assign_data = pd.read_csv('assignTRAIN.csv')\n",
    "\n",
    "expression_data = expression_data.iloc[:, 1:]\n",
    "methylation_data = methylation_data.iloc[:, 1:]\n",
    "assign_data = assign_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data = methylation_data.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data = assign_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expression_data.fillna(0, inplace=True)\n",
    "methylation_data.fillna(0, inplace=True)\n",
    "assign_data.fillna(0, inplace=True)\n",
    "\n",
    "expression_data = expression_data.values\n",
    "methylation_data = methylation_data.values\n",
    "assign_data=assign_data.values\n",
    "# Asegurar que tienes el mismo número de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data.shape[0] == methylation_data.shape[0], \"Los datos de expresión y metilación deben tener el mismo número de muestras.\"\n",
    "assert assign_data.shape[0] == methylation_data.shape[0], \"Los datos de asignación y metilación deben tener el mismo número de muestras.\"\n",
    "\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "expression_data = torch.FloatTensor(expression_data).to(device)\n",
    "methylation_data = torch.FloatTensor(methylation_data).to(device)\n",
    "assign_data = torch.FloatTensor(assign_data).to(device)\n",
    "combined_data = torch.cat((expression_data,methylation_data,assign_data), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000 | Disc Loss: 0.8517762422561646 | Gen Loss: 133.36300659179688| Gan Loss: 2.2126705646514893| Mse Loss: 145.72259521484375\n",
      "El código tardó 36.10115 segundos en ejecutarse.\n",
      "Epoch 200/10000 | Disc Loss: 0.49818575382232666 | Gen Loss: 122.342041015625| Gan Loss: 3.3783161640167236| Mse Loss: 132.18191528320312\n",
      "El código tardó 32.26716 segundos en ejecutarse.\n",
      "Epoch 300/10000 | Disc Loss: 1.0749439001083374 | Gen Loss: 106.70753479003906| Gan Loss: 4.975368022918701| Mse Loss: 113.03575134277344\n",
      "El código tardó 26.11014 segundos en ejecutarse.\n",
      "Epoch 400/10000 | Disc Loss: 0.23614604771137238 | Gen Loss: 99.58875274658203| Gan Loss: 5.688350200653076| Mse Loss: 104.33378601074219\n",
      "El código tardó 28.64194 segundos en ejecutarse.\n",
      "Epoch 500/10000 | Disc Loss: 0.09342308342456818 | Gen Loss: 90.9825210571289| Gan Loss: 5.252890586853027| Mse Loss: 95.2551498413086\n",
      "El código tardó 28.15149 segundos en ejecutarse.\n",
      "Epoch 600/10000 | Disc Loss: 0.14721158146858215 | Gen Loss: 89.90618896484375| Gan Loss: 5.696438789367676| Mse Loss: 93.56639099121094\n",
      "El código tardó 21.68286 segundos en ejecutarse.\n",
      "Epoch 700/10000 | Disc Loss: 0.1203579232096672 | Gen Loss: 84.83779907226562| Gan Loss: 6.024641036987305| Mse Loss: 87.57017517089844\n",
      "El código tardó 11.37928 segundos en ejecutarse.\n",
      "Epoch 800/10000 | Disc Loss: 0.2194744050502777 | Gen Loss: 81.10885620117188| Gan Loss: 5.179952621459961| Mse Loss: 84.36544799804688\n",
      "El código tardó 23.40970 segundos en ejecutarse.\n",
      "Epoch 900/10000 | Disc Loss: 0.25668036937713623 | Gen Loss: 78.95944213867188| Gan Loss: 6.809709548950195| Mse Loss: 80.16637420654297\n",
      "El código tardó 33.45511 segundos en ejecutarse.\n",
      "Epoch 1000/10000 | Disc Loss: 0.19496798515319824 | Gen Loss: 74.1919174194336| Gan Loss: 5.914484024047852| Mse Loss: 75.86381530761719\n",
      "El código tardó 32.99370 segundos en ejecutarse.\n",
      "Epoch 1100/10000 | Disc Loss: 0.13838723301887512 | Gen Loss: 72.4288558959961| Gan Loss: 6.339284420013428| Mse Loss: 73.43285369873047\n",
      "El código tardó 11.98167 segundos en ejecutarse.\n",
      "Epoch 1200/10000 | Disc Loss: 0.17393778264522552 | Gen Loss: 72.76789855957031| Gan Loss: 6.323049545288086| Mse Loss: 73.82760620117188\n",
      "El código tardó 32.87153 segundos en ejecutarse.\n",
      "Epoch 1300/10000 | Disc Loss: 0.5918251276016235 | Gen Loss: 69.82594299316406| Gan Loss: 5.911374092102051| Mse Loss: 71.01618957519531\n",
      "El código tardó 34.11602 segundos en ejecutarse.\n",
      "Epoch 1400/10000 | Disc Loss: 0.21622848510742188 | Gen Loss: 69.21990966796875| Gan Loss: 5.302403450012207| Mse Loss: 71.01945495605469\n",
      "El código tardó 33.80356 segundos en ejecutarse.\n",
      "Epoch 1500/10000 | Disc Loss: 0.12285459041595459 | Gen Loss: 69.9059066772461| Gan Loss: 6.184576511383057| Mse Loss: 70.80148315429688\n",
      "El código tardó 27.00150 segundos en ejecutarse.\n",
      "Epoch 1600/10000 | Disc Loss: 0.06298307329416275 | Gen Loss: 69.06185150146484| Gan Loss: 7.44516658782959| Mse Loss: 68.46298217773438\n",
      "El código tardó 30.61202 segundos en ejecutarse.\n",
      "Epoch 1700/10000 | Disc Loss: 0.06715548038482666 | Gen Loss: 73.78793334960938| Gan Loss: 7.92633581161499| Mse Loss: 73.17955017089844\n",
      "El código tardó 28.34260 segundos en ejecutarse.\n",
      "Epoch 1800/10000 | Disc Loss: 0.058467425405979156 | Gen Loss: 68.26573944091797| Gan Loss: 7.633164405822754| Mse Loss: 67.3695297241211\n",
      "El código tardó 28.13066 segundos en ejecutarse.\n",
      "Epoch 1900/10000 | Disc Loss: 0.0390336811542511 | Gen Loss: 68.12763977050781| Gan Loss: 7.63031530380249| Mse Loss: 67.21925354003906\n",
      "El código tardó 26.63727 segundos en ejecutarse.\n",
      "Epoch 2000/10000 | Disc Loss: 0.07657667249441147 | Gen Loss: 68.71537017822266| Gan Loss: 9.54421615600586| Mse Loss: 65.7457275390625\n",
      "El código tardó 29.64083 segundos en ejecutarse.\n",
      "Epoch 2100/10000 | Disc Loss: 0.3408472537994385 | Gen Loss: 67.84538269042969| Gan Loss: 8.715620994567871| Mse Loss: 65.69973754882812\n",
      "El código tardó 32.07609 segundos en ejecutarse.\n",
      "Epoch 2200/10000 | Disc Loss: 0.038331080228090286 | Gen Loss: 66.34227752685547| Gan Loss: 7.778080463409424| Mse Loss: 65.07132720947266\n",
      "El código tardó 33.31570 segundos en ejecutarse.\n",
      "Epoch 2300/10000 | Disc Loss: 0.4371602535247803 | Gen Loss: 61.48735046386719| Gan Loss: 4.304457187652588| Mse Loss: 63.53654861450195\n",
      "El código tardó 34.31160 segundos en ejecutarse.\n",
      "Epoch 2400/10000 | Disc Loss: 0.06834228336811066 | Gen Loss: 68.85929870605469| Gan Loss: 9.13742733001709| Mse Loss: 66.35763549804688\n",
      "El código tardó 32.43261 segundos en ejecutarse.\n",
      "Epoch 2500/10000 | Disc Loss: 0.6722896099090576 | Gen Loss: 62.45167541503906| Gan Loss: 5.219538688659668| Mse Loss: 63.59126281738281\n",
      "El código tardó 34.07356 segundos en ejecutarse.\n",
      "Epoch 2600/10000 | Disc Loss: 0.006585641298443079 | Gen Loss: 68.1797866821289| Gan Loss: 10.55271053314209| Mse Loss: 64.03009033203125\n",
      "El código tardó 33.55774 segundos en ejecutarse.\n",
      "Epoch 2700/10000 | Disc Loss: 0.0042295875027775764 | Gen Loss: 74.92727661132812| Gan Loss: 9.86596965789795| Mse Loss: 72.29034423828125\n",
      "El código tardó 24.42604 segundos en ejecutarse.\n",
      "Epoch 2800/10000 | Disc Loss: 0.007817360572516918 | Gen Loss: 71.17366790771484| Gan Loss: 11.293535232543945| Mse Loss: 66.53348541259766\n",
      "El código tardó 33.44255 segundos en ejecutarse.\n",
      "Epoch 2900/10000 | Disc Loss: 0.019988475367426872 | Gen Loss: 71.76691436767578| Gan Loss: 10.664850234985352| Mse Loss: 67.89118194580078\n",
      "El código tardó 33.29223 segundos en ejecutarse.\n",
      "Epoch 3000/10000 | Disc Loss: 0.9573895931243896 | Gen Loss: 68.5823745727539| Gan Loss: 7.016735553741455| Mse Loss: 68.40626525878906\n",
      "El código tardó 32.68665 segundos en ejecutarse.\n",
      "Epoch 3100/10000 | Disc Loss: 0.01866826042532921 | Gen Loss: 64.79535675048828| Gan Loss: 7.665703296661377| Mse Loss: 63.47739028930664\n",
      "El código tardó 32.48233 segundos en ejecutarse.\n",
      "Epoch 3200/10000 | Disc Loss: 0.1848863959312439 | Gen Loss: 67.6866683959961| Gan Loss: 8.295949935913086| Mse Loss: 65.98968505859375\n",
      "El código tardó 34.07074 segundos en ejecutarse.\n",
      "Epoch 3300/10000 | Disc Loss: 0.10281117260456085 | Gen Loss: 67.1093521118164| Gan Loss: 10.286677360534668| Mse Loss: 63.13630676269531\n",
      "El código tardó 34.26956 segundos en ejecutarse.\n",
      "Epoch 3400/10000 | Disc Loss: 0.18349885940551758 | Gen Loss: 68.8564224243164| Gan Loss: 9.813921928405762| Mse Loss: 65.602783203125\n",
      "El código tardó 33.84417 segundos en ejecutarse.\n",
      "Epoch 3500/10000 | Disc Loss: 0.051435600966215134 | Gen Loss: 65.56529998779297| Gan Loss: 10.131063461303711| Mse Loss: 61.59359359741211\n",
      "El código tardó 34.10231 segundos en ejecutarse.\n",
      "Epoch 3600/10000 | Disc Loss: 0.3130005896091461 | Gen Loss: 64.72428131103516| Gan Loss: 6.586656093597412| Mse Loss: 64.59736633300781\n",
      "El código tardó 34.34628 segundos en ejecutarse.\n",
      "Epoch 3700/10000 | Disc Loss: 0.028366323560476303 | Gen Loss: 73.74494934082031| Gan Loss: 12.398077011108398| Mse Loss: 68.16319274902344\n",
      "El código tardó 33.04413 segundos en ejecutarse.\n",
      "Epoch 3800/10000 | Disc Loss: 0.009763062000274658 | Gen Loss: 67.58656311035156| Gan Loss: 9.715289115905762| Mse Loss: 64.30142211914062\n",
      "El código tardó 33.55172 segundos en ejecutarse.\n",
      "Epoch 3900/10000 | Disc Loss: 0.010614718310534954 | Gen Loss: 67.01868438720703| Gan Loss: 10.330575942993164| Mse Loss: 62.986785888671875\n",
      "El código tardó 32.70405 segundos en ejecutarse.\n",
      "Epoch 4000/10000 | Disc Loss: 0.005890310741961002 | Gen Loss: 70.3868408203125| Gan Loss: 13.074166297912598| Mse Loss: 63.68075180053711\n",
      "El código tardó 33.72235 segundos en ejecutarse.\n",
      "Epoch 4100/10000 | Disc Loss: 0.007987044751644135 | Gen Loss: 70.37142944335938| Gan Loss: 12.004526138305664| Mse Loss: 64.85211944580078\n",
      "El código tardó 33.29813 segundos en ejecutarse.\n",
      "Epoch 4200/10000 | Disc Loss: 0.07473805546760559 | Gen Loss: 69.8629379272461| Gan Loss: 10.621031761169434| Mse Loss: 65.8243408203125\n",
      "El código tardó 29.85229 segundos en ejecutarse.\n",
      "Epoch 4300/10000 | Disc Loss: 0.00986207090318203 | Gen Loss: 71.39488220214844| Gan Loss: 11.471744537353516| Mse Loss: 66.58126068115234\n",
      "El código tardó 23.95689 segundos en ejecutarse.\n",
      "Epoch 4400/10000 | Disc Loss: 0.017473559826612473 | Gen Loss: 67.28915405273438| Gan Loss: 11.326769828796387| Mse Loss: 62.18042755126953\n",
      "El código tardó 33.11770 segundos en ejecutarse.\n",
      "Epoch 4500/10000 | Disc Loss: 0.0653618723154068 | Gen Loss: 64.67327117919922| Gan Loss: 9.604028701782227| Mse Loss: 61.18804931640625\n",
      "El código tardó 33.68078 segundos en ejecutarse.\n",
      "Epoch 4600/10000 | Disc Loss: 0.021083008497953415 | Gen Loss: 69.37972259521484| Gan Loss: 12.356720924377441| Mse Loss: 63.35889434814453\n",
      "El código tardó 33.87886 segundos en ejecutarse.\n",
      "Epoch 4700/10000 | Disc Loss: 0.03991180658340454 | Gen Loss: 66.56089782714844| Gan Loss: 11.371959686279297| Mse Loss: 61.321041107177734\n",
      "El código tardó 33.46543 segundos en ejecutarse.\n",
      "Epoch 4800/10000 | Disc Loss: 0.013408612459897995 | Gen Loss: 67.07949829101562| Gan Loss: 12.374719619750977| Mse Loss: 60.7830924987793\n",
      "El código tardó 33.94285 segundos en ejecutarse.\n",
      "Epoch 4900/10000 | Disc Loss: 0.11864957958459854 | Gen Loss: 67.16907501220703| Gan Loss: 10.149486541748047| Mse Loss: 63.355098724365234\n",
      "El código tardó 33.97549 segundos en ejecutarse.\n",
      "Epoch 5000/10000 | Disc Loss: 0.0023792185820639133 | Gen Loss: 69.95330810546875| Gan Loss: 13.319635391235352| Mse Loss: 62.926307678222656\n",
      "El código tardó 33.73240 segundos en ejecutarse.\n",
      "Epoch 5100/10000 | Disc Loss: 0.1176438182592392 | Gen Loss: 62.736534118652344| Gan Loss: 6.650399684906006| Mse Loss: 62.317928314208984\n",
      "El código tardó 33.87691 segundos en ejecutarse.\n",
      "Epoch 5200/10000 | Disc Loss: 0.002567145274952054 | Gen Loss: 69.57386779785156| Gan Loss: 11.820746421813965| Mse Loss: 64.17013549804688\n",
      "El código tardó 33.62681 segundos en ejecutarse.\n",
      "Epoch 5300/10000 | Disc Loss: 0.16128914058208466 | Gen Loss: 66.61990356445312| Gan Loss: 10.570377349853516| Mse Loss: 62.27724838256836\n",
      "El código tardó 33.71014 segundos en ejecutarse.\n",
      "Epoch 5400/10000 | Disc Loss: 0.01397315040230751 | Gen Loss: 66.76998138427734| Gan Loss: 10.243158340454102| Mse Loss: 62.807586669921875\n",
      "El código tardó 33.90383 segundos en ejecutarse.\n",
      "Epoch 5500/10000 | Disc Loss: 0.03488690406084061 | Gen Loss: 65.98981475830078| Gan Loss: 10.547574996948242| Mse Loss: 61.60249328613281\n",
      "El código tardó 34.05564 segundos en ejecutarse.\n",
      "Epoch 5600/10000 | Disc Loss: 0.001274441834539175 | Gen Loss: 67.60799407958984| Gan Loss: 12.738076210021973| Mse Loss: 60.96657180786133\n",
      "El código tardó 33.63423 segundos en ejecutarse.\n",
      "Epoch 5700/10000 | Disc Loss: 0.006398764904588461 | Gen Loss: 65.69405364990234| Gan Loss: 11.675431251525879| Mse Loss: 60.02069091796875\n",
      "El código tardó 33.89035 segundos en ejecutarse.\n",
      "Epoch 5800/10000 | Disc Loss: 0.0013923055958002806 | Gen Loss: 69.30236053466797| Gan Loss: 11.917943000793457| Mse Loss: 63.76046371459961\n",
      "El código tardó 33.90224 segundos en ejecutarse.\n",
      "Epoch 5900/10000 | Disc Loss: 0.006359109655022621 | Gen Loss: 65.63513946533203| Gan Loss: 10.923242568969727| Mse Loss: 60.79100036621094\n",
      "El código tardó 18.27004 segundos en ejecutarse.\n",
      "Epoch 6000/10000 | Disc Loss: 0.0007698335684835911 | Gen Loss: 67.70591735839844| Gan Loss: 13.448925018310547| Mse Loss: 60.285552978515625\n",
      "El código tardó 33.77996 segundos en ejecutarse.\n",
      "Epoch 6100/10000 | Disc Loss: 0.09800173342227936 | Gen Loss: 68.78269958496094| Gan Loss: 11.03604507446289| Mse Loss: 64.16295623779297\n",
      "El código tardó 29.19005 segundos en ejecutarse.\n",
      "Epoch 6200/10000 | Disc Loss: 0.001296258415095508 | Gen Loss: 68.11100006103516| Gan Loss: 12.276322364807129| Mse Loss: 62.03852844238281\n",
      "El código tardó 33.74366 segundos en ejecutarse.\n",
      "Epoch 6300/10000 | Disc Loss: 0.006814001128077507 | Gen Loss: 66.9111099243164| Gan Loss: 11.715828895568848| Mse Loss: 61.328086853027344\n",
      "El código tardó 33.29197 segundos en ejecutarse.\n",
      "Epoch 6400/10000 | Disc Loss: 0.02196323871612549 | Gen Loss: 65.61735534667969| Gan Loss: 11.6143217086792| Mse Loss: 60.00337219238281\n",
      "El código tardó 33.27133 segundos en ejecutarse.\n",
      "Epoch 6500/10000 | Disc Loss: 0.0016872423002496362 | Gen Loss: 73.67135620117188| Gan Loss: 16.284656524658203| Mse Loss: 63.762996673583984\n",
      "El código tardó 15.90203 segundos en ejecutarse.\n",
      "Epoch 6600/10000 | Disc Loss: 0.04363413155078888 | Gen Loss: 65.2428207397461| Gan Loss: 12.910799980163574| Mse Loss: 58.14669418334961\n",
      "El código tardó 12.12065 segundos en ejecutarse.\n",
      "Epoch 6700/10000 | Disc Loss: 2.2281806468963623 | Gen Loss: 57.02466583251953| Gan Loss: 2.2231435775756836| Mse Loss: 60.89057922363281\n",
      "El código tardó 28.10091 segundos en ejecutarse.\n",
      "Epoch 6800/10000 | Disc Loss: 0.002446084748953581 | Gen Loss: 67.48171997070312| Gan Loss: 13.799235343933105| Mse Loss: 59.64720916748047\n",
      "El código tardó 34.30852 segundos en ejecutarse.\n",
      "Epoch 6900/10000 | Disc Loss: 0.0024121326860040426 | Gen Loss: 66.10054016113281| Gan Loss: 11.954174995422363| Mse Loss: 60.16263198852539\n",
      "El código tardó 33.58599 segundos en ejecutarse.\n",
      "Epoch 7000/10000 | Disc Loss: 0.0022029769606888294 | Gen Loss: 63.96376037597656| Gan Loss: 11.288622856140137| Mse Loss: 58.527931213378906\n",
      "El código tardó 31.08625 segundos en ejecutarse.\n",
      "Epoch 7100/10000 | Disc Loss: 0.0015576056903228164 | Gen Loss: 67.14596557617188| Gan Loss: 12.5016508102417| Mse Loss: 60.715904235839844\n",
      "El código tardó 33.37136 segundos en ejecutarse.\n",
      "Epoch 7200/10000 | Disc Loss: 0.0037829112261533737 | Gen Loss: 68.09716033935547| Gan Loss: 12.8727445602417| Mse Loss: 61.36046600341797\n",
      "El código tardó 32.61394 segundos en ejecutarse.\n",
      "Epoch 7300/10000 | Disc Loss: 0.2752887010574341 | Gen Loss: 64.27716064453125| Gan Loss: 10.486328125| Mse Loss: 59.76759719848633\n",
      "El código tardó 32.78908 segundos en ejecutarse.\n",
      "Epoch 7400/10000 | Disc Loss: 0.045724090188741684 | Gen Loss: 65.5898208618164| Gan Loss: 12.214694023132324| Mse Loss: 59.305702209472656\n",
      "El código tardó 31.29670 segundos en ejecutarse.\n",
      "Epoch 7500/10000 | Disc Loss: 0.2511245310306549 | Gen Loss: 63.55643081665039| Gan Loss: 10.583683013916016| Mse Loss: 58.858612060546875\n",
      "El código tardó 32.73758 segundos en ejecutarse.\n",
      "Epoch 7600/10000 | Disc Loss: 0.1314123272895813 | Gen Loss: 66.65743255615234| Gan Loss: 9.900607109069824| Mse Loss: 63.063140869140625\n",
      "El código tardó 33.65288 segundos en ejecutarse.\n",
      "Epoch 7700/10000 | Disc Loss: 0.017630238085985184 | Gen Loss: 66.20365905761719| Gan Loss: 12.984186172485352| Mse Loss: 59.13275146484375\n",
      "El código tardó 34.18512 segundos en ejecutarse.\n",
      "Epoch 7800/10000 | Disc Loss: 0.01840219832956791 | Gen Loss: 65.93470764160156| Gan Loss: 11.609067916870117| Mse Loss: 60.361820220947266\n",
      "El código tardó 33.88590 segundos en ejecutarse.\n",
      "Epoch 7900/10000 | Disc Loss: 0.009345983155071735 | Gen Loss: 67.56523895263672| Gan Loss: 13.07544231414795| Mse Loss: 60.544219970703125\n",
      "El código tardó 33.82398 segundos en ejecutarse.\n",
      "Epoch 8000/10000 | Disc Loss: 0.0014731243718415499 | Gen Loss: 69.03450775146484| Gan Loss: 14.315914154052734| Mse Loss: 60.798439025878906\n",
      "El código tardó 14.68266 segundos en ejecutarse.\n",
      "Epoch 8100/10000 | Disc Loss: 0.0014949957840144634 | Gen Loss: 69.94149780273438| Gan Loss: 15.853991508483887| Mse Loss: 60.09722900390625\n",
      "El código tardó 26.13399 segundos en ejecutarse.\n",
      "Epoch 8200/10000 | Disc Loss: 0.025614526122808456 | Gen Loss: 66.94893646240234| Gan Loss: 13.691916465759277| Mse Loss: 59.174468994140625\n",
      "El código tardó 33.87610 segundos en ejecutarse.\n",
      "Epoch 8300/10000 | Disc Loss: 0.0007071398431435227 | Gen Loss: 66.64088439941406| Gan Loss: 12.77487850189209| Mse Loss: 59.85111999511719\n",
      "El código tardó 31.87304 segundos en ejecutarse.\n",
      "Epoch 8400/10000 | Disc Loss: 0.0017052273033186793 | Gen Loss: 66.4290542602539| Gan Loss: 12.52124309539795| Mse Loss: 59.89756774902344\n",
      "El código tardó 24.63625 segundos en ejecutarse.\n",
      "Epoch 8500/10000 | Disc Loss: 0.0006457238341681659 | Gen Loss: 70.35792541503906| Gan Loss: 14.494400024414062| Mse Loss: 62.07059097290039\n",
      "El código tardó 33.56141 segundos en ejecutarse.\n",
      "Epoch 8600/10000 | Disc Loss: 0.0005451372126117349 | Gen Loss: 66.07505798339844| Gan Loss: 13.852849960327148| Mse Loss: 58.02467727661133\n",
      "El código tardó 34.06527 segundos en ejecutarse.\n",
      "Epoch 8700/10000 | Disc Loss: 1.6233575344085693 | Gen Loss: 63.855682373046875| Gan Loss: 11.644658088684082| Mse Loss: 58.01225280761719\n",
      "El código tardó 33.66378 segundos en ejecutarse.\n",
      "Epoch 8800/10000 | Disc Loss: 0.03290155529975891 | Gen Loss: 66.12139892578125| Gan Loss: 14.000109672546387| Mse Loss: 57.91254425048828\n",
      "El código tardó 33.15050 segundos en ejecutarse.\n",
      "Epoch 8900/10000 | Disc Loss: 0.004662014078348875 | Gen Loss: 66.2528076171875| Gan Loss: 12.579843521118164| Mse Loss: 59.63663101196289\n",
      "El código tardó 33.05903 segundos en ejecutarse.\n",
      "Epoch 9000/10000 | Disc Loss: 0.004219915252178907 | Gen Loss: 65.35884857177734| Gan Loss: 13.050854682922363| Mse Loss: 58.1199951171875\n",
      "El código tardó 34.02539 segundos en ejecutarse.\n",
      "Epoch 9100/10000 | Disc Loss: 0.0013638227246701717 | Gen Loss: 67.212890625| Gan Loss: 14.004571914672852| Mse Loss: 59.120357513427734\n",
      "El código tardó 34.37843 segundos en ejecutarse.\n",
      "Epoch 9200/10000 | Disc Loss: 0.18563325703144073 | Gen Loss: 68.80123138427734| Gan Loss: 15.140865325927734| Mse Loss: 59.62263107299805\n",
      "El código tardó 33.83627 segundos en ejecutarse.\n",
      "Epoch 9300/10000 | Disc Loss: 0.06257154047489166 | Gen Loss: 65.50704956054688| Gan Loss: 12.5507230758667| Mse Loss: 58.840362548828125\n",
      "El código tardó 33.74189 segundos en ejecutarse.\n",
      "Epoch 9400/10000 | Disc Loss: 0.0252360962331295 | Gen Loss: 62.983543395996094| Gan Loss: 10.57624340057373| Mse Loss: 58.2303352355957\n",
      "El código tardó 33.76579 segundos en ejecutarse.\n",
      "Epoch 9500/10000 | Disc Loss: 0.0032039624638855457 | Gen Loss: 66.48750305175781| Gan Loss: 12.661076545715332| Mse Loss: 59.80714416503906\n",
      "El código tardó 33.65403 segundos en ejecutarse.\n",
      "Epoch 9600/10000 | Disc Loss: 0.008808246813714504 | Gen Loss: 65.24896240234375| Gan Loss: 12.385784149169922| Mse Loss: 58.73686218261719\n",
      "El código tardó 32.79913 segundos en ejecutarse.\n",
      "Epoch 9700/10000 | Disc Loss: 0.00135699100792408 | Gen Loss: 66.39891052246094| Gan Loss: 13.679866790771484| Mse Loss: 58.57672119140625\n",
      "El código tardó 32.90294 segundos en ejecutarse.\n",
      "Epoch 9800/10000 | Disc Loss: 0.001782449777238071 | Gen Loss: 67.358154296875| Gan Loss: 13.597052574157715| Mse Loss: 59.73455810546875\n",
      "El código tardó 32.02815 segundos en ejecutarse.\n",
      "Epoch 9900/10000 | Disc Loss: 0.019420623779296875 | Gen Loss: 64.63229370117188| Gan Loss: 12.25875473022461| Mse Loss: 58.19282531738281\n",
      "El código tardó 33.90741 segundos en ejecutarse.\n",
      "Epoch 10000/10000 | Disc Loss: 0.0018797792727127671 | Gen Loss: 69.39983367919922| Gan Loss: 15.306209564208984| Mse Loss: 60.104026794433594\n",
      "El código tardó 33.67781 segundos en ejecutarse.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modelos y optimizadores\n",
    "gen = Generator(expression_data.shape[1], methylation_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "disc = Discriminator(expression_data.shape[1] + methylation_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "gen_optimizer = optim.RMSprop(gen.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.RMSprop(disc.parameters(), lr=0.0002)\n",
    "criterion = nn.BCELoss()\n",
    "mse_criterion = nn.MSELoss(reduction='sum')\n",
    "# Entrenamiento\n",
    "n_samples = combined_data.size(0)\n",
    "batch_size = 64\n",
    "n_epochs = 10000\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for idx in range(0, n_samples, batch_size):\n",
    "        real_data = combined_data[idx:idx+batch_size].to(device)\n",
    "        current_batch_size = real_data.size(0)\n",
    "        real_labels = torch.ones(current_batch_size, 1).to(device)\n",
    "\n",
    "        #noise = torch.randn(current_batch_size, 100).to(device)\n",
    "        noise = expression_data[idx:idx+batch_size]\n",
    "        \n",
    "        fake_methyl = gen(noise).to(device)\n",
    "        #fake_data = torch.cat((expression_data[idx:idx+batch_size],fake_methyl), 1)\n",
    "\n",
    "        fake_data = torch.cat((expression_data[idx:idx+batch_size],fake_methyl ), 1).to(device)\n",
    "\n",
    "        fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
    "        \n",
    "        # Entrenar discriminador\n",
    "        disc_optimizer.zero_grad()\n",
    "\n",
    "        real_preds = disc(real_data).to(device)\n",
    "        real_loss = criterion(real_preds, real_labels)\n",
    "        \n",
    "        fake_preds = disc(fake_data).to(device)\n",
    "        fake_loss = criterion(fake_preds, fake_labels)\n",
    "\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Entrenar generador\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        #noise = torch.randn(current_batch_size, 100).to(device)\n",
    "        noise = expression_data[idx:idx+batch_size]\n",
    "        fake_methyl = gen(noise).to(device)\n",
    "        fake_data = torch.cat((expression_data[idx:idx+batch_size],fake_methyl ), 1).to(device)\n",
    "        fake_preds = disc(fake_data).to(device)\n",
    "\n",
    "\n",
    "        # Loss basado en la capacidad de engañar al discriminador\n",
    "        gan_loss = criterion(fake_preds, real_labels)\n",
    "\n",
    "        # MSE loss entre el metilación generado y el metilación real\n",
    "        mse_loss = mse_criterion(fake_methyl,  torch.cat((methylation_data[idx:idx+batch_size],assign_data[idx:idx+batch_size]),1).to(device))\n",
    "\n",
    "        # Combina ambos losses. El coeficiente 'alpha' permite ponderar la importancia relativa de cada loss.\n",
    "        alpha = 0.9\n",
    "        gen_loss = gan_loss + alpha * mse_loss\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Disc Loss: {disc_loss.item()} | Gen Loss: {gen_loss.item()}| Gan Loss: {gan_loss.item()}| Mse Loss: {mse_loss.item()}\")\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calcular la diferencia de tiempo\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"El código tardó {elapsed_time:.5f} segundos en ejecutarse.\")\n",
    "        start_time = time.time()\n",
    "\n",
    "torch.save(gen.state_dict(), 'generator_model4.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be72300-c585-4101-b1f9-9ff6cb527c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Error cuadrático medio entre datos generados y reales: 0.3650900721549988\n",
      "Media: 0.3579321503639221\n",
      "Fila 1: MSE = 0.31631356477737427\n",
      "Fila 2: MSE = 0.43797701597213745\n",
      "Fila 3: MSE = 0.3544849753379822\n",
      "Fila 4: MSE = 0.41427168250083923\n",
      "Fila 5: MSE = 0.409222811460495\n",
      "Fila 6: MSE = 0.3473685681819916\n",
      "Fila 7: MSE = 0.38940244913101196\n",
      "Fila 8: MSE = 0.3085019886493683\n",
      "Fila 9: MSE = 0.3276359438896179\n",
      "Fila 10: MSE = 0.2962554395198822\n",
      "Fila 11: MSE = 0.33425042033195496\n",
      "Fila 12: MSE = 0.37498101592063904\n",
      "Fila 13: MSE = 0.31900259852409363\n",
      "Fila 14: MSE = 0.33521366119384766\n",
      "Fila 15: MSE = 0.416235089302063\n",
      "Fila 16: MSE = 0.317575603723526\n",
      "Fila 17: MSE = 0.3094697594642639\n",
      "Fila 18: MSE = 0.36907801032066345\n",
      "Fila 19: MSE = 0.4620431959629059\n",
      "Fila 20: MSE = 0.3046186864376068\n",
      "Fila 21: MSE = 0.47924524545669556\n",
      "Fila 22: MSE = 0.3319769501686096\n",
      "Fila 23: MSE = 0.5178526639938354\n",
      "Fila 24: MSE = 0.313897043466568\n",
      "Fila 25: MSE = 0.2977640926837921\n",
      "Fila 26: MSE = 0.3935711681842804\n",
      "Fila 27: MSE = 0.33708539605140686\n",
      "Fila 28: MSE = 0.3128221333026886\n",
      "Fila 29: MSE = 0.4155406951904297\n",
      "Fila 30: MSE = 0.27534863352775574\n",
      "Fila 31: MSE = 0.34095296263694763\n",
      "Fila 32: MSE = 0.38697123527526855\n",
      "Fila 33: MSE = 0.3212031424045563\n",
      "Fila 34: MSE = 0.4146132469177246\n",
      "Fila 35: MSE = 0.33729758858680725\n",
      "Fila 36: MSE = 0.3128710687160492\n",
      "Fila 37: MSE = 0.36080503463745117\n",
      "Fila 38: MSE = 0.46296095848083496\n",
      "Fila 39: MSE = 0.3032851815223694\n",
      "Fila 40: MSE = 0.3250749111175537\n",
      "Fila 41: MSE = 0.3352621793746948\n",
      "Fila 42: MSE = 0.3222222626209259\n",
      "Fila 43: MSE = 0.3184208869934082\n",
      "Fila 44: MSE = 0.32924458384513855\n",
      "Fila 45: MSE = 0.3808889091014862\n",
      "Fila 46: MSE = 0.3481636941432953\n",
      "Fila 47: MSE = 0.3193054795265198\n",
      "Fila 48: MSE = 0.339404433965683\n",
      "Fila 49: MSE = 0.3600234091281891\n",
      "Fila 50: MSE = 0.30480316281318665\n",
      "Fila 51: MSE = 0.4151785671710968\n",
      "Fila 52: MSE = 0.3373780846595764\n",
      "Fila 53: MSE = 0.5386481285095215\n",
      "Fila 54: MSE = 0.31395283341407776\n",
      "Fila 55: MSE = 0.31230491399765015\n",
      "Fila 56: MSE = 0.3593774437904358\n",
      "Fila 57: MSE = 0.34757333993911743\n",
      "Fila 58: MSE = 0.33633917570114136\n",
      "Fila 59: MSE = 0.3481364846229553\n",
      "Fila 60: MSE = 0.3530470132827759\n",
      "Fila 61: MSE = 0.3110692799091339\n",
      "Fila 62: MSE = 0.34426814317703247\n",
      "Fila 63: MSE = 0.31726983189582825\n",
      "Fila 64: MSE = 0.3345460891723633\n",
      "Fila 65: MSE = 0.3594617545604706\n",
      "Fila 66: MSE = 0.5577679872512817\n",
      "Fila 67: MSE = 0.3384222388267517\n",
      "Fila 68: MSE = 0.2785767912864685\n",
      "Fila 69: MSE = 0.31857407093048096\n",
      "Fila 70: MSE = 0.5021384954452515\n",
      "Fila 71: MSE = 0.3851621448993683\n",
      "Fila 72: MSE = 0.47149890661239624\n",
      "Fila 73: MSE = 0.49613434076309204\n",
      "Fila 74: MSE = 0.3832474648952484\n",
      "Fila 75: MSE = 0.47744250297546387\n",
      "Fila 76: MSE = 0.3298324644565582\n",
      "Fila 77: MSE = 0.40776410698890686\n",
      "Fila 78: MSE = 0.3475727438926697\n",
      "Fila 79: MSE = 0.36099401116371155\n",
      "Fila 80: MSE = 0.3293484151363373\n",
      "Fila 81: MSE = 0.30265605449676514\n",
      "Fila 82: MSE = 0.45605406165122986\n",
      "Fila 83: MSE = 0.331096887588501\n",
      "Fila 84: MSE = 0.45887160301208496\n",
      "Fila 85: MSE = 0.39431408047676086\n",
      "Fila 86: MSE = 0.3368723690509796\n",
      "Fila 87: MSE = 0.343148410320282\n",
      "Fila 88: MSE = 0.4001423418521881\n",
      "Fila 89: MSE = 0.32072171568870544\n",
      "Fila 90: MSE = 0.27561652660369873\n",
      "Fila 91: MSE = 0.28859642148017883\n",
      "Fila 92: MSE = 0.29364147782325745\n",
      "Fila 93: MSE = 0.5129743218421936\n",
      "Fila 94: MSE = 0.3668387234210968\n",
      "Fila 95: MSE = 0.2948800325393677\n",
      "Fila 96: MSE = 0.3305695950984955\n",
      "Fila 97: MSE = 0.34185704588890076\n",
      "Fila 98: MSE = 0.3242505192756653\n",
      "Fila 99: MSE = 0.3153538703918457\n",
      "Fila 100: MSE = 0.3421706259250641\n",
      "Fila 101: MSE = 0.34709233045578003\n",
      "Fila 102: MSE = 0.3857875466346741\n",
      "Fila 103: MSE = 0.30654004216194153\n",
      "Fila 104: MSE = 0.30740270018577576\n",
      "Fila 105: MSE = 0.34400826692581177\n",
      "Fila 106: MSE = 0.31379079818725586\n",
      "Fila 107: MSE = 0.32516756653785706\n",
      "Fila 108: MSE = 0.363208144903183\n",
      "Fila 109: MSE = 0.37514570355415344\n",
      "Fila 110: MSE = 0.30709412693977356\n",
      "Fila 111: MSE = 0.38120535016059875\n",
      "Fila 112: MSE = 0.35710012912750244\n",
      "Fila 113: MSE = 0.33280590176582336\n",
      "Fila 114: MSE = 0.29749542474746704\n",
      "Fila 115: MSE = 0.5588539838790894\n",
      "Fila 116: MSE = 0.3731072247028351\n",
      "Fila 117: MSE = 0.3183274567127228\n",
      "Fila 118: MSE = 0.2988268733024597\n",
      "Fila 119: MSE = 0.4466119110584259\n",
      "Fila 120: MSE = 0.44017472863197327\n",
      "Fila 121: MSE = 0.32146865129470825\n",
      "Fila 122: MSE = 0.38398388028144836\n",
      "Fila 123: MSE = 0.3090098202228546\n",
      "Fila 124: MSE = 0.33406662940979004\n",
      "Fila 125: MSE = 0.2990856170654297\n",
      "Fila 126: MSE = 0.33618927001953125\n",
      "Fila 127: MSE = 0.3854661285877228\n",
      "Fila 128: MSE = 0.3389105200767517\n",
      "Fila 129: MSE = 0.3744771480560303\n",
      "Fila 130: MSE = 0.35189735889434814\n",
      "Fila 131: MSE = 0.3517332077026367\n",
      "Fila 132: MSE = 0.43020084500312805\n",
      "Fila 133: MSE = 0.3603317439556122\n",
      "Fila 134: MSE = 0.4551638066768646\n",
      "Fila 135: MSE = 0.3853093087673187\n",
      "Fila 136: MSE = 0.29150334000587463\n",
      "Fila 137: MSE = 0.30348846316337585\n",
      "Fila 138: MSE = 0.3827531337738037\n",
      "Fila 139: MSE = 0.30426618456840515\n",
      "Fila 140: MSE = 0.42051684856414795\n",
      "Fila 141: MSE = 0.44103118777275085\n",
      "Fila 142: MSE = 0.31498950719833374\n",
      "Fila 143: MSE = 0.3664526045322418\n",
      "Fila 144: MSE = 0.33525773882865906\n",
      "Fila 145: MSE = 0.3470071852207184\n",
      "Fila 146: MSE = 0.3092363774776459\n",
      "Fila 147: MSE = 0.3100730776786804\n",
      "Fila 148: MSE = 0.34250348806381226\n",
      "Fila 149: MSE = 0.33529233932495117\n",
      "Fila 150: MSE = 0.4984606206417084\n",
      "Fila 151: MSE = 0.5365118980407715\n",
      "Fila 152: MSE = 0.3465772867202759\n",
      "Fila 153: MSE = 0.4664309620857239\n",
      "Fila 154: MSE = 0.3918796181678772\n",
      "Fila 155: MSE = 0.39407747983932495\n",
      "Fila 156: MSE = 0.361288458108902\n",
      "Fila 157: MSE = 0.4226881265640259\n",
      "Fila 158: MSE = 0.3720068037509918\n",
      "Fila 159: MSE = 0.37446466088294983\n",
      "Fila 160: MSE = 0.49184203147888184\n",
      "Fila 161: MSE = 0.3557412624359131\n",
      "Fila 162: MSE = 0.39248529076576233\n",
      "Fila 163: MSE = 0.3235844075679779\n",
      "Fila 164: MSE = 0.369208961725235\n",
      "Fila 165: MSE = 0.33751076459884644\n",
      "Fila 166: MSE = 0.39989566802978516\n",
      "Fila 167: MSE = 0.39652401208877563\n",
      "Fila 168: MSE = 0.3461528420448303\n",
      "Fila 169: MSE = 0.4073371887207031\n",
      "Fila 170: MSE = 0.38672128319740295\n",
      "Fila 171: MSE = 0.3481971025466919\n",
      "Fila 172: MSE = 0.36191293597221375\n",
      "Fila 173: MSE = 0.31825533509254456\n",
      "Fila 174: MSE = 0.3880895674228668\n",
      "Fila 175: MSE = 0.29720965027809143\n",
      "Fila 176: MSE = 0.4203192889690399\n",
      "Fila 177: MSE = 0.3081071674823761\n",
      "Fila 178: MSE = 0.35217535495758057\n",
      "Fila 179: MSE = 0.3640356659889221\n",
      "Fila 180: MSE = 0.3689691722393036\n",
      "Fila 181: MSE = 0.3819487392902374\n",
      "Fila 182: MSE = 0.33697253465652466\n",
      "Fila 183: MSE = 0.5050124526023865\n",
      "Fila 184: MSE = 0.39820995926856995\n",
      "Fila 185: MSE = 0.33749639987945557\n",
      "Fila 186: MSE = 0.3826480209827423\n",
      "Fila 187: MSE = 0.43641090393066406\n",
      "Fila 188: MSE = 0.4045693874359131\n",
      "Fila 189: MSE = 0.3640091121196747\n",
      "Fila 190: MSE = 0.35653066635131836\n",
      "Fila 191: MSE = 0.3360569179058075\n",
      "Fila 192: MSE = 0.347930371761322\n",
      "Fila 193: MSE = 0.49406173825263977\n",
      "Fila 194: MSE = 0.3590274453163147\n",
      "Fila 195: MSE = 0.3390291631221771\n",
      "Fila 196: MSE = 0.3142662048339844\n",
      "Fila 197: MSE = 0.4006490409374237\n",
      "Fila 198: MSE = 0.30596476793289185\n",
      "Fila 199: MSE = 0.3060721158981323\n",
      "Fila 200: MSE = 0.3201781213283539\n",
      "Fila 201: MSE = 0.3831496238708496\n",
      "Fila 202: MSE = 0.3066023886203766\n",
      "Fila 203: MSE = 0.3534943163394928\n",
      "Fila 204: MSE = 0.40302774310112\n",
      "Fila 205: MSE = 0.30252113938331604\n",
      "Fila 206: MSE = 0.2911079227924347\n",
      "Fila 207: MSE = 0.33516550064086914\n",
      "Fila 208: MSE = 0.34983953833580017\n",
      "Fila 209: MSE = 0.2992597818374634\n",
      "Fila 210: MSE = 0.33416643738746643\n",
      "Fila 211: MSE = 0.3993191123008728\n",
      "Fila 212: MSE = 0.3812299072742462\n",
      "Fila 213: MSE = 0.3282572329044342\n",
      "Fila 214: MSE = 0.3160236179828644\n",
      "Fila 215: MSE = 0.3903377950191498\n",
      "Fila 216: MSE = 0.3196425437927246\n",
      "Fila 217: MSE = 0.275215208530426\n",
      "Fila 218: MSE = 0.3601054251194\n",
      "Fila 219: MSE = 0.4130370616912842\n",
      "Fila 220: MSE = 0.3212869167327881\n",
      "Fila 221: MSE = 0.4048987030982971\n",
      "Fila 222: MSE = 0.3792712986469269\n",
      "Fila 223: MSE = 0.41129055619239807\n",
      "Fila 224: MSE = 0.3481575846672058\n",
      "Fila 225: MSE = 0.42004427313804626\n",
      "Fila 226: MSE = 0.3556802570819855\n",
      "Fila 227: MSE = 0.3346678614616394\n",
      "Fila 228: MSE = 0.30022239685058594\n",
      "Fila 229: MSE = 0.29943814873695374\n",
      "Fila 230: MSE = 0.40739938616752625\n",
      "Fila 231: MSE = 0.33735573291778564\n",
      "Fila 232: MSE = 0.3462984263896942\n",
      "Fila 233: MSE = 0.452311635017395\n",
      "Fila 234: MSE = 0.3126397430896759\n",
      "Fila 235: MSE = 0.3667461574077606\n",
      "Fila 236: MSE = 0.3674606680870056\n",
      "Fila 237: MSE = 0.3969816267490387\n",
      "Fila 238: MSE = 0.3652079403400421\n",
      "Fila 239: MSE = 0.3618013262748718\n",
      "Fila 240: MSE = 0.40235599875450134\n",
      "Fila 241: MSE = 0.36122483015060425\n",
      "Fila 242: MSE = 0.3857419788837433\n",
      "Fila 243: MSE = 0.3229789435863495\n",
      "Fila 244: MSE = 0.4232105612754822\n",
      "Fila 245: MSE = 0.2648196816444397\n",
      "Fila 246: MSE = 0.329820454120636\n",
      "Fila 247: MSE = 0.37513962388038635\n",
      "Fila 248: MSE = 0.3061175048351288\n",
      "Fila 249: MSE = 0.303267240524292\n",
      "Fila 250: MSE = 0.4727708101272583\n",
      "Fila 251: MSE = 0.27285900712013245\n",
      "Fila 252: MSE = 0.3041379451751709\n",
      "Fila 253: MSE = 0.35009193420410156\n",
      "Fila 254: MSE = 0.3931106626987457\n",
      "Fila 255: MSE = 0.27465829253196716\n",
      "Fila 256: MSE = 0.3561750650405884\n",
      "Fila 257: MSE = 0.44331827759742737\n",
      "Fila 258: MSE = 0.40011242032051086\n",
      "Fila 259: MSE = 0.47787824273109436\n",
      "Fila 260: MSE = 0.38402798771858215\n",
      "Fila 261: MSE = 0.48693791031837463\n",
      "Fila 262: MSE = 0.32803505659103394\n",
      "Fila 263: MSE = 0.4690454304218292\n",
      "Fila 264: MSE = 0.437822550535202\n",
      "Fila 265: MSE = 0.3844716548919678\n",
      "Fila 266: MSE = 0.38656044006347656\n",
      "Fila 267: MSE = 0.38074833154678345\n",
      "Fila 268: MSE = 0.40848827362060547\n",
      "Fila 269: MSE = 0.33943814039230347\n",
      "Fila 270: MSE = 0.5168216824531555\n",
      "Fila 271: MSE = 0.31548118591308594\n",
      "Fila 272: MSE = 0.46187227964401245\n",
      "Fila 273: MSE = 0.354211688041687\n",
      "Fila 274: MSE = 0.4233396351337433\n",
      "Fila 275: MSE = 0.4047956168651581\n",
      "Fila 276: MSE = 0.3015412986278534\n",
      "Fila 277: MSE = 0.29526668787002563\n",
      "Fila 278: MSE = 0.3635643422603607\n",
      "Fila 279: MSE = 0.3317912518978119\n",
      "Fila 280: MSE = 0.2915891110897064\n",
      "Fila 281: MSE = 0.3277512788772583\n",
      "Fila 282: MSE = 0.36539608240127563\n",
      "Fila 283: MSE = 0.24876250326633453\n",
      "Fila 284: MSE = 0.3367599546909332\n",
      "Fila 285: MSE = 0.40053823590278625\n",
      "Fila 286: MSE = 0.41343867778778076\n",
      "Fila 287: MSE = 0.398252010345459\n",
      "Fila 288: MSE = 0.31408852338790894\n",
      "Fila 289: MSE = 0.40153566002845764\n",
      "Fila 290: MSE = 0.3225560784339905\n",
      "Fila 291: MSE = 0.30649545788764954\n",
      "Fila 292: MSE = 0.35148876905441284\n",
      "Fila 293: MSE = 0.325481116771698\n",
      "Fila 294: MSE = 0.3913867473602295\n",
      "Fila 295: MSE = 0.5879905819892883\n",
      "Fila 296: MSE = 0.47840243577957153\n",
      "Fila 297: MSE = 0.5515397191047668\n",
      "Fila 298: MSE = 0.381024569272995\n",
      "Fila 299: MSE = 0.38417932391166687\n",
      "Fila 300: MSE = 0.32090696692466736\n",
      "Fila 301: MSE = 0.3225952982902527\n",
      "Fila 302: MSE = 0.3344208002090454\n",
      "Fila 303: MSE = 0.29322877526283264\n",
      "Fila 304: MSE = 0.5382198691368103\n",
      "Fila 305: MSE = 0.3894943594932556\n",
      "Fila 306: MSE = 0.316473126411438\n",
      "Fila 307: MSE = 0.3098081946372986\n",
      "Fila 308: MSE = 0.3039597272872925\n",
      "Fila 309: MSE = 0.32206377387046814\n",
      "Fila 310: MSE = 0.3287729322910309\n",
      "Fila 311: MSE = 0.34687545895576477\n",
      "Fila 312: MSE = 0.3962417244911194\n",
      "Fila 313: MSE = 0.31075572967529297\n",
      "Fila 314: MSE = 0.48203349113464355\n",
      "Fila 315: MSE = 0.2920677065849304\n",
      "Fila 316: MSE = 0.33989304304122925\n",
      "Fila 317: MSE = 0.3729962408542633\n",
      "Fila 318: MSE = 0.3003700077533722\n",
      "Fila 319: MSE = 0.3802292048931122\n",
      "Fila 320: MSE = 0.31442973017692566\n",
      "Fila 321: MSE = 0.35309121012687683\n",
      "Fila 322: MSE = 0.3159428536891937\n",
      "Fila 323: MSE = 0.3729476034641266\n",
      "Fila 324: MSE = 0.4230230152606964\n",
      "Fila 325: MSE = 0.3535197675228119\n",
      "Fila 326: MSE = 0.29618701338768005\n",
      "Fila 327: MSE = 0.34104910492897034\n",
      "Fila 328: MSE = 0.2862165868282318\n",
      "Fila 329: MSE = 0.48217135667800903\n",
      "Fila 330: MSE = 0.383943647146225\n",
      "Fila 331: MSE = 0.421463280916214\n",
      "Fila 332: MSE = 0.4403754472732544\n",
      "Fila 333: MSE = 0.488830029964447\n",
      "Fila 334: MSE = 0.3482847809791565\n",
      "Fila 335: MSE = 0.3529447913169861\n",
      "Fila 336: MSE = 0.36389970779418945\n",
      "Fila 337: MSE = 0.3803473711013794\n",
      "Fila 338: MSE = 0.34754425287246704\n",
      "Fila 339: MSE = 0.3382927477359772\n",
      "Fila 340: MSE = 0.39486461877822876\n",
      "Fila 341: MSE = 0.31836453080177307\n",
      "Fila 342: MSE = 0.3571104109287262\n",
      "Fila 343: MSE = 0.3195722997188568\n",
      "Fila 344: MSE = 0.3322587013244629\n",
      "Fila 345: MSE = 0.3343721032142639\n",
      "Fila 346: MSE = 0.485639750957489\n",
      "Fila 347: MSE = 0.3673381805419922\n",
      "Fila 348: MSE = 0.3120653033256531\n",
      "Fila 349: MSE = 0.3511119484901428\n",
      "Fila 350: MSE = 0.36740344762802124\n",
      "Fila 351: MSE = 0.2875039279460907\n",
      "Fila 352: MSE = 0.345357209444046\n",
      "Fila 353: MSE = 0.3414526581764221\n",
      "Fila 354: MSE = 0.3160211443901062\n",
      "Fila 355: MSE = 0.3021491467952728\n",
      "Fila 356: MSE = 0.3123687207698822\n",
      "Fila 357: MSE = 0.4384478032588959\n",
      "Fila 358: MSE = 0.46009254455566406\n",
      "Fila 359: MSE = 0.41135266423225403\n",
      "Fila 360: MSE = 0.3568519055843353\n",
      "Fila 361: MSE = 0.37408846616744995\n",
      "Fila 362: MSE = 0.5905899405479431\n",
      "Fila 363: MSE = 0.38667231798171997\n",
      "Fila 364: MSE = 0.372103214263916\n",
      "Fila 365: MSE = 0.3160530626773834\n",
      "Fila 366: MSE = 0.3664088845252991\n",
      "Fila 367: MSE = 0.3455679714679718\n",
      "Fila 368: MSE = 0.3401082456111908\n",
      "Fila 369: MSE = 0.4794345498085022\n",
      "Fila 370: MSE = 0.3200175166130066\n",
      "Fila 371: MSE = 0.44509854912757874\n",
      "Fila 372: MSE = 0.3360404968261719\n",
      "Fila 373: MSE = 0.39952561259269714\n",
      "Fila 374: MSE = 0.4715762138366699\n",
      "Fila 375: MSE = 0.384156197309494\n",
      "Fila 376: MSE = 0.32861456274986267\n",
      "Fila 377: MSE = 0.3684481382369995\n",
      "Fila 378: MSE = 0.3515469431877136\n",
      "Fila 379: MSE = 0.31601017713546753\n",
      "Fila 380: MSE = 0.4026390612125397\n",
      "Fila 381: MSE = 0.3185216784477234\n",
      "Fila 382: MSE = 0.30675429105758667\n",
      "Fila 383: MSE = 0.3246120810508728\n",
      "Fila 384: MSE = 0.345976322889328\n",
      "Fila 385: MSE = 0.356517493724823\n",
      "Fila 386: MSE = 0.4593631625175476\n",
      "Fila 387: MSE = 0.34489405155181885\n",
      "Fila 388: MSE = 0.3352244794368744\n",
      "Fila 389: MSE = 0.32430195808410645\n",
      "Fila 390: MSE = 0.34641364216804504\n",
      "Fila 391: MSE = 0.29580655694007874\n",
      "Fila 392: MSE = 0.2872715890407562\n",
      "Fila 393: MSE = 0.42576831579208374\n",
      "Fila 394: MSE = 0.42151904106140137\n",
      "Fila 395: MSE = 0.3627913296222687\n",
      "Fila 396: MSE = 0.3569899797439575\n",
      "Fila 397: MSE = 0.3303190767765045\n",
      "Fila 398: MSE = 0.5122184157371521\n",
      "Fila 399: MSE = 0.36485207080841064\n",
      "Fila 400: MSE = 0.3546542227268219\n",
      "Fila 401: MSE = 0.26921093463897705\n",
      "Fila 402: MSE = 0.5096377730369568\n",
      "Fila 403: MSE = 0.3670163154602051\n",
      "Fila 404: MSE = 0.48666611313819885\n",
      "Fila 405: MSE = 0.3186150789260864\n",
      "Fila 406: MSE = 0.3213951289653778\n",
      "Fila 407: MSE = 0.33060336112976074\n",
      "Fila 408: MSE = 0.30723443627357483\n",
      "Fila 409: MSE = 0.35160404443740845\n",
      "Fila 410: MSE = 0.3432748019695282\n",
      "Fila 411: MSE = 0.30800601840019226\n",
      "Fila 412: MSE = 0.36531269550323486\n",
      "Fila 413: MSE = 0.3736729025840759\n",
      "Fila 414: MSE = 0.5054759979248047\n",
      "Fila 415: MSE = 0.3059649169445038\n",
      "Fila 416: MSE = 0.4246837794780731\n",
      "Fila 417: MSE = 0.29153814911842346\n",
      "Fila 418: MSE = 0.49567803740501404\n",
      "Fila 419: MSE = 0.3518195152282715\n",
      "Fila 420: MSE = 0.3387858271598816\n",
      "Fila 421: MSE = 0.3019509017467499\n",
      "Fila 422: MSE = 0.4354071617126465\n",
      "Fila 423: MSE = 0.29552850127220154\n",
      "Fila 424: MSE = 0.5048789381980896\n",
      "Fila 425: MSE = 0.37989410758018494\n",
      "Fila 426: MSE = 0.36741432547569275\n",
      "Fila 427: MSE = 0.34057480096817017\n",
      "Fila 428: MSE = 0.3453313708305359\n",
      "Fila 429: MSE = 0.36226820945739746\n",
      "Fila 430: MSE = 0.3406619131565094\n",
      "Fila 431: MSE = 0.31564798951148987\n",
      "Fila 432: MSE = 0.3238867223262787\n",
      "Fila 433: MSE = 0.3826604187488556\n",
      "Fila 434: MSE = 0.3875691890716553\n",
      "Fila 435: MSE = 0.46328604221343994\n",
      "Fila 436: MSE = 0.35610252618789673\n",
      "Fila 437: MSE = 0.32000595331192017\n",
      "Fila 438: MSE = 0.3099886476993561\n",
      "Fila 439: MSE = 0.2812640964984894\n",
      "Fila 440: MSE = 0.39748430252075195\n",
      "Fila 441: MSE = 0.3237854838371277\n",
      "Fila 442: MSE = 0.3314085900783539\n",
      "Fila 443: MSE = 0.31909582018852234\n",
      "Fila 444: MSE = 0.3192127048969269\n",
      "Fila 445: MSE = 0.4701501727104187\n",
      "Fila 446: MSE = 0.4456189274787903\n",
      "Fila 447: MSE = 0.33901742100715637\n",
      "Fila 448: MSE = 0.2794339060783386\n",
      "Fila 449: MSE = 0.3520051836967468\n",
      "Fila 450: MSE = 0.32016295194625854\n",
      "Fila 451: MSE = 0.32091110944747925\n",
      "Fila 452: MSE = 0.3615538477897644\n",
      "Fila 453: MSE = 0.3016997277736664\n",
      "Fila 454: MSE = 0.3642203211784363\n",
      "Fila 455: MSE = 0.45899930596351624\n",
      "Fila 456: MSE = 0.26447784900665283\n",
      "Fila 457: MSE = 0.4323256313800812\n",
      "Fila 458: MSE = 0.3535035252571106\n",
      "Fila 459: MSE = 0.5283682346343994\n",
      "Fila 460: MSE = 0.4602097272872925\n",
      "Fila 461: MSE = 0.3127024471759796\n",
      "Fila 462: MSE = 0.34701210260391235\n",
      "Fila 463: MSE = 0.3554818630218506\n",
      "Fila 464: MSE = 0.34841564297676086\n",
      "Fila 465: MSE = 0.5123116374015808\n",
      "Fila 466: MSE = 0.45380550622940063\n",
      "Fila 467: MSE = 0.31091469526290894\n",
      "Fila 468: MSE = 0.3475501239299774\n",
      "Fila 469: MSE = 0.35523828864097595\n",
      "Fila 470: MSE = 0.4520014524459839\n",
      "Fila 471: MSE = 0.3242858052253723\n",
      "Fila 472: MSE = 0.35138508677482605\n",
      "Fila 473: MSE = 0.3643832802772522\n",
      "Fila 474: MSE = 0.33423715829849243\n",
      "Fila 475: MSE = 0.3977622985839844\n",
      "Fila 476: MSE = 0.30582815408706665\n",
      "Fila 477: MSE = 0.4383840262889862\n",
      "Fila 478: MSE = 0.37143874168395996\n",
      "Fila 479: MSE = 0.3688373267650604\n",
      "Fila 480: MSE = 0.3433305621147156\n",
      "Fila 481: MSE = 0.35623958706855774\n",
      "Fila 482: MSE = 0.41580748558044434\n",
      "Fila 483: MSE = 0.36431723833084106\n",
      "Fila 484: MSE = 0.32929474115371704\n",
      "Fila 485: MSE = 0.2707165777683258\n",
      "Fila 486: MSE = 0.4374057948589325\n",
      "Fila 487: MSE = 0.33811697363853455\n",
      "Fila 488: MSE = 0.36093538999557495\n",
      "Fila 489: MSE = 0.39779970049858093\n",
      "Fila 490: MSE = 0.3650308847427368\n",
      "Fila 491: MSE = 0.3007552921772003\n",
      "Fila 492: MSE = 0.3262154757976532\n",
      "Fila 493: MSE = 0.318820983171463\n",
      "Fila 494: MSE = 0.31476232409477234\n",
      "Fila 495: MSE = 0.33698198199272156\n",
      "Fila 496: MSE = 0.3094298243522644\n",
      "Fila 497: MSE = 0.45233267545700073\n",
      "Fila 498: MSE = 0.29734495282173157\n",
      "Fila 499: MSE = 0.41328099370002747\n",
      "Fila 500: MSE = 0.3350893557071686\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "# Leer datos\n",
    "expression_data2 = pd.read_csv('exprTEST.csv')\n",
    "methylation_data2 = pd.read_csv('methylTEST.csv')\n",
    "assign_data2 = pd.read_csv('assignTEST.csv')\n",
    "\n",
    "expression_data2 = expression_data2.iloc[:, 1:]\n",
    "methylation_data2 = methylation_data2.iloc[:, 1:]\n",
    "assign_data2 = assign_data2.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data2 = expression_data2.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data2 = methylation_data2.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data2 = assign_data2.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expression_data2.fillna(0, inplace=True)\n",
    "methylation_data2.fillna(0, inplace=True)\n",
    "assign_data2.fillna(0, inplace=True)\n",
    "\n",
    "expression_data2 = expression_data2.values\n",
    "methylation_data2 = methylation_data2.values\n",
    "assign_data2=assign_data2.values\n",
    "# Asegurar que tienes el mismo número de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data2.shape[0] == methylation_data2.shape[0], \"Los datos de expresión y metilación deben tener el mismo número de muestras.\"\n",
    "assert assign_data2.shape[0] == methylation_data2.shape[0], \"Los datos de asignación y metilación deben tener el mismo número de muestras.\"\n",
    "\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "expression_data2 = torch.FloatTensor(expression_data2).to(device)\n",
    "methylation_data2 = torch.FloatTensor(methylation_data2).to(device)\n",
    "assign_data2 = torch.FloatTensor(assign_data2).to(device)\n",
    "combined_data2= torch.cat((expression_data2,methylation_data2,assign_data2), 1).to(device)\n",
    "# Carga del modelo previamente entrenado\n",
    "gen = Generator(expression_data2.shape[1], methylation_data2.shape[1]+assign_data2.shape[1])\n",
    "gen.load_state_dict(torch.load('generator_model4.pth'))\n",
    "gen.eval()\n",
    "gen.to(device)\n",
    "\n",
    "# Pasar todos los datos de expr.csv a través del generador\n",
    "with torch.no_grad():\n",
    "    generated_methyl = gen(expression_data2)\n",
    "\n",
    "# Asegúrate de que los datos generados y los datos reales estén en la misma forma\n",
    "generated_methyl = generated_methyl.cpu().numpy()\n",
    "\n",
    "# Aquí estamos asumiendo que la primera parte de la salida generada corresponde a methyl.csv\n",
    "# Si tu salida generada incluye más datos además de methyl.csv, necesitas ajustar esto\n",
    "generated_methyl_data = generated_methyl[:, :methylation_data2.shape[1]]\n",
    "\n",
    "# Calcula el error cuadrático medio entre los datos generados y los reales\n",
    "mse = mean_squared_error(methylation_data2.cpu().numpy(), generated_methyl_data)\n",
    "print(f\"Error cuadrático medio entre datos generados y reales: {mse}\")\n",
    "print(f\"Media: {methylation_data2.mean()}\")\n",
    "# Si el MSE es bajo, significa que los datos generados y los datos reales son muy similares.\n",
    "\n",
    "individual_mse_errors = []\n",
    "\n",
    "# Procesar datos\n",
    "with torch.no_grad():\n",
    "    for i in range(expression_data2.shape[0]):\n",
    "        input_expr = expression_data2[i].unsqueeze(0)  # Añadir dimensión de batch\n",
    "        real_output = torch.cat([methylation_data2[i], assign_data2[i]]).unsqueeze(0)  # Añadir dimensión de batch\n",
    "        generated_output = gen(input_expr)\n",
    "        \n",
    "        error = mse_loss(generated_output, real_output)\n",
    "        #print(generated_output)\n",
    "        #print(real_output)\n",
    "        individual_mse_errors.append(error.item())\n",
    "\n",
    "# Si deseas, puedes imprimir el error MSE para cada fila\n",
    "for i, error in enumerate(individual_mse_errors):\n",
    "    print(f\"Fila {i + 1}: MSE = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4389ef-27fa-4d77-b5cb-ac1662916c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo SVC\u001b[39;00m\n\u001b[1;32m     29\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Predicciones\u001b[39;00m\n\u001b[1;32m     33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:747\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    750\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Generar datos de expresión sintética\n",
    "gen2 = Generator(expression_data.shape[1], methylation_data.shape[1]+assign_data.shape[1]).to(device)\n",
    "gen2.load_state_dict(torch.load('generator_model2.pth'))\n",
    "gen2.eval()\n",
    "expressions_list = []\n",
    "for i in range(100000):\n",
    "    noise = torch.randn(1, expression_data.shape[1]).to(device)\n",
    "    synthetic_expression = gen2(noise).to(device)\n",
    "    #print(synthetic_expression)\n",
    "    expressions_list.append(synthetic_expression.detach().cpu().numpy().squeeze())\n",
    "\n",
    "X = pd.DataFrame(expressions_list)\n",
    "#X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = X.iloc[:, -1].values.astype(int)  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "X = X.iloc[:, :-1]\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a54d7865-fa40-496c-944d-a230c3a87b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     23934\n",
      "           2       1.00      1.00      1.00      6066\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprMID.csv')\n",
    "meth_data = pd.read_csv('methylMID.csv')\n",
    "assig_data = pd.read_csv('assignMID.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = pd.concat([expr_data, meth_data], axis=1)\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4684f20f-50aa-4729-b7f8-fa924b20592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     23934\n",
      "           2       1.00      1.00      1.00      6066\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprMID.csv')\n",
    "meth_data = pd.read_csv('methylMID.csv')\n",
    "assig_data = pd.read_csv('assignMID.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = meth_data\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8b7b5-c373-4a86-9b69-eb5ddebd998a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
