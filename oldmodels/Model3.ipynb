{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946ab409-0c2b-442c-8497-1d847be17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c3e50c-ac00-4334-9f09-484fdf78991b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster.id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjects</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject496</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject498</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject499</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject500</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cluster.id\n",
       "subjects              \n",
       "subject1             1\n",
       "subject2             2\n",
       "subject3             1\n",
       "subject4             1\n",
       "subject5             1\n",
       "...                ...\n",
       "subject496           1\n",
       "subject497           1\n",
       "subject498           1\n",
       "subject499           2\n",
       "subject500           2\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methyldata = pd.read_csv(\"methyl.csv\",index_col=0)\n",
    "exprdata = pd.read_csv(\"expr.csv\",index_col=0)\n",
    "assigndata = pd.read_csv(\"assign.csv\",usecols=[1,2],index_col=0)\n",
    "assigndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d14c143d-8fb3-4bd7-8436-0015a40f7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACACA</th>\n",
       "      <th>ACVRL1</th>\n",
       "      <th>AKT1</th>\n",
       "      <th>AKT1S1</th>\n",
       "      <th>ANXA1</th>\n",
       "      <th>AR</th>\n",
       "      <th>ARAF</th>\n",
       "      <th>ASNS</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAD</th>\n",
       "      <th>...</th>\n",
       "      <th>VHL</th>\n",
       "      <th>WWTR1</th>\n",
       "      <th>XBP1</th>\n",
       "      <th>XRCC1</th>\n",
       "      <th>XRCC5</th>\n",
       "      <th>YAP1</th>\n",
       "      <th>YBX1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject1</th>\n",
       "      <td>-0.778773</td>\n",
       "      <td>6.661350</td>\n",
       "      <td>0.601915</td>\n",
       "      <td>-1.381072</td>\n",
       "      <td>0.276366</td>\n",
       "      <td>6.826198</td>\n",
       "      <td>5.217787</td>\n",
       "      <td>-0.531270</td>\n",
       "      <td>-0.685574</td>\n",
       "      <td>4.965979</td>\n",
       "      <td>...</td>\n",
       "      <td>4.997389</td>\n",
       "      <td>5.227471</td>\n",
       "      <td>-1.173942</td>\n",
       "      <td>4.990185</td>\n",
       "      <td>4.739730</td>\n",
       "      <td>0.752404</td>\n",
       "      <td>4.925204</td>\n",
       "      <td>-0.332289</td>\n",
       "      <td>4.740385</td>\n",
       "      <td>1.588527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2</th>\n",
       "      <td>-1.202237</td>\n",
       "      <td>6.599935</td>\n",
       "      <td>-1.059473</td>\n",
       "      <td>-1.343091</td>\n",
       "      <td>0.567053</td>\n",
       "      <td>2.980298</td>\n",
       "      <td>-1.060058</td>\n",
       "      <td>-0.543807</td>\n",
       "      <td>4.112408</td>\n",
       "      <td>-0.377627</td>\n",
       "      <td>...</td>\n",
       "      <td>5.324622</td>\n",
       "      <td>-0.575675</td>\n",
       "      <td>-0.521891</td>\n",
       "      <td>1.270019</td>\n",
       "      <td>5.396299</td>\n",
       "      <td>1.926090</td>\n",
       "      <td>0.432328</td>\n",
       "      <td>0.453741</td>\n",
       "      <td>0.124431</td>\n",
       "      <td>1.516044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3</th>\n",
       "      <td>-0.351933</td>\n",
       "      <td>7.037106</td>\n",
       "      <td>-0.169862</td>\n",
       "      <td>-0.600338</td>\n",
       "      <td>0.467504</td>\n",
       "      <td>4.409149</td>\n",
       "      <td>5.523396</td>\n",
       "      <td>-0.820251</td>\n",
       "      <td>-0.896702</td>\n",
       "      <td>6.273091</td>\n",
       "      <td>...</td>\n",
       "      <td>6.548016</td>\n",
       "      <td>6.983861</td>\n",
       "      <td>1.174447</td>\n",
       "      <td>5.292834</td>\n",
       "      <td>5.405190</td>\n",
       "      <td>1.720668</td>\n",
       "      <td>5.243290</td>\n",
       "      <td>0.383033</td>\n",
       "      <td>7.387739</td>\n",
       "      <td>1.114908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4</th>\n",
       "      <td>-0.560752</td>\n",
       "      <td>6.302922</td>\n",
       "      <td>-1.095674</td>\n",
       "      <td>-0.335570</td>\n",
       "      <td>-1.799302</td>\n",
       "      <td>4.858635</td>\n",
       "      <td>4.789491</td>\n",
       "      <td>-0.335323</td>\n",
       "      <td>-1.509982</td>\n",
       "      <td>5.800878</td>\n",
       "      <td>...</td>\n",
       "      <td>4.780794</td>\n",
       "      <td>6.124618</td>\n",
       "      <td>-1.631147</td>\n",
       "      <td>5.213470</td>\n",
       "      <td>5.813819</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>4.457551</td>\n",
       "      <td>-0.836184</td>\n",
       "      <td>5.890996</td>\n",
       "      <td>-0.108535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5</th>\n",
       "      <td>-0.260576</td>\n",
       "      <td>5.974995</td>\n",
       "      <td>-1.597704</td>\n",
       "      <td>-1.239989</td>\n",
       "      <td>-0.848312</td>\n",
       "      <td>6.202194</td>\n",
       "      <td>3.625550</td>\n",
       "      <td>-0.896659</td>\n",
       "      <td>-0.772299</td>\n",
       "      <td>5.427887</td>\n",
       "      <td>...</td>\n",
       "      <td>5.509940</td>\n",
       "      <td>5.235778</td>\n",
       "      <td>-1.466720</td>\n",
       "      <td>5.391635</td>\n",
       "      <td>5.309465</td>\n",
       "      <td>1.113457</td>\n",
       "      <td>5.062244</td>\n",
       "      <td>-1.151481</td>\n",
       "      <td>5.118148</td>\n",
       "      <td>-0.504209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject496</th>\n",
       "      <td>-0.933188</td>\n",
       "      <td>6.758515</td>\n",
       "      <td>0.141599</td>\n",
       "      <td>1.175414</td>\n",
       "      <td>-0.199922</td>\n",
       "      <td>5.404964</td>\n",
       "      <td>4.587526</td>\n",
       "      <td>-2.250117</td>\n",
       "      <td>-0.043880</td>\n",
       "      <td>4.791108</td>\n",
       "      <td>...</td>\n",
       "      <td>4.960378</td>\n",
       "      <td>4.375743</td>\n",
       "      <td>-0.822462</td>\n",
       "      <td>6.340625</td>\n",
       "      <td>5.713636</td>\n",
       "      <td>1.483030</td>\n",
       "      <td>5.739724</td>\n",
       "      <td>0.294931</td>\n",
       "      <td>5.237734</td>\n",
       "      <td>1.493332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject497</th>\n",
       "      <td>-0.351743</td>\n",
       "      <td>6.307647</td>\n",
       "      <td>-0.395698</td>\n",
       "      <td>-0.410632</td>\n",
       "      <td>-0.335842</td>\n",
       "      <td>5.095289</td>\n",
       "      <td>5.294062</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>-0.599051</td>\n",
       "      <td>4.903005</td>\n",
       "      <td>...</td>\n",
       "      <td>6.760614</td>\n",
       "      <td>6.283541</td>\n",
       "      <td>0.793421</td>\n",
       "      <td>5.089189</td>\n",
       "      <td>4.882482</td>\n",
       "      <td>0.584146</td>\n",
       "      <td>5.546557</td>\n",
       "      <td>-0.167474</td>\n",
       "      <td>5.152645</td>\n",
       "      <td>1.066775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject498</th>\n",
       "      <td>-0.555563</td>\n",
       "      <td>6.201945</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>-0.527820</td>\n",
       "      <td>1.141942</td>\n",
       "      <td>4.788988</td>\n",
       "      <td>4.594360</td>\n",
       "      <td>-1.039993</td>\n",
       "      <td>-0.339906</td>\n",
       "      <td>6.553227</td>\n",
       "      <td>...</td>\n",
       "      <td>4.372792</td>\n",
       "      <td>5.008954</td>\n",
       "      <td>-0.950863</td>\n",
       "      <td>4.932677</td>\n",
       "      <td>5.589072</td>\n",
       "      <td>1.423199</td>\n",
       "      <td>4.820329</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>6.264235</td>\n",
       "      <td>0.935781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject499</th>\n",
       "      <td>-0.496976</td>\n",
       "      <td>6.736355</td>\n",
       "      <td>0.309428</td>\n",
       "      <td>1.002468</td>\n",
       "      <td>-1.421916</td>\n",
       "      <td>0.913188</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>-2.264783</td>\n",
       "      <td>4.140739</td>\n",
       "      <td>1.007924</td>\n",
       "      <td>...</td>\n",
       "      <td>4.496152</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>-0.513729</td>\n",
       "      <td>1.362927</td>\n",
       "      <td>5.841151</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>-0.240316</td>\n",
       "      <td>0.078770</td>\n",
       "      <td>0.380601</td>\n",
       "      <td>1.431541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject500</th>\n",
       "      <td>-1.368186</td>\n",
       "      <td>6.686480</td>\n",
       "      <td>-1.006249</td>\n",
       "      <td>-1.156804</td>\n",
       "      <td>0.353833</td>\n",
       "      <td>0.752743</td>\n",
       "      <td>0.222256</td>\n",
       "      <td>-0.858002</td>\n",
       "      <td>4.597509</td>\n",
       "      <td>0.781072</td>\n",
       "      <td>...</td>\n",
       "      <td>5.391630</td>\n",
       "      <td>0.374457</td>\n",
       "      <td>1.049327</td>\n",
       "      <td>-0.022351</td>\n",
       "      <td>5.551886</td>\n",
       "      <td>1.589490</td>\n",
       "      <td>1.385238</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.945422</td>\n",
       "      <td>0.832131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ACACA    ACVRL1      AKT1    AKT1S1     ANXA1        AR  \\\n",
       "subject1   -0.778773  6.661350  0.601915 -1.381072  0.276366  6.826198   \n",
       "subject2   -1.202237  6.599935 -1.059473 -1.343091  0.567053  2.980298   \n",
       "subject3   -0.351933  7.037106 -0.169862 -0.600338  0.467504  4.409149   \n",
       "subject4   -0.560752  6.302922 -1.095674 -0.335570 -1.799302  4.858635   \n",
       "subject5   -0.260576  5.974995 -1.597704 -1.239989 -0.848312  6.202194   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "subject496 -0.933188  6.758515  0.141599  1.175414 -0.199922  5.404964   \n",
       "subject497 -0.351743  6.307647 -0.395698 -0.410632 -0.335842  5.095289   \n",
       "subject498 -0.555563  6.201945  0.369510 -0.527820  1.141942  4.788988   \n",
       "subject499 -0.496976  6.736355  0.309428  1.002468 -1.421916  0.913188   \n",
       "subject500 -1.368186  6.686480 -1.006249 -1.156804  0.353833  0.752743   \n",
       "\n",
       "                ARAF      ASNS       ATM       BAD  ...       VHL     WWTR1  \\\n",
       "subject1    5.217787 -0.531270 -0.685574  4.965979  ...  4.997389  5.227471   \n",
       "subject2   -1.060058 -0.543807  4.112408 -0.377627  ...  5.324622 -0.575675   \n",
       "subject3    5.523396 -0.820251 -0.896702  6.273091  ...  6.548016  6.983861   \n",
       "subject4    4.789491 -0.335323 -1.509982  5.800878  ...  4.780794  6.124618   \n",
       "subject5    3.625550 -0.896659 -0.772299  5.427887  ...  5.509940  5.235778   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "subject496  4.587526 -2.250117 -0.043880  4.791108  ...  4.960378  4.375743   \n",
       "subject497  5.294062  0.064668 -0.599051  4.903005  ...  6.760614  6.283541   \n",
       "subject498  4.594360 -1.039993 -0.339906  6.553227  ...  4.372792  5.008954   \n",
       "subject499  0.544914 -2.264783  4.140739  1.007924  ...  4.496152  0.002271   \n",
       "subject500  0.222256 -0.858002  4.597509  0.781072  ...  5.391630  0.374457   \n",
       "\n",
       "                XBP1     XRCC1     XRCC5      YAP1      YBX1     YWHAB  \\\n",
       "subject1   -1.173942  4.990185  4.739730  0.752404  4.925204 -0.332289   \n",
       "subject2   -0.521891  1.270019  5.396299  1.926090  0.432328  0.453741   \n",
       "subject3    1.174447  5.292834  5.405190  1.720668  5.243290  0.383033   \n",
       "subject4   -1.631147  5.213470  5.813819  0.552651  4.457551 -0.836184   \n",
       "subject5   -1.466720  5.391635  5.309465  1.113457  5.062244 -1.151481   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "subject496 -0.822462  6.340625  5.713636  1.483030  5.739724  0.294931   \n",
       "subject497  0.793421  5.089189  4.882482  0.584146  5.546557 -0.167474   \n",
       "subject498 -0.950863  4.932677  5.589072  1.423199  4.820329  0.054766   \n",
       "subject499 -0.513729  1.362927  5.841151  0.990882 -0.240316  0.078770   \n",
       "subject500  1.049327 -0.022351  5.551886  1.589490  1.385238 -0.000684   \n",
       "\n",
       "               YWHAE     YWHAZ  \n",
       "subject1    4.740385  1.588527  \n",
       "subject2    0.124431  1.516044  \n",
       "subject3    7.387739  1.114908  \n",
       "subject4    5.890996 -0.108535  \n",
       "subject5    5.118148 -0.504209  \n",
       "...              ...       ...  \n",
       "subject496  5.237734  1.493332  \n",
       "subject497  5.152645  1.066775  \n",
       "subject498  6.264235  0.935781  \n",
       "subject499  0.380601  1.431541  \n",
       "subject500  0.945422  0.832131  \n",
       "\n",
       "[500 rows x 131 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exprdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000 | Disc Loss: 5.945760494796559e-05 | Gen Loss: 10.515534400939941\n",
      "Epoch 200/10000 | Disc Loss: 1.399466327711707e-05 | Gen Loss: 12.196635246276855\n",
      "Epoch 300/10000 | Disc Loss: 5.123167284182273e-06 | Gen Loss: 13.017976760864258\n",
      "Epoch 400/10000 | Disc Loss: 3.7316140151233412e-06 | Gen Loss: 14.001824378967285\n",
      "Epoch 500/10000 | Disc Loss: 1.7040133570844773e-06 | Gen Loss: 14.61600399017334\n",
      "Epoch 600/10000 | Disc Loss: 9.082845622288005e-07 | Gen Loss: 14.930032730102539\n",
      "Epoch 700/10000 | Disc Loss: 3.8317355688377575e-07 | Gen Loss: 15.814483642578125\n",
      "Epoch 800/10000 | Disc Loss: 1.9683631080624764e-07 | Gen Loss: 16.492481231689453\n",
      "Epoch 900/10000 | Disc Loss: 3.140907551824057e-07 | Gen Loss: 16.668394088745117\n",
      "Epoch 1000/10000 | Disc Loss: 1.4640127687925997e-07 | Gen Loss: 17.35086441040039\n",
      "Epoch 1100/10000 | Disc Loss: 7.728576179033553e-08 | Gen Loss: 17.871023178100586\n",
      "Epoch 1200/10000 | Disc Loss: 8.77089547657306e-08 | Gen Loss: 17.349214553833008\n",
      "Epoch 1300/10000 | Disc Loss: 3.1962738233914934e-08 | Gen Loss: 18.422895431518555\n",
      "Epoch 1400/10000 | Disc Loss: 1.4053101793365386e-08 | Gen Loss: 19.208227157592773\n",
      "Epoch 1500/10000 | Disc Loss: 1.3573529855648303e-08 | Gen Loss: 19.33045768737793\n",
      "Epoch 1600/10000 | Disc Loss: 9.30782029229249e-09 | Gen Loss: 20.12434196472168\n",
      "Epoch 1700/10000 | Disc Loss: 5.9627272008810905e-09 | Gen Loss: 20.76742172241211\n",
      "Epoch 1800/10000 | Disc Loss: 2.8224820258770933e-09 | Gen Loss: 20.92830467224121\n",
      "Epoch 1900/10000 | Disc Loss: 1.9127435191279574e-09 | Gen Loss: 21.24684715270996\n",
      "Epoch 2000/10000 | Disc Loss: 9.64363477962138e-10 | Gen Loss: 21.939411163330078\n",
      "Epoch 2100/10000 | Disc Loss: 5.103412137330565e-10 | Gen Loss: 22.605045318603516\n",
      "Epoch 2200/10000 | Disc Loss: 4.0677414214762564e-10 | Gen Loss: 22.837453842163086\n",
      "Epoch 2300/10000 | Disc Loss: 6.287619314093718e-10 | Gen Loss: 22.59515380859375\n",
      "Epoch 2400/10000 | Disc Loss: 3.060766362583678e-10 | Gen Loss: 23.472759246826172\n",
      "Epoch 2500/10000 | Disc Loss: 1.8823630154596316e-10 | Gen Loss: 24.06419563293457\n",
      "Epoch 2600/10000 | Disc Loss: 1.1443260922172627e-10 | Gen Loss: 24.600778579711914\n",
      "Epoch 2700/10000 | Disc Loss: 7.245941346933904e-11 | Gen Loss: 25.059284210205078\n",
      "Epoch 2800/10000 | Disc Loss: 5.084867596427678e-11 | Gen Loss: 25.421527862548828\n",
      "Epoch 2900/10000 | Disc Loss: 3.674170689915712e-11 | Gen Loss: 25.759859085083008\n",
      "Epoch 3000/10000 | Disc Loss: 3.532705031283889e-11 | Gen Loss: 25.839473724365234\n",
      "Epoch 3100/10000 | Disc Loss: 2.630559312044589e-11 | Gen Loss: 26.142759323120117\n",
      "Epoch 3200/10000 | Disc Loss: 1.9211004315122793e-11 | Gen Loss: 26.47648048400879\n",
      "Epoch 3300/10000 | Disc Loss: 1.4926558253303135e-11 | Gen Loss: 26.750669479370117\n",
      "Epoch 3400/10000 | Disc Loss: 1.15949949294758e-11 | Gen Loss: 27.02149772644043\n",
      "Epoch 3500/10000 | Disc Loss: 9.255474858749224e-12 | Gen Loss: 27.267587661743164\n",
      "Epoch 3600/10000 | Disc Loss: 8.08446712352584e-12 | Gen Loss: 27.424264907836914\n",
      "Epoch 3700/10000 | Disc Loss: 6.449057000229574e-12 | Gen Loss: 27.665742874145508\n",
      "Epoch 3800/10000 | Disc Loss: 5.597766174203489e-12 | Gen Loss: 27.826427459716797\n",
      "Epoch 3900/10000 | Disc Loss: 4.677800681529565e-12 | Gen Loss: 28.01958465576172\n",
      "Epoch 4000/10000 | Disc Loss: 3.99578677437451e-12 | Gen Loss: 28.189531326293945\n",
      "Epoch 4100/10000 | Disc Loss: 3.702968053714839e-12 | Gen Loss: 28.279436111450195\n",
      "Epoch 4200/10000 | Disc Loss: 3.2437707034316254e-12 | Gen Loss: 28.42253875732422\n",
      "Epoch 4300/10000 | Disc Loss: 2.9164528864839e-12 | Gen Loss: 28.526309967041016\n",
      "Epoch 4400/10000 | Disc Loss: 2.765163315335273e-12 | Gen Loss: 28.593238830566406\n",
      "Epoch 4500/10000 | Disc Loss: 2.494997481186645e-12 | Gen Loss: 28.704631805419922\n",
      "Epoch 4600/10000 | Disc Loss: 2.2725829464803615e-12 | Gen Loss: 28.805864334106445\n",
      "Epoch 4700/10000 | Disc Loss: 2.13024433162734e-12 | Gen Loss: 28.87272071838379\n",
      "Epoch 4800/10000 | Disc Loss: 2.070078266788733e-12 | Gen Loss: 28.915203094482422\n",
      "Epoch 4900/10000 | Disc Loss: 1.9145303831874516e-12 | Gen Loss: 28.999876022338867\n",
      "Epoch 5000/10000 | Disc Loss: 1.7808603983843208e-12 | Gen Loss: 29.078327178955078\n",
      "Epoch 5100/10000 | Disc Loss: 6.952998073128658e-12 | Gen Loss: 27.43357276916504\n",
      "Epoch 5200/10000 | Disc Loss: 5.589808997619183e-12 | Gen Loss: 27.674030303955078\n",
      "Epoch 5300/10000 | Disc Loss: 4.843528356168747e-12 | Gen Loss: 27.83304214477539\n",
      "Epoch 5400/10000 | Disc Loss: 4.13984428335179e-12 | Gen Loss: 28.006046295166016\n",
      "Epoch 5500/10000 | Disc Loss: 3.6141376349996257e-12 | Gen Loss: 28.155723571777344\n",
      "Epoch 5600/10000 | Disc Loss: 3.2068275986063544e-12 | Gen Loss: 28.287527084350586\n",
      "Epoch 5700/10000 | Disc Loss: 2.8820810755308957e-12 | Gen Loss: 28.40522575378418\n",
      "Epoch 5800/10000 | Disc Loss: 2.697903315682093e-12 | Gen Loss: 28.48023796081543\n",
      "Epoch 5900/10000 | Disc Loss: 2.4652836196070727e-12 | Gen Loss: 28.579662322998047\n",
      "Epoch 6000/10000 | Disc Loss: 2.2697213032663033e-12 | Gen Loss: 28.670795440673828\n",
      "Epoch 6100/10000 | Disc Loss: 2.103104149164814e-12 | Gen Loss: 28.754873275756836\n",
      "Epoch 6200/10000 | Disc Loss: 1.959538651133408e-12 | Gen Loss: 28.83285140991211\n",
      "Epoch 6300/10000 | Disc Loss: 1.8345136607728074e-12 | Gen Loss: 28.905559539794922\n",
      "Epoch 6400/10000 | Disc Loss: 1.724682354396767e-12 | Gen Loss: 28.97364616394043\n",
      "Epoch 6500/10000 | Disc Loss: 2.015686879561218e-12 | Gen Loss: 28.820894241333008\n",
      "Epoch 6600/10000 | Disc Loss: 1.8844192699318807e-12 | Gen Loss: 28.89521598815918\n",
      "Epoch 6700/10000 | Disc Loss: 1.7694115487035256e-12 | Gen Loss: 28.964712142944336\n",
      "Epoch 6800/10000 | Disc Loss: 1.6678492354565977e-12 | Gen Loss: 29.029951095581055\n",
      "Epoch 6900/10000 | Disc Loss: 1.638876317641702e-12 | Gen Loss: 29.050212860107422\n",
      "Epoch 7000/10000 | Disc Loss: 1.5517916812646249e-12 | Gen Loss: 29.110471725463867\n",
      "Epoch 7100/10000 | Disc Loss: 1.4735702892273994e-12 | Gen Loss: 29.16754913330078\n",
      "Epoch 7200/10000 | Disc Loss: 1.4029521901853803e-12 | Gen Loss: 29.221742630004883\n",
      "Epoch 7300/10000 | Disc Loss: 1.3389031636862336e-12 | Gen Loss: 29.273311614990234\n",
      "Epoch 7400/10000 | Disc Loss: 1.2805082515165989e-12 | Gen Loss: 29.322513580322266\n",
      "Epoch 7500/10000 | Disc Loss: 1.2270643485676191e-12 | Gen Loss: 29.369548797607422\n",
      "Epoch 7600/10000 | Disc Loss: 1.1779644100429199e-12 | Gen Loss: 29.41459846496582\n",
      "Epoch 7700/10000 | Disc Loss: 1.1327030893099055e-12 | Gen Loss: 29.457813262939453\n",
      "Epoch 7800/10000 | Disc Loss: 1.0908663295589038e-12 | Gen Loss: 29.499317169189453\n",
      "Epoch 7900/10000 | Disc Loss: 1.052086477833225e-12 | Gen Loss: 29.53923225402832\n",
      "Epoch 8000/10000 | Disc Loss: 1.0160333945713473e-12 | Gen Loss: 29.57768440246582\n",
      "Epoch 8100/10000 | Disc Loss: 9.82481240460742e-13 | Gen Loss: 29.61471176147461\n",
      "Epoch 8200/10000 | Disc Loss: 9.511458461120004e-13 | Gen Loss: 29.650449752807617\n",
      "Epoch 8300/10000 | Disc Loss: 9.218445234590589e-13 | Gen Loss: 29.68494987487793\n",
      "Epoch 8400/10000 | Disc Loss: 8.96832738281339e-13 | Gen Loss: 29.716522216796875\n",
      "Epoch 8500/10000 | Disc Loss: 8.709078380339519e-13 | Gen Loss: 29.748863220214844\n",
      "Epoch 8600/10000 | Disc Loss: 8.465336721538708e-13 | Gen Loss: 29.780160903930664\n",
      "Epoch 8700/10000 | Disc Loss: 8.235706496113882e-13 | Gen Loss: 29.810482025146484\n",
      "Epoch 8800/10000 | Disc Loss: 8.018831367147261e-13 | Gen Loss: 29.83990478515625\n",
      "Epoch 8900/10000 | Disc Loss: 7.813890593594275e-13 | Gen Loss: 29.868450164794922\n",
      "Epoch 9000/10000 | Disc Loss: 7.619908393499686e-13 | Gen Loss: 29.896167755126953\n",
      "Epoch 9100/10000 | Disc Loss: 7.436260266412142e-13 | Gen Loss: 29.923072814941406\n",
      "Epoch 9200/10000 | Disc Loss: 7.261770395075584e-13 | Gen Loss: 29.94925308227539\n",
      "Epoch 9300/10000 | Disc Loss: 7.095970946252583e-13 | Gen Loss: 29.974729537963867\n",
      "Epoch 9400/10000 | Disc Loss: 6.938240672098306e-13 | Gen Loss: 29.999523162841797\n",
      "Epoch 9500/10000 | Disc Loss: 6.787878093807154e-13 | Gen Loss: 30.023693084716797\n",
      "Epoch 9600/10000 | Disc Loss: 6.644525424662207e-13 | Gen Loss: 30.047231674194336\n",
      "Epoch 9700/10000 | Disc Loss: 6.507722962942331e-13 | Gen Loss: 30.070186614990234\n",
      "Epoch 9800/10000 | Disc Loss: 6.65335354085167e-13 | Gen Loss: 30.04683494567871\n",
      "Epoch 9900/10000 | Disc Loss: 6.51673485140003e-13 | Gen Loss: 30.069721221923828\n",
      "Epoch 10000/10000 | Disc Loss: 6.385916643571188e-13 | Gen Loss: 30.092092514038086\n",
      "tensor([[ 1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
      "         -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
      "          1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
      "          1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
      "         -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
      "         -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
      "          1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,\n",
      "         -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
      "         -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         -1., -1.,  1.,  1., -1.]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer datos\n",
    "expression_data = pd.read_csv('expr.csv')\n",
    "methylation_data = pd.read_csv('methyl.csv')\n",
    "\n",
    "expression_data = expression_data.iloc[:, 1:]\n",
    "methylation_data = methylation_data.iloc[:, 1:]\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data = methylation_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). AquÃ­ simplemente los lleno con 0, pero podrÃ­as manejarlo de otra manera.\n",
    "expression_data.fillna(0, inplace=True)\n",
    "methylation_data.fillna(0, inplace=True)\n",
    "\n",
    "expression_data = expression_data.values\n",
    "methylation_data = methylation_data.values\n",
    "# Asegurar que tienes el mismo nÃºmero de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data.shape[0] == methylation_data.shape[0], \"Los datos de expresiÃ³n y metilaciÃ³n deben tener el mismo nÃºmero de muestras.\"\n",
    "\n",
    "\n",
    "# Concatenar los datos\n",
    "combined_data = torch.FloatTensor(np.hstack((expression_data, methylation_data)))\n",
    "\n",
    "expression_data = torch.FloatTensor(expression_data)\n",
    "methylation_data = torch.FloatTensor(methylation_data)\n",
    "# Definir el generador y el discriminador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_size, heads):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Inicio de la red\n",
    "        self.start = nn.Sequential(\n",
    "            nn.Linear(input_dim, embed_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Capa de atenciÃ³n del Transformer\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        \n",
    "        # Salida\n",
    "        self.end = nn.Sequential(\n",
    "            nn.Linear(embed_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        # Agregar una dimensiÃ³n adicional para tratar la entrada como una \"secuencia\"\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.attention(x, x, x)  \n",
    "        # Eliminar la dimensiÃ³n extra\n",
    "        x = x.squeeze(1)\n",
    "        x = self.end(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Modelos y optimizadores\n",
    "embed_size = 128\n",
    "heads = 4\n",
    "gen = Generator(100, expression_data.shape[1], embed_size, heads)\n",
    "disc = Discriminator(expression_data.shape[1] + methylation_data.shape[1])\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.Adam(disc.parameters(), lr=0.0002)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Entrenamiento\n",
    "n_samples = combined_data.size(0)\n",
    "batch_size = 64\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for idx in range(0, n_samples, batch_size):\n",
    "        real_data = combined_data[idx:idx+batch_size]\n",
    "        current_batch_size = real_data.size(0)\n",
    "        real_labels = torch.ones(current_batch_size, 1)\n",
    "\n",
    "        noise = torch.randn(current_batch_size, 100)\n",
    "        fake_expression = gen(noise)\n",
    "        fake_data = torch.cat((fake_expression, methylation_data[idx:idx+batch_size]), 1)\n",
    "        fake_labels = torch.zeros(current_batch_size, 1)\n",
    "        \n",
    "        # Entrenar discriminador\n",
    "        disc_optimizer.zero_grad()\n",
    "\n",
    "        real_preds = disc(real_data)\n",
    "        real_loss = criterion(real_preds, real_labels)\n",
    "        \n",
    "        fake_preds = disc(fake_data)\n",
    "        fake_loss = criterion(fake_preds, fake_labels)\n",
    "\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Entrenar generador\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        noise = torch.randn(current_batch_size, 100)\n",
    "        fake_expression = gen(noise)\n",
    "        fake_data = torch.cat((fake_expression, methylation_data[idx:idx+batch_size]), 1)\n",
    "        fake_preds = disc(fake_data)\n",
    "\n",
    "        gen_loss = criterion(fake_preds, real_labels)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Disc Loss: {disc_loss.item()} | Gen Loss: {gen_loss.item()}\")\n",
    "\n",
    "# Generar datos de expresiÃ³n sintÃ©tica\n",
    "noise = torch.randn(1, 100)\n",
    "synthetic_expression = gen(noise)\n",
    "print(synthetic_expression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d7865-fa40-496c-944d-a230c3a87b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
