{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefe2b6d-ee4c-44a3-b1f2-2dda2ca33fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade19a53-9154-49af-a8d1-6136ad278adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2 * latent_dim)  # 2 for mean and variance\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "        \n",
    "# Leer datos\n",
    "expression_data = pd.read_csv('expr.csv')\n",
    "methylation_data = pd.read_csv('methyl.csv')\n",
    "assign_data = pd.read_csv('assign.csv')\n",
    "\n",
    "expression_data = expression_data.iloc[:, 1:]\n",
    "methylation_data = methylation_data.iloc[:, 1:]\n",
    "assign_data = assign_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "methylation_data = methylation_data.apply(pd.to_numeric, errors='coerce')\n",
    "assign_data = assign_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expression_data.fillna(0, inplace=True)\n",
    "methylation_data.fillna(0, inplace=True)\n",
    "assign_data.fillna(0, inplace=True)\n",
    "\n",
    "expression_data = expression_data.values\n",
    "methylation_data = methylation_data.values\n",
    "assign_data=assign_data.values\n",
    "# Asegurar que tienes el mismo número de muestras en ambos conjuntos de datos\n",
    "\n",
    "\n",
    "assert expression_data.shape[0] == methylation_data.shape[0], \"Los datos de expresión y metilación deben tener el mismo número de muestras.\"\n",
    "assert assign_data.shape[0] == methylation_data.shape[0], \"Los datos de asignación y metilación deben tener el mismo número de muestras.\"\n",
    "\n",
    "# Concatenar los datos\n",
    "#combined_data =torch.FloatTensor(np.hstack((expression_data, methylation_data,assign_data)))\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "expression_data = torch.FloatTensor(expression_data).to(device)\n",
    "methylation_data = torch.FloatTensor(methylation_data).to(device)\n",
    "assign_data = torch.FloatTensor(assign_data).to(device)\n",
    "combined_data = torch.cat((expression_data,methylation_data,assign_data), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86d3896-e768-4b62-ae06-ddf92e142812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo VAE tiene 333499 parámetros entrenables.\n",
      "Epoch 100/10000 | Loss: 61025.046875\n",
      "El código tardó 2.19170 segundos en ejecutarse.\n",
      "Epoch 200/10000 | Loss: 60719.3671875\n",
      "El código tardó 2.34933 segundos en ejecutarse.\n",
      "Epoch 300/10000 | Loss: 60713.9609375\n",
      "El código tardó 2.10741 segundos en ejecutarse.\n",
      "Epoch 400/10000 | Loss: 60559.2421875\n",
      "El código tardó 1.97082 segundos en ejecutarse.\n",
      "Epoch 500/10000 | Loss: 60499.91015625\n",
      "El código tardó 2.30673 segundos en ejecutarse.\n",
      "Epoch 600/10000 | Loss: 60744.6640625\n",
      "El código tardó 2.33242 segundos en ejecutarse.\n",
      "Epoch 700/10000 | Loss: 60442.75390625\n",
      "El código tardó 2.20828 segundos en ejecutarse.\n",
      "Epoch 800/10000 | Loss: 60410.140625\n",
      "El código tardó 1.98779 segundos en ejecutarse.\n",
      "Epoch 900/10000 | Loss: 60362.08984375\n",
      "El código tardó 1.97725 segundos en ejecutarse.\n",
      "Epoch 1000/10000 | Loss: 60323.390625\n",
      "El código tardó 2.12631 segundos en ejecutarse.\n",
      "Epoch 1100/10000 | Loss: 60345.7734375\n",
      "El código tardó 2.32382 segundos en ejecutarse.\n",
      "Epoch 1200/10000 | Loss: 60353.5859375\n",
      "El código tardó 2.28590 segundos en ejecutarse.\n",
      "Epoch 1300/10000 | Loss: 60294.8203125\n",
      "El código tardó 2.34133 segundos en ejecutarse.\n",
      "Epoch 1400/10000 | Loss: 60264.984375\n",
      "El código tardó 1.31535 segundos en ejecutarse.\n",
      "Epoch 1500/10000 | Loss: 60271.40234375\n",
      "El código tardó 1.88487 segundos en ejecutarse.\n",
      "Epoch 1600/10000 | Loss: 60264.4765625\n",
      "El código tardó 2.36461 segundos en ejecutarse.\n",
      "Epoch 1700/10000 | Loss: 60264.6171875\n",
      "El código tardó 2.31699 segundos en ejecutarse.\n",
      "Epoch 1800/10000 | Loss: 60236.07421875\n",
      "El código tardó 2.30885 segundos en ejecutarse.\n",
      "Epoch 1900/10000 | Loss: 60265.296875\n",
      "El código tardó 2.33203 segundos en ejecutarse.\n",
      "Epoch 2000/10000 | Loss: 60261.28125\n",
      "El código tardó 1.95015 segundos en ejecutarse.\n",
      "Epoch 2100/10000 | Loss: 60219.41796875\n",
      "El código tardó 0.82769 segundos en ejecutarse.\n",
      "Epoch 2200/10000 | Loss: 60273.19921875\n",
      "El código tardó 1.05671 segundos en ejecutarse.\n",
      "Epoch 2300/10000 | Loss: 60211.71875\n",
      "El código tardó 1.00104 segundos en ejecutarse.\n",
      "Epoch 2400/10000 | Loss: 60208.68359375\n",
      "El código tardó 1.27731 segundos en ejecutarse.\n",
      "Epoch 2500/10000 | Loss: 60206.13671875\n",
      "El código tardó 2.04941 segundos en ejecutarse.\n",
      "Epoch 2600/10000 | Loss: 60175.3671875\n",
      "El código tardó 2.31208 segundos en ejecutarse.\n",
      "Epoch 2700/10000 | Loss: 60208.0859375\n",
      "El código tardó 2.28407 segundos en ejecutarse.\n",
      "Epoch 2800/10000 | Loss: 60218.36328125\n",
      "El código tardó 2.32272 segundos en ejecutarse.\n",
      "Epoch 2900/10000 | Loss: 60194.88671875\n",
      "El código tardó 2.31788 segundos en ejecutarse.\n",
      "Epoch 3000/10000 | Loss: 60184.546875\n",
      "El código tardó 2.32516 segundos en ejecutarse.\n",
      "Epoch 3100/10000 | Loss: 60193.08984375\n",
      "El código tardó 1.80589 segundos en ejecutarse.\n",
      "Epoch 3200/10000 | Loss: 60192.80859375\n",
      "El código tardó 2.29564 segundos en ejecutarse.\n",
      "Epoch 3300/10000 | Loss: 60212.34375\n",
      "El código tardó 1.86691 segundos en ejecutarse.\n",
      "Epoch 3400/10000 | Loss: 60161.87890625\n",
      "El código tardó 1.46647 segundos en ejecutarse.\n",
      "Epoch 3500/10000 | Loss: 60146.578125\n",
      "El código tardó 0.71082 segundos en ejecutarse.\n",
      "Epoch 3600/10000 | Loss: 60156.5546875\n",
      "El código tardó 1.19719 segundos en ejecutarse.\n",
      "Epoch 3700/10000 | Loss: 60173.6171875\n",
      "El código tardó 1.95468 segundos en ejecutarse.\n",
      "Epoch 3800/10000 | Loss: 60206.5234375\n",
      "El código tardó 2.03511 segundos en ejecutarse.\n",
      "Epoch 3900/10000 | Loss: 60139.328125\n",
      "El código tardó 2.03533 segundos en ejecutarse.\n",
      "Epoch 4000/10000 | Loss: 60166.38671875\n",
      "El código tardó 1.65023 segundos en ejecutarse.\n",
      "Epoch 4100/10000 | Loss: 60135.8359375\n",
      "El código tardó 2.24187 segundos en ejecutarse.\n",
      "Epoch 4200/10000 | Loss: 60210.7109375\n",
      "El código tardó 2.21339 segundos en ejecutarse.\n",
      "Epoch 4300/10000 | Loss: 60173.4296875\n",
      "El código tardó 2.10253 segundos en ejecutarse.\n",
      "Epoch 4400/10000 | Loss: 60107.83203125\n",
      "El código tardó 1.42879 segundos en ejecutarse.\n",
      "Epoch 4500/10000 | Loss: 60119.01953125\n",
      "El código tardó 1.85860 segundos en ejecutarse.\n",
      "Epoch 4600/10000 | Loss: 60150.23828125\n",
      "El código tardó 1.25055 segundos en ejecutarse.\n",
      "Epoch 4700/10000 | Loss: 60116.84375\n",
      "El código tardó 1.14638 segundos en ejecutarse.\n",
      "Epoch 4800/10000 | Loss: 60152.91015625\n",
      "El código tardó 1.10115 segundos en ejecutarse.\n",
      "Epoch 4900/10000 | Loss: 60145.90625\n",
      "El código tardó 1.15336 segundos en ejecutarse.\n",
      "Epoch 5000/10000 | Loss: 60108.703125\n",
      "El código tardó 1.22467 segundos en ejecutarse.\n",
      "Epoch 5100/10000 | Loss: 60123.80078125\n",
      "El código tardó 0.96310 segundos en ejecutarse.\n",
      "Epoch 5200/10000 | Loss: 60091.69921875\n",
      "El código tardó 0.75893 segundos en ejecutarse.\n",
      "Epoch 5300/10000 | Loss: 60141.40625\n",
      "El código tardó 0.73177 segundos en ejecutarse.\n",
      "Epoch 5400/10000 | Loss: 60136.3671875\n",
      "El código tardó 0.86368 segundos en ejecutarse.\n",
      "Epoch 5500/10000 | Loss: 60145.0\n",
      "El código tardó 1.44654 segundos en ejecutarse.\n",
      "Epoch 5600/10000 | Loss: 60122.84765625\n",
      "El código tardó 1.95962 segundos en ejecutarse.\n",
      "Epoch 5700/10000 | Loss: 60153.40625\n",
      "El código tardó 1.04955 segundos en ejecutarse.\n",
      "Epoch 5800/10000 | Loss: 60113.0078125\n",
      "El código tardó 0.78468 segundos en ejecutarse.\n",
      "Epoch 5900/10000 | Loss: 60128.59765625\n",
      "El código tardó 0.73511 segundos en ejecutarse.\n",
      "Epoch 6000/10000 | Loss: 60139.1953125\n",
      "El código tardó 0.95876 segundos en ejecutarse.\n",
      "Epoch 6100/10000 | Loss: 60104.3125\n",
      "El código tardó 0.95648 segundos en ejecutarse.\n",
      "Epoch 6200/10000 | Loss: 60077.5390625\n",
      "El código tardó 1.18909 segundos en ejecutarse.\n",
      "Epoch 6300/10000 | Loss: 60098.36328125\n",
      "El código tardó 1.21395 segundos en ejecutarse.\n",
      "Epoch 6400/10000 | Loss: 60106.5859375\n",
      "El código tardó 1.14246 segundos en ejecutarse.\n",
      "Epoch 6500/10000 | Loss: 60137.75\n",
      "El código tardó 1.13370 segundos en ejecutarse.\n",
      "Epoch 6600/10000 | Loss: 60102.21875\n",
      "El código tardó 1.14736 segundos en ejecutarse.\n",
      "Epoch 6700/10000 | Loss: 60099.80859375\n",
      "El código tardó 1.67943 segundos en ejecutarse.\n",
      "Epoch 6800/10000 | Loss: 60113.359375\n",
      "El código tardó 2.03246 segundos en ejecutarse.\n",
      "Epoch 6900/10000 | Loss: 60120.3359375\n",
      "El código tardó 1.89862 segundos en ejecutarse.\n",
      "Epoch 7000/10000 | Loss: 60084.7734375\n",
      "El código tardó 1.83664 segundos en ejecutarse.\n",
      "Epoch 7100/10000 | Loss: 60113.6953125\n",
      "El código tardó 1.94578 segundos en ejecutarse.\n",
      "Epoch 7200/10000 | Loss: 60094.921875\n",
      "El código tardó 1.99254 segundos en ejecutarse.\n",
      "Epoch 7300/10000 | Loss: 60081.97265625\n",
      "El código tardó 2.04239 segundos en ejecutarse.\n",
      "Epoch 7400/10000 | Loss: 60075.14453125\n",
      "El código tardó 1.89923 segundos en ejecutarse.\n",
      "Epoch 7500/10000 | Loss: 60100.9140625\n",
      "El código tardó 1.41002 segundos en ejecutarse.\n",
      "Epoch 7600/10000 | Loss: 60099.61328125\n",
      "El código tardó 1.23840 segundos en ejecutarse.\n",
      "Epoch 7700/10000 | Loss: 60053.515625\n",
      "El código tardó 1.25789 segundos en ejecutarse.\n",
      "Epoch 7800/10000 | Loss: 60119.37890625\n",
      "El código tardó 1.07015 segundos en ejecutarse.\n",
      "Epoch 7900/10000 | Loss: 60072.49609375\n",
      "El código tardó 1.06077 segundos en ejecutarse.\n",
      "Epoch 8000/10000 | Loss: 60066.5390625\n",
      "El código tardó 1.56933 segundos en ejecutarse.\n",
      "Epoch 8100/10000 | Loss: 60091.21875\n",
      "El código tardó 2.06288 segundos en ejecutarse.\n",
      "Epoch 8200/10000 | Loss: 60066.71875\n",
      "El código tardó 1.98051 segundos en ejecutarse.\n",
      "Epoch 8300/10000 | Loss: 60083.93359375\n",
      "El código tardó 1.54795 segundos en ejecutarse.\n",
      "Epoch 8400/10000 | Loss: 60079.328125\n",
      "El código tardó 1.21170 segundos en ejecutarse.\n",
      "Epoch 8500/10000 | Loss: 60078.2265625\n",
      "El código tardó 1.02907 segundos en ejecutarse.\n",
      "Epoch 8600/10000 | Loss: 60071.45703125\n",
      "El código tardó 1.29293 segundos en ejecutarse.\n",
      "Epoch 8700/10000 | Loss: 60099.8984375\n",
      "El código tardó 1.28410 segundos en ejecutarse.\n",
      "Epoch 8800/10000 | Loss: 60063.85546875\n",
      "El código tardó 1.49294 segundos en ejecutarse.\n",
      "Epoch 8900/10000 | Loss: 60095.328125\n",
      "El código tardó 2.34058 segundos en ejecutarse.\n",
      "Epoch 9000/10000 | Loss: 60077.125\n",
      "El código tardó 2.29804 segundos en ejecutarse.\n",
      "Epoch 9100/10000 | Loss: 60078.1015625\n",
      "El código tardó 1.81083 segundos en ejecutarse.\n",
      "Epoch 9200/10000 | Loss: 60072.46875\n",
      "El código tardó 2.08037 segundos en ejecutarse.\n",
      "Epoch 9300/10000 | Loss: 60090.1484375\n",
      "El código tardó 1.65024 segundos en ejecutarse.\n",
      "Epoch 9400/10000 | Loss: 60104.19921875\n",
      "El código tardó 2.15771 segundos en ejecutarse.\n",
      "Epoch 9500/10000 | Loss: 60078.234375\n",
      "El código tardó 2.29662 segundos en ejecutarse.\n",
      "Epoch 9600/10000 | Loss: 60051.25390625\n",
      "El código tardó 2.33352 segundos en ejecutarse.\n",
      "Epoch 9700/10000 | Loss: 60059.20703125\n",
      "El código tardó 2.35335 segundos en ejecutarse.\n",
      "Epoch 9800/10000 | Loss: 60093.31640625\n",
      "El código tardó 2.37314 segundos en ejecutarse.\n",
      "Epoch 9900/10000 | Loss: 60062.40625\n",
      "El código tardó 2.37629 segundos en ejecutarse.\n",
      "Epoch 10000/10000 | Loss: 60067.7578125\n",
      "El código tardó 2.37915 segundos en ejecutarse.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modelos y optimizadores\n",
    "vae = VAE(combined_data.shape[1], 256, 100).to(device)\n",
    "print(f'El modelo VAE tiene {count_parameters(vae)} parámetros entrenables.')\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=0.0002)\n",
    "\n",
    "# Función de pérdida para VAE\n",
    "def loss_vae(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.MSELoss(reduction='sum')(recon_x, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_divergence\n",
    "\n",
    "# Entrenamiento\n",
    "n_samples = combined_data.size(0)\n",
    "batch_size = 64\n",
    "n_epochs = 10000\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for idx in range(0, n_samples, batch_size):\n",
    "        real_data = combined_data[idx:idx+batch_size].to(device)\n",
    "\n",
    "        vae_optimizer.zero_grad()\n",
    "        \n",
    "        recon_data, mu, logvar = vae(real_data)\n",
    "        loss = loss_vae(recon_data, real_data, mu, logvar)\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {loss.item()}\")\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calcular la diferencia de tiempo\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"El código tardó {elapsed_time:.5f} segundos en ejecutarse.\")\n",
    "        start_time = time.time()\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae_model.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4389ef-27fa-4d77-b5cb-ac1662916c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7385,  1.0000, -0.9938, -0.9995, -0.9218,  1.0000,  1.0000, -1.0000,\n",
      "         -0.8767,  1.0000,  0.9660,  1.0000, -0.1753,  1.0000,  0.7080, -0.3841,\n",
      "          0.1776, -0.4876,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          0.0538,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.8933, -1.0000,\n",
      "          1.0000,  1.0000, -0.4481,  0.4203,  1.0000,  1.0000,  1.0000,  0.8649,\n",
      "          1.0000,  0.5784,  1.0000, -0.7655,  1.0000,  1.0000, -0.9990,  1.0000,\n",
      "         -0.5031,  1.0000,  1.0000,  1.0000,  0.5538, -1.0000,  0.9960, -0.7485,\n",
      "         -1.0000,  0.3896,  1.0000,  0.9990,  0.0706, -1.0000,  1.0000,  1.0000,\n",
      "          0.9841, -1.0000,  1.0000,  0.3478,  0.0649,  0.8190,  1.0000, -0.0369,\n",
      "         -0.1057,  1.0000, -0.7624,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5613, -1.0000,  1.0000,  1.0000,  1.0000,  0.7111,  1.0000,  0.3173,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, -0.7408, -1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  0.5149, -0.9971,  1.0000,  1.0000,  1.0000, -1.0000, -0.4587,\n",
      "          1.0000,  1.0000, -0.9895,  1.0000,  1.0000,  1.0000,  0.9469,  1.0000,\n",
      "         -0.5845,  0.1173,  1.0000, -0.9720,  1.0000, -0.1923, -0.8798,  1.0000,\n",
      "         -0.7197,  1.0000,  1.0000, -0.4912,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.7178,  1.0000,  0.8749,  0.0497,  0.0141,  0.9964,  0.1241,  0.4663,\n",
      "          0.0395,  0.6213,  0.0464,  0.8212,  0.3606,  0.5521,  0.4287,  0.4304,\n",
      "          0.3079,  0.2229,  0.4581,  0.9619,  0.2202,  0.2831,  0.9596,  0.9484,\n",
      "          0.4657,  0.0440,  0.0442,  0.1082,  0.0703,  0.0366,  0.0459,  0.7165,\n",
      "          0.0341,  0.1143,  0.9710,  0.3281,  0.0211,  0.0953,  0.8571,  0.0361,\n",
      "          0.0788,  0.0642,  0.0806,  0.6099,  0.0693,  0.0681,  0.0572,  0.0900,\n",
      "          0.7087,  0.0362,  0.0378, -0.0061,  0.8163,  0.8012,  0.0466,  0.1556,\n",
      "          0.8145,  0.0711,  0.8973,  0.0335,  0.1690,  0.0506,  0.6723,  0.6879,\n",
      "          0.7620,  0.4775,  0.0467,  0.7006,  0.7031,  0.5873,  0.0721,  0.0583,\n",
      "          0.9007,  0.1678,  0.0463,  0.0480,  0.9135,  0.0334,  0.9073,  0.9998,\n",
      "          0.9919,  0.9270,  0.0802,  0.0592,  0.9999,  0.0319,  0.5078,  0.0376,\n",
      "          0.8882,  1.0000,  0.9351,  0.7668,  0.7772,  0.8247,  0.0550,  0.7402,\n",
      "          0.0299,  0.7096,  0.0674,  0.1770,  0.0578,  0.1773,  0.0602,  0.1769,\n",
      "          0.0596,  0.0575,  0.0752,  0.9999,  0.0249,  0.9183,  0.8787,  0.0374,\n",
      "          0.0708,  0.8414,  0.0542,  0.0252,  0.1935,  0.0680,  0.0398,  0.0305,\n",
      "          0.1183,  0.9999,  0.5923,  0.1042,  0.0462,  0.0355,  0.0385,  0.6275,\n",
      "          0.9780,  0.9867,  0.5905,  0.8615,  0.5620,  0.4588,  0.8793,  0.9918,\n",
      "          0.6855,  0.4494,  0.9890,  0.8060,  0.6790,  0.8799,  0.4402,  0.8406,\n",
      "          0.0551,  0.0350,  0.1072,  0.6638,  0.1341,  0.1018,  0.0171,  0.9050,\n",
      "          0.0197,  0.1591,  0.1283,  0.0832,  0.9456,  0.6245,  0.0380,  0.2503,\n",
      "          0.0494,  0.0657,  0.0386,  0.0380,  0.9039,  0.1108,  0.0884,  0.8390,\n",
      "          0.0808,  0.0982,  0.8274,  0.9748,  0.0668,  0.9628,  0.0365,  0.0747,\n",
      "          0.8779,  0.0136,  0.9663,  0.9416,  0.7698,  0.0535,  0.0211,  0.0451,\n",
      "          0.4338,  0.2789,  0.0492,  0.0185,  0.0025,  0.0553,  0.1356,  0.0324,\n",
      "          0.0293,  0.9490,  0.1208,  0.8233,  0.0623,  0.0517,  0.0273,  0.0600,\n",
      "          0.0922,  0.0213,  0.8947,  0.0542,  0.1846,  0.0676,  0.5871,  0.9393,\n",
      "          0.8512,  0.0267,  0.4649,  0.0282,  0.0681,  0.0335,  0.7968,  0.0903,\n",
      "          0.9496,  0.0453,  0.1239,  0.0270,  0.0193,  0.0473,  0.0555,  0.0326,\n",
      "          0.0722,  0.0512,  0.9026,  0.1439,  0.0434,  0.0426,  0.0690,  0.9136,\n",
      "          0.0897,  0.0365,  0.1509,  0.0667,  0.0819,  0.0097,  0.7552,  0.0228,\n",
      "          0.8350,  0.7459,  0.0447,  0.8164,  0.0584,  0.0930,  0.9998,  0.9992,\n",
      "          0.1322,  0.0657,  0.3252,  0.9986,  0.6954,  0.9390, -0.0298,  0.0441,\n",
      "          0.0172,  0.0620,  0.0313,  0.0335,  0.0452,  0.6612,  0.0976,  0.0913,\n",
      "          0.0653,  0.5231,  0.4061,  0.8126,  0.0417,  0.8016,  0.0303,  0.8691,\n",
      "          0.8826,  0.0225,  0.7390,  0.6665,  0.0449,  0.0164,  0.7000,  0.7415,\n",
      "          0.5364,  0.1999,  0.7264,  0.3901,  0.8737,  0.8911,  0.8583,  0.6223,\n",
      "          0.0577,  0.7593,  0.9293,  0.0109,  0.0397,  0.0392,  0.9230,  1.0000,\n",
      "          0.7299,  0.6122,  0.8420,  0.8800,  0.0876,  0.1652,  0.0654,  0.0234,\n",
      "          0.8054,  0.1057,  0.1322,  0.2206,  0.8445,  0.0182,  0.0214,  0.0511,\n",
      "          0.0787,  0.9046,  0.0477,  1.0000,  0.8932,  0.0606,  0.1494,  0.4849,\n",
      "          0.9787,  0.8817,  0.0204,  0.2223,  0.7708,  0.2061,  0.0726,  0.1033,\n",
      "          0.0137,  0.0334,  0.0374,  0.0463,  0.1007,  0.5436,  0.0475,  0.7660,\n",
      "          0.0303, -0.0088,  0.0184,  0.1251,  0.7393,  0.7173,  0.0715,  0.8732,\n",
      "          0.4963,  0.0730,  0.0320,  0.0548,  0.5232,  0.5489,  0.8239,  0.2465,\n",
      "          0.8284,  0.9046,  0.0454,  0.7231,  0.0613,  0.1234,  0.0339,  0.9093,\n",
      "          0.9122,  0.9275,  0.0558,  0.0110,  0.5910,  0.8819,  0.0450,  0.0752,\n",
      "          0.9285,  0.0497,  1.0000]], device='cuda:0')\n",
      "tensor([[-4.3216e-01,  1.0000e+00, -5.9293e-01, -3.2897e-01, -4.4764e-01,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00, -9.9744e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  8.7320e-02, -9.9937e-01,  9.5164e-01,\n",
      "          4.0283e-02, -2.4792e-02,  9.8688e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.8178e-01,\n",
      "          1.0000e+00,  1.0000e+00, -9.3243e-01,  1.0000e+00,  1.0000e+00,\n",
      "         -5.8383e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00, -5.5551e-01,\n",
      "          9.9481e-01,  1.0000e+00,  1.0000e+00,  1.0000e+00,  4.2964e-02,\n",
      "          1.0000e+00,  9.9132e-01,  1.0000e+00, -9.6263e-01,  1.0000e+00,\n",
      "          1.0000e+00, -9.8310e-01,  1.0000e+00,  4.3717e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -6.1499e-01, -1.0000e+00,  6.8951e-01,\n",
      "         -9.7144e-01, -1.0000e+00,  5.8878e-01,  1.0000e+00,  1.0000e+00,\n",
      "         -9.9842e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00, -5.4664e-01,\n",
      "         -1.0000e+00,  1.0000e+00,  4.8237e-01,  8.8176e-01,  9.9630e-01,\n",
      "          1.0000e+00,  5.6036e-01, -9.2225e-01,  1.0000e+00,  8.1947e-01,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "          8.4708e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -9.3435e-01,  1.0000e+00, -6.0311e-02,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -9.8645e-01, -9.9998e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  9.2519e-02, -9.9339e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00, -8.6305e-01,  1.0000e+00,\n",
      "          1.0000e+00, -9.9956e-01,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          9.9914e-01,  1.0000e+00, -1.2500e-01, -1.0332e-01,  1.0000e+00,\n",
      "          3.5701e-01,  1.0000e+00, -4.5298e-01, -9.9595e-01,  1.0000e+00,\n",
      "         -8.5683e-01,  1.0000e+00,  1.0000e+00,  9.6049e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  6.7524e-01,  1.0000e+00,\n",
      "          9.9932e-01,  1.8296e-02,  2.8366e-02,  9.9880e-01,  1.3210e-01,\n",
      "          5.7858e-01,  3.2496e-02,  6.6072e-01,  2.0631e-02,  8.5458e-01,\n",
      "          4.8556e-01,  6.0570e-01,  6.1699e-01,  6.0227e-01,  3.2409e-01,\n",
      "          3.8558e-01,  5.6478e-01,  9.8857e-01,  2.9613e-01,  4.3303e-01,\n",
      "          9.8406e-01,  9.3682e-01,  3.2872e-01,  3.9201e-02,  4.3236e-02,\n",
      "          5.6161e-02,  3.5209e-02,  2.5279e-02,  4.5004e-02,  7.9055e-01,\n",
      "          1.3411e-02,  8.7804e-02,  9.5000e-01,  2.3584e-01,  2.6813e-02,\n",
      "          8.1087e-02,  8.2281e-01,  6.3066e-02,  5.4679e-03,  2.9701e-02,\n",
      "          6.2028e-02,  7.6406e-01,  4.9254e-02,  4.4580e-02,  2.3275e-02,\n",
      "          8.1942e-02,  7.8562e-01,  9.6916e-03,  2.8701e-02,  1.0637e-01,\n",
      "          8.2520e-01,  8.1624e-01,  3.2716e-02,  7.4886e-02,  8.4943e-01,\n",
      "          1.3717e-01,  8.6199e-01,  2.0935e-02,  1.0825e-01,  1.4532e-02,\n",
      "          6.4016e-01,  4.9799e-01,  6.4387e-01,  3.5724e-01,  4.0393e-02,\n",
      "          6.8534e-01,  5.8554e-01,  5.0149e-01,  4.8901e-02,  3.5118e-02,\n",
      "          8.5639e-01,  1.2956e-01,  2.6674e-02,  4.0292e-02,  9.4654e-01,\n",
      "          2.0674e-02,  8.4335e-01,  9.9755e-01,  9.6361e-01,  8.3239e-01,\n",
      "          3.7347e-02,  5.2882e-02,  9.9997e-01,  1.6853e-02,  5.8514e-01,\n",
      "          2.7227e-02,  8.2512e-01,  1.0000e+00,  9.5471e-01,  8.6261e-01,\n",
      "          7.3177e-01,  7.5519e-01, -2.2815e-03,  6.1185e-01,  1.2955e-02,\n",
      "          7.1283e-01,  8.8448e-02,  3.1803e-01,  1.1325e-01,  1.7425e-01,\n",
      "          9.3649e-02,  3.5046e-01,  6.6891e-02,  6.5049e-02,  5.5313e-02,\n",
      "          9.9793e-01,  2.7059e-02,  9.3556e-01,  7.7258e-01,  2.7488e-02,\n",
      "          4.3947e-02,  6.5371e-01,  5.2425e-02,  1.9667e-02,  1.9252e-01,\n",
      "          6.6443e-02,  2.6634e-02,  2.7827e-02,  5.8852e-02,  9.9832e-01,\n",
      "          4.9246e-01,  1.3725e-01,  2.2280e-02,  2.5799e-02,  2.9453e-02,\n",
      "          4.5032e-01,  9.9472e-01,  9.9454e-01,  4.2343e-01,  7.8432e-01,\n",
      "          4.2782e-01,  3.9129e-01,  7.1618e-01,  9.9709e-01,  4.6607e-01,\n",
      "          4.3586e-01,  9.9516e-01,  7.8824e-01,  7.2005e-01,  8.1315e-01,\n",
      "          3.8744e-01,  6.9729e-01,  3.3454e-02,  5.2884e-02,  1.3530e-01,\n",
      "          7.0261e-01,  1.2410e-01,  6.5765e-02,  3.5440e-02,  7.7559e-01,\n",
      "          2.3976e-02,  9.1948e-02,  9.0040e-02,  1.4546e-01,  8.7117e-01,\n",
      "          7.1180e-01,  8.3131e-03,  2.2746e-01,  1.8517e-02,  3.5209e-02,\n",
      "          2.7440e-02,  4.3209e-02,  7.9917e-01,  2.2645e-01,  1.1452e-01,\n",
      "          8.1468e-01,  5.9241e-02,  1.4537e-01,  8.1454e-01,  9.8631e-01,\n",
      "          7.7069e-02,  9.4282e-01,  4.6852e-02,  1.5262e-02,  7.7389e-01,\n",
      "          5.6918e-03,  9.7305e-01,  8.6044e-01,  7.7789e-01,  3.4629e-02,\n",
      "          1.4989e-02,  3.3578e-02,  5.4040e-01,  3.8077e-01,  2.7521e-02,\n",
      "         -1.3759e-02, -5.1350e-03,  2.9547e-02,  8.9092e-02,  3.7324e-02,\n",
      "          3.8164e-02,  8.9922e-01, -2.1287e-03,  9.0510e-01,  1.1515e-01,\n",
      "          4.4471e-02,  4.7401e-03,  3.7061e-02,  9.1832e-02,  1.2577e-02,\n",
      "          8.6114e-01,  9.3224e-02, -7.0118e-02,  5.5583e-02,  6.0003e-01,\n",
      "          7.6158e-01,  8.5228e-01,  1.4597e-02,  2.2609e-01,  2.1447e-02,\n",
      "          5.0860e-02,  1.6778e-02,  8.3092e-01,  5.8933e-02,  9.2489e-01,\n",
      "          2.6503e-02,  5.8984e-02,  3.8870e-02, -9.6420e-03, -2.0979e-05,\n",
      "          4.3889e-02,  6.2567e-03,  5.6723e-02,  2.4355e-02,  7.8946e-01,\n",
      "          8.6900e-02,  2.4704e-02,  3.4811e-02,  4.0670e-02,  8.1113e-01,\n",
      "          1.2489e-01,  6.2795e-02,  1.1826e-01,  5.0714e-02,  6.9438e-02,\n",
      "         -5.1824e-02,  7.5981e-01,  1.3014e-02,  7.0139e-01,  6.3559e-01,\n",
      "          2.7293e-02,  8.1743e-01,  5.2148e-02,  4.5729e-02,  9.9966e-01,\n",
      "          9.9907e-01,  6.2539e-02,  1.5587e-02,  2.4648e-01,  9.9289e-01,\n",
      "          6.0431e-01,  9.1248e-01,  1.0381e-02,  4.7784e-02,  1.7322e-02,\n",
      "          2.0670e-02, -6.0198e-02,  3.0299e-02,  4.8943e-02,  5.6400e-01,\n",
      "          5.7482e-02,  8.4315e-02,  5.2781e-01,  6.0952e-01,  2.2638e-01,\n",
      "          7.8088e-01,  3.5932e-02,  6.5075e-01,  7.2418e-03,  6.9144e-01,\n",
      "          7.3157e-01,  2.3267e-02,  6.0176e-01,  5.1808e-01,  2.3270e-02,\n",
      "          2.0199e-02,  7.7903e-01,  5.3445e-01,  5.1749e-01,  1.0875e-01,\n",
      "          7.7674e-01,  5.2299e-01,  8.4048e-01,  8.3806e-01,  8.9362e-01,\n",
      "          6.7630e-01,  2.9135e-02,  3.9066e-01,  9.0610e-01, -1.0414e-02,\n",
      "          2.5897e-02,  3.4178e-02,  7.6039e-01,  9.9978e-01,  7.7010e-01,\n",
      "          6.1584e-01,  7.7811e-01,  8.7479e-01,  3.2827e-02,  9.5371e-02,\n",
      "          2.8639e-02,  2.2382e-02,  6.9743e-01,  1.6198e-02,  6.6272e-02,\n",
      "          1.5086e-01,  6.9834e-01,  4.7528e-03,  1.2970e-02,  4.5207e-02,\n",
      "          6.2787e-02,  8.2197e-01,  4.4145e-02,  9.9932e-01,  8.8277e-01,\n",
      "          4.2728e-02,  1.0618e-01,  2.0928e-01,  9.7445e-01,  8.8128e-01,\n",
      "         -3.2103e-02,  3.0114e-01,  7.7698e-01,  2.8187e-01,  5.3050e-02,\n",
      "          6.7274e-02,  1.4871e-02,  5.5712e-02,  2.9195e-02,  4.4594e-02,\n",
      "          2.3003e-01,  2.9688e-01,  2.5650e-02,  6.2481e-01,  2.9638e-02,\n",
      "         -1.1849e-02,  1.2510e-02,  8.3296e-02,  5.9143e-01,  8.9007e-01,\n",
      "          4.0447e-02,  7.5973e-01,  6.5350e-01,  3.3417e-02,  3.4391e-02,\n",
      "          3.8358e-03,  2.1519e-01,  2.2138e-01,  7.6426e-01,  2.7906e-01,\n",
      "          8.3884e-01,  9.1539e-01,  5.1845e-02,  5.3143e-01,  3.5483e-02,\n",
      "          3.5223e-01,  3.9672e-02,  8.3446e-01,  8.3408e-01,  8.9355e-01,\n",
      "          2.4831e-02,  9.3919e-03,  6.3256e-01,  8.8013e-01,  4.6558e-02,\n",
      "          6.1867e-02,  8.8493e-01,  5.1442e-02,  1.0000e+00]], device='cuda:0')\n",
      "tensor([[-0.6774,  1.0000, -0.6592, -0.9742,  0.5480,  1.0000,  1.0000, -1.0000,\n",
      "         -0.6504,  1.0000,  0.8601,  1.0000,  0.0461,  0.9999,  0.5469, -0.3528,\n",
      "         -0.2571,  0.4541,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -0.4432,  1.0000,  1.0000,  0.9990,  1.0000,  1.0000,  0.7427, -1.0000,\n",
      "          1.0000,  1.0000, -0.0229,  0.9480,  1.0000,  1.0000,  1.0000,  0.8055,\n",
      "          1.0000,  0.7331,  1.0000, -0.5031,  1.0000,  1.0000,  0.1584,  1.0000,\n",
      "          0.0820,  1.0000,  1.0000,  1.0000,  0.4139, -1.0000,  0.9339, -0.8322,\n",
      "         -1.0000,  0.8088,  1.0000,  0.9984,  0.6388, -1.0000,  1.0000,  1.0000,\n",
      "          0.9116, -1.0000,  1.0000, -0.7070, -0.0290,  0.2017,  1.0000,  0.0595,\n",
      "          0.1287,  1.0000, -0.2419,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6105, -1.0000,  1.0000,  1.0000,  1.0000, -0.6744,  1.0000,  0.0173,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, -0.3830, -0.2953,  1.0000,  1.0000,\n",
      "          1.0000, -0.2780, -0.9936,  1.0000,  1.0000,  1.0000, -1.0000, -0.4565,\n",
      "          1.0000,  1.0000, -0.3015,  0.9999,  1.0000,  1.0000,  0.7169,  1.0000,\n",
      "         -0.5424, -0.3312,  1.0000, -0.7036,  1.0000,  0.5252, -0.2990,  1.0000,\n",
      "         -0.8911,  1.0000,  1.0000, -0.6486,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.5514,  1.0000,  0.9591,  0.0460,  0.0193,  0.9957,  0.1033,  0.2983,\n",
      "          0.0296,  0.4787,  0.0473,  0.8194,  0.2659,  0.2174,  0.3972,  0.4075,\n",
      "          0.2125,  0.2416,  0.3587,  0.9825,  0.2321,  0.2840,  0.9674,  0.9359,\n",
      "          0.2975,  0.0368,  0.0463,  0.0493,  0.0348,  0.0335,  0.0368,  0.8038,\n",
      "          0.0392,  0.0726,  0.9620,  0.2951,  0.0259,  0.0413,  0.8637,  0.0495,\n",
      "          0.0848,  0.0637,  0.0855,  0.6952,  0.0653,  0.0662,  0.0567,  0.0968,\n",
      "          0.7228,  0.0219,  0.0212,  0.0026,  0.6508,  0.8277,  0.0495,  0.1128,\n",
      "          0.8427,  0.0917,  0.8843,  0.0305,  0.1226,  0.0361,  0.7265,  0.6910,\n",
      "          0.6440,  0.2433,  0.0427,  0.7515,  0.7083,  0.5960,  0.0616,  0.0259,\n",
      "          0.8736,  0.1405,  0.0655,  0.0290,  0.9229,  0.0364,  0.8844,  0.9997,\n",
      "          0.9888,  0.9325,  0.0627,  0.0598,  0.9998,  0.0282,  0.5073,  0.0437,\n",
      "          0.8781,  1.0000,  0.9408,  0.8075,  0.7250,  0.8259,  0.0661,  0.7266,\n",
      "          0.0283,  0.7574,  0.0562,  0.1807,  0.0674,  0.1707,  0.0676,  0.1891,\n",
      "          0.0412,  0.0608,  0.0716,  0.9991,  0.0228,  0.9158,  0.8725,  0.0326,\n",
      "          0.0527,  0.8337,  0.0499,  0.0266,  0.1476,  0.0775,  0.0442,  0.0208,\n",
      "          0.1040,  0.9996,  0.6112,  0.1092,  0.0311,  0.0498,  0.0373,  0.5880,\n",
      "          0.9840,  0.9950,  0.5561,  0.7751,  0.5072,  0.4554,  0.7978,  0.9966,\n",
      "          0.6483,  0.4015,  0.9949,  0.7551,  0.5317,  0.8794,  0.1893,  0.8533,\n",
      "          0.0504,  0.0445,  0.0363,  0.7219,  0.0882,  0.0975,  0.0273,  0.8930,\n",
      "          0.0140,  0.0959,  0.1259,  0.0760,  0.9269,  0.6824,  0.0529,  0.2124,\n",
      "          0.0429,  0.0672,  0.0478,  0.0349,  0.8813,  0.1209,  0.0769,  0.8458,\n",
      "          0.0419,  0.0966,  0.8282,  0.9749,  0.0647,  0.9574,  0.0350,  0.0300,\n",
      "          0.8552,  0.0524,  0.9659,  0.9355,  0.8050,  0.0453,  0.0196,  0.0311,\n",
      "          0.3745,  0.2715,  0.0349,  0.0444,  0.0396,  0.0669,  0.0045,  0.0214,\n",
      "          0.0447,  0.8068,  0.0571,  0.8668,  0.0627,  0.0493,  0.0300,  0.0340,\n",
      "          0.1000,  0.0470,  0.8965,  0.0443,  0.1482,  0.0604,  0.5188,  0.8854,\n",
      "          0.8811,  0.0237,  0.3157,  0.0334,  0.0583,  0.0252,  0.8182,  0.0803,\n",
      "          0.9467,  0.0254,  0.1006,  0.0251,  0.0454,  0.0320,  0.0990,  0.0203,\n",
      "          0.0642,  0.0374,  0.9067,  0.0339,  0.0485,  0.0603,  0.0681,  0.8797,\n",
      "          0.0715,  0.0136,  0.1399,  0.0729,  0.0623,  0.0309,  0.7996,  0.0256,\n",
      "          0.8117,  0.7778,  0.0198,  0.8137,  0.0640,  0.0689,  0.9996,  0.9976,\n",
      "          0.1136,  0.0687,  0.2084,  0.9976,  0.7327,  0.9330,  0.0391,  0.0593,\n",
      "          0.0190,  0.0745,  0.0353,  0.0503,  0.0359,  0.6823,  0.0828,  0.0721,\n",
      "          0.4239,  0.4521,  0.2024,  0.8023,  0.0581,  0.7960,  0.0234,  0.8529,\n",
      "          0.9387,  0.0422,  0.8140,  0.8215,  0.0460,  0.0258,  0.3754,  0.8495,\n",
      "          0.7587,  0.1388,  0.8851,  0.6841,  0.9139,  0.9411,  0.9060,  0.6501,\n",
      "          0.0462,  0.8363,  0.9676,  0.0332,  0.0443,  0.0483,  0.8877,  0.9978,\n",
      "          0.7787,  0.7289,  0.8619,  0.8744,  0.0542,  0.1170,  0.0460,  0.0180,\n",
      "          0.7941,  0.0403,  0.0777,  0.2122,  0.8416,  0.0124,  0.0198,  0.0377,\n",
      "          0.0701,  0.9094,  0.0436,  0.9995,  0.8766,  0.0596,  0.0974,  0.2296,\n",
      "          0.9775,  0.8487,  0.0722,  0.1547,  0.7676,  0.1784,  0.0623,  0.1144,\n",
      "          0.0218,  0.0450,  0.0463,  0.0489,  0.1046,  0.3406,  0.0581,  0.7669,\n",
      "          0.0360,  0.0169,  0.0619,  0.1047,  0.5452,  0.8335,  0.0503,  0.8672,\n",
      "          0.5155,  0.0382,  0.0192,  0.0461,  0.3940,  0.3637,  0.8092,  0.3460,\n",
      "          0.8380,  0.9189,  0.0508,  0.7534,  0.0429,  0.1898,  0.0304,  0.8916,\n",
      "          0.8919,  0.9273,  0.0573,  0.0076,  0.6916,  0.8948,  0.0435,  0.0963,\n",
      "          0.9189,  0.0452,  1.0000]], device='cuda:0')\n",
      "tensor([[-7.9446e-01,  1.0000e+00, -9.9691e-01, -9.9902e-01,  9.9786e-01,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00, -9.4372e-01,  1.0000e+00,\n",
      "          9.9764e-01,  1.0000e+00, -2.0646e-01, -9.3893e-01,  5.0314e-01,\n",
      "         -6.9818e-02, -9.9895e-02,  4.5521e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00, -9.8872e-01,\n",
      "          1.0000e+00,  1.0000e+00, -9.9998e-01,  1.0000e+00,  1.0000e+00,\n",
      "         -1.9532e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  1.4407e-01,\n",
      "          9.0117e-01,  1.0000e+00,  1.0000e+00,  1.0000e+00, -3.7049e-01,\n",
      "          1.0000e+00,  8.6715e-01,  1.0000e+00, -9.9557e-01,  1.0000e+00,\n",
      "          1.0000e+00,  3.3547e-01,  1.0000e+00, -4.2921e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -9.4297e-01, -1.0000e+00,  9.7373e-01,\n",
      "         -9.8895e-01, -1.0000e+00,  1.4218e-01,  1.0000e+00, -9.7950e-01,\n",
      "         -4.5326e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00, -7.0818e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -4.4341e-01,  8.8942e-02,  5.3038e-01,\n",
      "          1.0000e+00,  4.4398e-02, -6.0122e-01,  1.0000e+00,  9.4682e-01,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "          4.2498e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -4.4240e-01,  1.0000e+00, -6.8829e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -4.5122e-01,  9.6350e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -6.7707e-01, -9.9294e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00, -6.2104e-01,  1.0000e+00,\n",
      "          1.0000e+00, -7.6227e-01,  9.9905e-01,  1.0000e+00,  1.0000e+00,\n",
      "          5.8667e-01,  1.0000e+00, -4.0917e-01, -9.9404e-01,  1.0000e+00,\n",
      "         -5.2447e-01,  1.0000e+00, -4.1788e-01, -9.1241e-01,  1.0000e+00,\n",
      "         -9.7119e-01,  1.0000e+00,  1.0000e+00,  9.2833e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00, -1.4878e-01,  1.0000e+00,\n",
      "          9.9390e-01,  4.8604e-02,  5.4000e-02,  9.8512e-01,  1.5089e-01,\n",
      "          3.7823e-01,  3.2028e-02,  5.5850e-01,  3.0107e-02,  8.0281e-01,\n",
      "          2.0721e-01,  4.1660e-01,  4.8480e-01,  3.8937e-01,  3.2124e-01,\n",
      "          2.4503e-01,  5.1180e-01,  9.7675e-01,  2.3017e-01,  4.0141e-01,\n",
      "          9.4638e-01,  9.2576e-01,  3.7715e-01,  4.0306e-02,  5.0630e-02,\n",
      "          8.7712e-02,  5.9182e-02,  4.6874e-02,  4.1428e-02,  7.5978e-01,\n",
      "          3.1665e-02,  1.0794e-01,  9.5476e-01,  4.6123e-01,  3.7745e-02,\n",
      "          1.2109e-01,  8.0902e-01,  5.6932e-02,  5.9227e-02,  9.6558e-02,\n",
      "          6.9001e-02,  8.0707e-01,  8.7054e-02,  4.6098e-02,  6.3020e-02,\n",
      "          1.0898e-01,  8.4995e-01,  2.3460e-02,  5.8599e-02,  1.8864e-01,\n",
      "          7.4655e-01,  8.7497e-01,  4.9091e-02,  1.0652e-01,  8.2684e-01,\n",
      "          1.2316e-01,  8.3564e-01,  3.6876e-02,  1.1594e-01,  4.1074e-02,\n",
      "          7.6602e-01,  6.6569e-01,  6.8461e-01,  2.9841e-01,  6.0598e-02,\n",
      "          8.9738e-01,  7.1284e-01,  6.7049e-01,  5.0481e-02,  2.9691e-02,\n",
      "          8.3804e-01,  8.9659e-02,  7.0967e-02,  3.3566e-02,  9.4044e-01,\n",
      "          5.5170e-02,  8.9700e-01,  9.9833e-01,  9.5525e-01,  8.5572e-01,\n",
      "          7.2215e-02,  5.3606e-02,  9.9404e-01,  3.3326e-02,  5.5711e-01,\n",
      "          3.3744e-02,  7.3887e-01,  1.0000e+00,  8.4816e-01,  7.8318e-01,\n",
      "          6.5114e-01,  8.0747e-01,  7.6462e-02,  7.6816e-01,  3.9774e-02,\n",
      "          7.4492e-01,  3.4753e-02,  1.6219e-01,  6.0774e-02,  1.7810e-01,\n",
      "          6.6035e-02,  9.6187e-02,  4.8673e-02,  5.6369e-02,  5.3800e-02,\n",
      "          9.9677e-01,  4.6455e-02,  8.9474e-01,  8.3107e-01,  4.3015e-02,\n",
      "          4.9629e-02,  8.7027e-01,  6.6460e-02,  2.5498e-02,  1.9202e-01,\n",
      "          5.6857e-02,  3.9749e-02,  2.7311e-02,  1.4184e-01,  9.7997e-01,\n",
      "          6.1287e-01,  8.6514e-02,  1.3483e-02,  4.9642e-02,  4.1471e-02,\n",
      "          5.4796e-01,  9.8582e-01,  9.9377e-01,  5.1224e-01,  8.8539e-01,\n",
      "          4.6632e-01,  4.5015e-01,  8.9486e-01,  9.9650e-01,  6.2809e-01,\n",
      "          4.8736e-01,  9.9288e-01,  8.6641e-01,  7.6217e-01,  8.6327e-01,\n",
      "          2.9664e-01,  8.4355e-01,  6.7092e-02,  4.6502e-02,  1.0195e-01,\n",
      "          7.8361e-01,  7.0455e-02,  9.9373e-02,  4.1229e-02,  8.9843e-01,\n",
      "          4.3869e-02,  6.6968e-02,  8.9071e-02,  1.0659e-01,  9.1341e-01,\n",
      "          6.7893e-01,  7.3967e-02,  1.6275e-01,  4.6983e-02,  4.5463e-02,\n",
      "          6.1280e-02,  3.3659e-02,  8.5527e-01,  1.5206e-01,  8.2478e-02,\n",
      "          8.5540e-01,  5.9341e-02,  1.1985e-01,  8.0129e-01,  9.6417e-01,\n",
      "          9.6530e-02,  9.5512e-01,  4.8167e-02,  4.8521e-02,  8.8968e-01,\n",
      "          3.8583e-02,  9.3758e-01,  8.8241e-01,  8.3957e-01,  3.9504e-02,\n",
      "          3.1997e-02,  3.6393e-02,  3.3240e-01,  3.0304e-01,  4.4179e-02,\n",
      "          4.2856e-02,  3.1472e-02,  8.4766e-02,  9.0559e-02,  3.0383e-02,\n",
      "          6.4729e-02,  8.8944e-01,  2.5569e-02,  8.1591e-01,  8.9651e-02,\n",
      "          3.0932e-02,  3.6509e-02,  3.8470e-02,  9.3594e-02,  4.0848e-02,\n",
      "          8.9000e-01,  5.2318e-02,  2.3848e-01,  5.6722e-02,  3.3890e-01,\n",
      "          8.7439e-01,  8.9682e-01,  2.9874e-02,  3.0279e-01,  5.2352e-02,\n",
      "          6.9026e-02,  2.6130e-02,  8.3600e-01,  7.0375e-02,  9.2143e-01,\n",
      "          2.4801e-02,  1.2727e-01,  3.3987e-02,  2.2790e-02,  1.8554e-02,\n",
      "          4.9551e-02,  2.4670e-02,  1.0407e-01,  7.6109e-02,  8.6560e-01,\n",
      "          1.0578e-01,  2.7014e-02,  6.8840e-02,  6.1113e-02,  8.4759e-01,\n",
      "          1.1608e-01,  5.6226e-02,  1.3432e-01,  7.1349e-02,  9.7624e-02,\n",
      "         -8.1529e-04,  8.0676e-01,  2.1184e-02,  7.9518e-01,  7.6555e-01,\n",
      "          3.8915e-02,  7.9651e-01,  6.4542e-02,  4.8443e-02,  9.9886e-01,\n",
      "          9.9528e-01,  6.2535e-02,  7.9855e-02,  2.9102e-01,  9.9610e-01,\n",
      "          7.2450e-01,  9.1195e-01,  2.3442e-02,  7.6050e-02,  2.9020e-02,\n",
      "          5.7781e-02,  4.1284e-02,  4.5438e-02,  4.8187e-02,  7.4063e-01,\n",
      "          5.8212e-02,  7.1422e-02,  1.4275e-01,  1.6751e-01,  2.1253e-01,\n",
      "          8.2574e-01,  5.5079e-02,  8.0508e-01,  4.0344e-02,  7.8766e-01,\n",
      "          9.3673e-01,  5.6092e-02,  8.3369e-01,  8.0967e-01,  3.9062e-02,\n",
      "          2.1931e-02,  5.0260e-01,  8.7110e-01,  8.0958e-01,  1.6864e-01,\n",
      "          9.2966e-01,  7.0756e-01,  8.8224e-01,  9.4034e-01,  9.4142e-01,\n",
      "          6.8121e-01,  4.2414e-02,  6.6968e-01,  9.6799e-01,  3.5284e-02,\n",
      "          2.7110e-02,  3.8797e-02,  8.1790e-01,  9.8621e-01,  8.1015e-01,\n",
      "          7.0916e-01,  8.3663e-01,  8.3450e-01,  3.8893e-02,  1.0015e-01,\n",
      "          4.0082e-02,  3.4478e-02,  7.8544e-01,  4.2626e-02,  7.3617e-02,\n",
      "          2.1401e-01,  8.4264e-01,  2.1399e-02,  2.6494e-02,  4.5612e-02,\n",
      "          8.5512e-02,  8.8997e-01,  6.6035e-02,  9.9223e-01,  8.8123e-01,\n",
      "          5.8146e-02,  7.6166e-02,  2.0563e-01,  9.7026e-01,  8.6226e-01,\n",
      "          8.6986e-02,  1.2261e-01,  8.1637e-01,  1.4374e-01,  7.0251e-02,\n",
      "          1.0187e-01,  3.0043e-02,  7.1198e-02,  4.6540e-02,  4.4597e-02,\n",
      "          1.3872e-01,  4.6729e-01,  5.3377e-02,  7.5123e-01,  3.5840e-02,\n",
      "          1.0803e-02,  3.9792e-02,  1.0710e-01,  5.7562e-01,  9.2582e-01,\n",
      "          6.0879e-02,  8.5244e-01,  5.5545e-01,  3.7050e-02,  2.9741e-02,\n",
      "          3.5758e-02,  3.7790e-01,  3.7884e-01,  8.0120e-01,  4.1057e-01,\n",
      "          7.5887e-01,  8.5844e-01,  5.8004e-02,  7.4115e-01,  3.5065e-02,\n",
      "          8.1456e-02,  4.1621e-02,  8.6798e-01,  8.7658e-01,  9.0717e-01,\n",
      "          5.2632e-02,  2.2477e-02,  6.9655e-01,  8.9673e-01,  5.0973e-02,\n",
      "          1.0812e-01,  8.5355e-01,  5.7129e-02,  1.0000e+00]], device='cuda:0')\n",
      "tensor([[-0.3292,  1.0000, -0.9981, -1.0000, -0.8737,  1.0000, -0.9893, -1.0000,\n",
      "          1.0000, -0.6911,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.7034,\n",
      "         -0.9056,  0.2211,  1.0000, -0.9987,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -0.2522, -0.9999,  1.0000,  0.9562,  1.0000,  1.0000,  0.4911,  1.0000,\n",
      "          0.7259,  1.0000, -0.8833, -0.5208,  1.0000, -0.3745, -0.9920, -0.1611,\n",
      "          1.0000,  0.5397,  1.0000, -0.9980,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.4675,  1.0000, -0.9997, -0.9958, -0.6545, -1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  0.5739,  1.0000,  0.9986, -0.8651, -1.0000,  1.0000,  1.0000,\n",
      "         -0.9094, -1.0000, -0.4171, -0.9993, -0.6490, -0.9698, -1.0000,  0.1620,\n",
      "          1.0000,  1.0000, -0.9990,  0.2422,  0.8689, -1.0000,  0.9888,  1.0000,\n",
      "          0.6239,  1.0000,  1.0000, -0.1174,  1.0000,  0.9993,  1.0000, -0.7258,\n",
      "          1.0000, -0.6786,  1.0000,  1.0000, -0.9032,  1.0000,  1.0000, -0.9987,\n",
      "         -0.7893,  1.0000,  1.0000,  1.0000, -0.2552, -1.0000, -1.0000, -0.3571,\n",
      "          1.0000,  1.0000, -0.7676,  0.9559,  1.0000,  0.3679, -0.0996,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.7515,  1.0000,  0.7743, -0.9999, -0.4144,  1.0000,  1.0000, -0.1646,\n",
      "         -0.7046, -0.0354,  0.5448,  0.0761,  0.8353,  0.5460,  0.1619,  0.0995,\n",
      "          0.0830,  0.2262,  0.0807,  0.6888,  0.1059,  0.2963,  0.1100,  0.2533,\n",
      "          0.1802,  0.1176,  0.0091,  0.1499,  0.1203,  0.1294, -0.0840,  0.1945,\n",
      "          0.2753,  0.0627,  0.0787,  0.9349,  0.1220,  0.0842,  0.0557,  0.7258,\n",
      "          0.0729,  0.1841,  0.3058,  0.9837,  0.0608,  0.9491,  0.1165,  0.8976,\n",
      "          0.9346,  0.1007,  0.9414,  0.7251,  0.1215,  0.1036,  0.1147,  0.9585,\n",
      "          0.6025,  0.0614,  0.8706,  0.8858,  0.4902,  0.7984,  0.0649,  0.9462,\n",
      "          0.7439,  0.1216,  0.7870,  0.0649,  0.2224,  0.1319,  0.1298,  0.2988,\n",
      "          0.8806,  0.2876,  0.0696,  0.6825,  0.0216,  0.1337,  0.1093,  0.0745,\n",
      "          0.1414,  0.1657,  0.9335,  0.0573,  0.9193,  0.0706,  0.9535,  0.9995,\n",
      "          0.9322,  0.7756,  0.9345,  0.0799,  0.7669,  0.0349,  0.5216,  0.8930,\n",
      "          0.9054,  0.8196,  1.0000,  0.7853,  0.5642,  0.1307,  0.1201,  0.1025,\n",
      "          0.0507,  0.0420,  0.0685,  0.1654,  0.0743,  0.9763,  0.9272,  0.9682,\n",
      "          0.0521,  0.0853,  0.0760,  0.8206,  0.0710,  0.1661,  0.1984,  0.0730,\n",
      "          0.1213,  0.9413,  0.0670,  0.0542,  0.9790,  0.0730,  0.0559,  0.0363,\n",
      "          0.2317,  0.6000,  0.1953,  0.2211,  0.0803,  0.0687,  0.0977,  0.9976,\n",
      "          0.3800,  0.4285,  0.5773,  0.6808,  0.6139,  0.9948,  0.6715,  0.4130,\n",
      "          0.9987,  0.4242,  0.4162,  0.6141,  0.5158,  0.1325,  0.0130,  0.3157,\n",
      "          0.0970,  0.0634,  0.9116,  0.1377,  0.1020,  0.1403,  0.0497,  0.9718,\n",
      "          0.0453,  0.9316,  0.1763,  0.0480,  0.3525,  0.1201,  0.8942,  0.2311,\n",
      "          0.0721,  0.1099,  0.0659,  0.0682,  0.9378,  0.1239,  0.8857,  0.0372,\n",
      "          0.0679,  0.9671,  0.1711,  0.2503,  0.1014,  0.3026,  0.0724,  0.1214,\n",
      "          0.9309,  0.9158,  0.4817,  0.3127,  0.0991,  0.0621,  0.0394,  0.0681,\n",
      "          0.2767,  0.0023,  0.0643,  0.9118,  0.8974,  0.9431,  0.0676,  0.0480,\n",
      "          0.0910,  0.4531,  0.9165,  0.9770,  0.0351,  0.0548,  0.0663,  0.0689,\n",
      "          0.1220,  0.8941,  0.1139,  0.0592,  0.9441,  0.0935,  0.4796,  0.9263,\n",
      "          0.2424,  0.0414,  0.3330,  0.0858,  0.0777,  0.0518,  0.7474,  0.1201,\n",
      "          0.1800,  0.0516,  0.2119,  0.0577,  0.9213,  0.0630,  0.1257,  0.0385,\n",
      "          0.1300,  0.0986,  0.1405,  0.1750,  0.0975,  0.1078,  0.0945,  0.1355,\n",
      "          0.9309,  0.0398,  0.1866,  0.1413,  0.1730,  0.8614,  0.5821,  0.0539,\n",
      "          0.2107,  0.1394,  0.0532,  0.7308,  0.9310,  0.0985,  0.8829,  0.9973,\n",
      "          0.1739,  0.1510,  0.1776,  0.9969,  0.1849,  0.1203,  0.8511,  0.0944,\n",
      "          0.0339,  0.1312,  0.8147,  0.0769,  0.0621,  0.0361,  0.1038,  0.1518,\n",
      "          0.9912,  0.6206,  0.9725,  0.1935,  0.1061,  0.1708,  0.0787,  0.1598,\n",
      "          0.9286,  0.9046,  0.8568,  0.6565,  0.0550,  0.0329,  0.3651,  0.9048,\n",
      "          0.8094,  0.2422,  1.0000,  0.6015,  0.8630,  0.9444,  0.8729,  0.6187,\n",
      "          0.0807,  0.9982,  1.0000,  0.8582,  0.0793,  0.0946,  0.9579,  0.9169,\n",
      "          0.0764,  0.3499,  0.1719,  0.7269,  0.0971,  0.2206,  0.0817,  0.0298,\n",
      "          0.0762,  0.8884,  0.1664,  0.3678,  0.1866,  0.0285,  0.0513,  0.0755,\n",
      "          0.9407,  0.1036,  0.1167,  0.9536,  0.8953,  0.1105,  0.1588,  0.9596,\n",
      "          0.9910,  0.8378,  0.8883,  0.1190,  0.7147,  0.9794,  0.1427,  0.1799,\n",
      "          0.0316,  0.9267,  0.0912,  0.0469,  0.1629,  0.4167,  0.9174,  0.1963,\n",
      "          0.1063,  0.8528,  0.8871,  0.2204,  0.9984,  0.6514,  0.1132,  0.9381,\n",
      "          0.3471,  0.0606,  0.0246,  0.8491,  0.9814,  0.0415,  0.1434,  0.2509,\n",
      "          0.7456,  0.8653,  0.0815,  0.1855,  0.0901,  0.2040,  0.0546,  0.1094,\n",
      "          0.9511,  0.2379,  0.1059,  0.0270,  0.1099,  0.8976,  0.0618,  0.1666,\n",
      "          0.2739,  0.0787,  1.0000]], device='cuda:0')\n",
      "tensor([[-0.5377,  1.0000, -0.9736, -0.9856,  0.7804,  1.0000,  1.0000, -1.0000,\n",
      "         -0.0105,  1.0000,  0.9739,  1.0000, -0.3839, -0.5773,  0.5351,  0.3148,\n",
      "         -0.5304, -0.4612,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          0.8139,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.8979, -1.0000,\n",
      "          1.0000,  1.0000, -0.7495, -0.6192,  1.0000,  1.0000,  1.0000,  0.5159,\n",
      "          1.0000,  0.6964,  1.0000, -0.9865,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -0.8272,  1.0000,  1.0000,  1.0000, -0.6384, -1.0000,  0.9514, -0.4320,\n",
      "         -1.0000,  0.1760,  1.0000,  0.3725, -0.9475, -1.0000,  1.0000,  1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -0.9401, -0.1402, -0.1881,  1.0000,  0.4612,\n",
      "          0.9253,  1.0000, -0.9791,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.3409, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.2031,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, -0.7937, -1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -0.9623, -0.7887,  1.0000,  1.0000,  1.0000, -1.0000, -0.1781,\n",
      "          1.0000,  1.0000, -0.8843,  1.0000,  1.0000,  1.0000,  0.8200,  1.0000,\n",
      "         -0.7078,  0.3760,  1.0000, -0.8786,  1.0000,  0.3108, -0.7497,  1.0000,\n",
      "          0.1363,  1.0000,  1.0000, -0.9325,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.5232,  1.0000,  0.6555,  0.0729,  0.0062,  0.9756,  0.1915,  0.4696,\n",
      "          0.0436,  0.7162,  0.0614,  0.8354,  0.3866,  0.4877,  0.4162,  0.4662,\n",
      "          0.3129,  0.2811,  0.3325,  0.9823,  0.2652,  0.2463,  0.9846,  0.9794,\n",
      "          0.4935,  0.0712,  0.0885,  0.1486,  0.0661,  0.0756,  0.0729,  0.8530,\n",
      "          0.0538,  0.1849,  0.9709,  0.4466,  0.0268,  0.1333,  0.9130,  0.0428,\n",
      "          0.0785,  0.1185,  0.1071,  0.8247,  0.1032,  0.0986,  0.0769,  0.1782,\n",
      "          0.8985,  0.0216,  0.1180,  0.3844,  0.8053,  0.8209,  0.0663,  0.1672,\n",
      "          0.7527,  0.1281,  0.9202,  0.0512,  0.1815,  0.0507,  0.8643,  0.8559,\n",
      "          0.9147,  0.4182,  0.0887,  0.7040,  0.8576,  0.8588,  0.0696,  0.0836,\n",
      "          0.9315,  0.1609,  0.0606,  0.0746,  0.8417,  0.0814,  0.9691,  0.9997,\n",
      "          0.9772,  0.9244,  0.1404,  0.0706,  0.9983,  0.0583,  0.5238,  0.0423,\n",
      "          0.9376,  1.0000,  0.9543,  0.7806,  0.6425,  0.9007,  0.0764,  0.8917,\n",
      "          0.0533,  0.8707,  0.0932,  0.2769,  0.1010,  0.1960,  0.1135,  0.2836,\n",
      "          0.0721,  0.0827,  0.0960,  0.9998,  0.0523,  0.9422,  0.9188,  0.0711,\n",
      "          0.1107,  0.9525,  0.0812,  0.0395,  0.3227,  0.1088,  0.0732,  0.0495,\n",
      "          0.1546,  0.9972,  0.7960,  0.1797,  0.0426,  0.0789,  0.0762,  0.5246,\n",
      "          0.9871,  0.9919,  0.5625,  0.9004,  0.5674,  0.4808,  0.8356,  0.9975,\n",
      "          0.6492,  0.6471,  0.9951,  0.8521,  0.7342,  0.9000,  0.5631,  0.8943,\n",
      "          0.0947,  0.0760,  0.1426,  0.8944,  0.1538,  0.2228,  0.0519,  0.9284,\n",
      "          0.0562,  0.1783,  0.1531,  0.1307,  0.9785,  0.8817,  0.0657,  0.3039,\n",
      "          0.0617,  0.0812,  0.0888,  0.0582,  0.9169,  0.2396,  0.1148,  0.9439,\n",
      "          0.1006,  0.2226,  0.8785,  0.9779,  0.1274,  0.9672,  0.0949,  0.1005,\n",
      "          0.9433,  0.0119,  0.8761,  0.9494,  0.9061,  0.0632,  0.0356,  0.0414,\n",
      "          0.3368,  0.3918,  0.0801,  0.0413, -0.0053,  0.0931,  0.1668,  0.0499,\n",
      "          0.0621,  0.9647,  0.2523,  0.9699,  0.1564,  0.0538,  0.0468,  0.0620,\n",
      "          0.1180,  0.0244,  0.9184,  0.0469,  0.3018,  0.0779,  0.6682,  0.8760,\n",
      "          0.9412,  0.0393,  0.5256,  0.0511,  0.0842,  0.0398,  0.7766,  0.1282,\n",
      "          0.9620,  0.0397,  0.1590,  0.0239,  0.0179,  0.0622,  0.1189,  0.0380,\n",
      "          0.1407,  0.0698,  0.9212,  0.2868,  0.0510,  0.1130,  0.0806,  0.9458,\n",
      "          0.1256,  0.0981,  0.1849,  0.1042,  0.1146,  0.0137,  0.7124,  0.0404,\n",
      "          0.8895,  0.8415,  0.0535,  0.8522,  0.0644,  0.1056,  0.9996,  0.9997,\n",
      "          0.1263,  0.1219,  0.1794,  0.9973,  0.8202,  0.9332,  0.0295,  0.1019,\n",
      "          0.0458,  0.0895,  0.0055,  0.0730,  0.0526,  0.8754,  0.0895,  0.1114,\n",
      "          0.5118,  0.5556,  0.3760,  0.9268,  0.0714,  0.8778,  0.0806,  0.8993,\n",
      "          0.8794,  0.0629,  0.7588,  0.6877,  0.0667,  0.0444,  0.6443,  0.7566,\n",
      "          0.6910,  0.2193,  0.7613,  0.5379,  0.8638,  0.8650,  0.8734,  0.7043,\n",
      "          0.0757,  0.4683,  0.9129,  0.0369,  0.0589,  0.0677,  0.9298,  0.9999,\n",
      "          0.9115,  0.7182,  0.9062,  0.8884,  0.1161,  0.2289,  0.0754,  0.0509,\n",
      "          0.8600,  0.1755,  0.1482,  0.2933,  0.9248,  0.0371,  0.0350,  0.1060,\n",
      "          0.1154,  0.9305,  0.0591,  0.9995,  0.7235,  0.0877,  0.1739,  0.4804,\n",
      "          0.9851,  0.8494,  0.0574,  0.2279,  0.7849,  0.3408,  0.0721,  0.1933,\n",
      "          0.0282,  0.0835,  0.0654,  0.0705,  0.1678,  0.5332,  0.0492,  0.8617,\n",
      "          0.0711, -0.0060,  0.0687,  0.1731,  0.7795,  0.8606,  0.0847,  0.9170,\n",
      "          0.6766,  0.0994,  0.0462,  0.0556,  0.5698,  0.5312,  0.9214,  0.6384,\n",
      "          0.6951,  0.7667,  0.0801,  0.8563,  0.0418,  0.5529,  0.0968,  0.9285,\n",
      "          0.9160,  0.9468,  0.0904,  0.0168,  0.8209,  0.8763,  0.0801,  0.0967,\n",
      "          0.9246,  0.0998,  1.0000]], device='cuda:0')\n",
      "tensor([[-9.8418e-01,  1.0000e+00,  5.3839e-01,  6.5975e-01, -5.8163e-02,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  5.2702e-01,  1.0000e+00,\n",
      "          9.7919e-01,  1.0000e+00,  4.0384e-01,  1.0431e-01,  9.4966e-01,\n",
      "         -4.6022e-01, -2.7915e-01, -8.3402e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.8162e-01,\n",
      "          1.0000e+00,  1.0000e+00,  9.9974e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0123e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  9.1265e-01,\n",
      "          9.9974e-01,  1.0000e+00,  1.0000e+00,  1.0000e+00,  6.2043e-01,\n",
      "          1.0000e+00,  3.2989e-02,  1.0000e+00,  7.3638e-01,  1.0000e+00,\n",
      "          1.0000e+00, -9.9938e-01,  1.0000e+00, -1.3850e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.9490e-01, -1.0000e+00,  9.9996e-01,\n",
      "         -5.5686e-02, -1.0000e+00,  9.7444e-01,  1.0000e+00,  1.0000e+00,\n",
      "         -9.4618e-02, -1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -9.6983e-01, -8.0980e-01, -4.5779e-01,\n",
      "          1.0000e+00,  2.9771e-01, -6.7648e-01,  1.0000e+00, -1.6102e-02,\n",
      "          1.0000e+00,  9.4703e-01, -9.9995e-01,  1.0000e+00, -1.0000e+00,\n",
      "          3.6566e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -8.6855e-01,  1.0000e+00, -5.4699e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -4.4079e-01,  9.7311e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  4.3749e-01, -5.8847e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00, -8.5501e-02,  1.0000e+00,\n",
      "          1.0000e+00, -2.2855e-01,  5.9364e-01,  1.0000e+00,  1.0000e+00,\n",
      "          9.6989e-01,  1.0000e+00, -3.2858e-02,  2.0934e-01,  1.0000e+00,\n",
      "          8.1546e-01,  1.0000e+00,  8.5570e-01, -5.3298e-01,  1.0000e+00,\n",
      "         -5.2345e-01,  1.0000e+00,  1.0000e+00, -8.4768e-01,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.2903e-01,  1.0000e+00,\n",
      "          3.0487e-01,  2.9279e-02,  4.2584e-02,  9.5823e-01,  1.1126e-01,\n",
      "          3.2017e-01,  8.1849e-03,  5.7320e-01,  1.8032e-02,  9.0203e-01,\n",
      "          2.0875e-01,  1.9886e-01,  3.5423e-01,  2.6941e-01,  1.5644e-01,\n",
      "          1.6689e-01,  4.0721e-01,  9.7820e-01,  1.4230e-01,  2.1212e-01,\n",
      "          9.8057e-01,  9.4607e-01,  1.5997e-01,  1.6273e-02,  3.1753e-02,\n",
      "          5.4513e-02,  4.2693e-02,  1.9624e-02,  1.8707e-02,  7.3042e-01,\n",
      "          8.0644e-03,  7.9307e-02,  9.6025e-01,  4.8357e-01,  3.0609e-02,\n",
      "          1.0100e-01,  7.8334e-01,  1.4381e-02,  4.6379e-02,  7.7083e-02,\n",
      "          3.6424e-02,  8.7284e-01,  3.8339e-02,  3.2793e-02,  1.9378e-02,\n",
      "          9.2771e-02,  7.8103e-01,  1.0129e-03, -3.2881e-03,  3.4608e-02,\n",
      "          7.5193e-01,  9.0047e-01,  2.3675e-02,  1.2637e-02,  8.4915e-01,\n",
      "          8.1742e-02,  8.1802e-01,  1.6734e-02,  5.6834e-02,  9.5237e-03,\n",
      "          5.1958e-01,  3.8309e-01,  5.7050e-01,  2.5091e-01,  3.0847e-02,\n",
      "          7.0873e-01,  4.7232e-01,  4.1280e-01,  2.5792e-02,  8.5273e-03,\n",
      "          8.3659e-01,  8.2263e-02,  5.0327e-02,  2.9373e-02,  9.2401e-01,\n",
      "          2.3099e-02,  7.8301e-01,  9.9848e-01,  9.7941e-01,  7.4852e-01,\n",
      "         -1.8898e-03,  5.9552e-02,  9.9728e-01,  6.9500e-03,  5.5195e-01,\n",
      "          3.1722e-02,  6.2448e-01,  9.9996e-01,  9.5069e-01,  7.5829e-01,\n",
      "          6.2095e-01,  6.8826e-01,  1.0769e-02,  4.6629e-01,  7.3221e-03,\n",
      "          5.5137e-01,  1.4670e-02,  1.2662e-01,  3.9027e-02,  8.9644e-02,\n",
      "         -2.3578e-03, -3.0487e-03,  3.3778e-02,  3.6031e-02,  4.2114e-02,\n",
      "          9.9439e-01,  2.2581e-02,  8.6643e-01,  6.4981e-01,  1.9867e-02,\n",
      "          1.5073e-02,  6.7805e-01,  5.0666e-02,  8.9620e-03,  8.6136e-02,\n",
      "          4.0628e-02,  2.7291e-02,  1.1252e-02,  5.8690e-02,  9.9628e-01,\n",
      "          2.1188e-01,  5.7880e-02,  2.9280e-03,  2.6991e-02,  1.6855e-02,\n",
      "          3.8465e-01,  9.8633e-01,  9.8916e-01,  3.6879e-01,  7.2729e-01,\n",
      "          4.1327e-01,  1.8326e-01,  8.2041e-01,  9.9261e-01,  4.6953e-01,\n",
      "          2.0155e-01,  9.9256e-01,  7.0341e-01,  6.8145e-01,  8.3620e-01,\n",
      "          3.0521e-01,  6.4740e-01,  4.2430e-02,  4.0580e-02,  4.1612e-03,\n",
      "          2.3622e-01,  7.4299e-02,  1.0000e-01,  3.1271e-02,  7.9967e-01,\n",
      "          1.2650e-02,  3.1922e-02,  9.4783e-02,  7.7052e-02,  7.4602e-01,\n",
      "          5.3433e-01,  1.0836e-01,  9.1390e-02,  2.8831e-02,  3.2606e-02,\n",
      "          1.5856e-02,  1.7746e-02,  8.1211e-01,  9.7136e-02, -5.6630e-02,\n",
      "          6.7607e-01,  5.2365e-02,  6.4361e-02,  7.1964e-01,  9.7971e-01,\n",
      "          2.9728e-02,  9.3389e-01,  4.1172e-02,  1.1867e-02,  8.5955e-01,\n",
      "          6.5837e-03,  8.7901e-01,  7.8186e-01,  6.5874e-01,  2.9532e-02,\n",
      "          1.6038e-02,  4.2853e-02,  2.3042e-01,  7.4315e-02,  1.3495e-02,\n",
      "          2.8673e-02,  1.9651e-02,  1.2152e-02,  6.6913e-02,  2.1544e-02,\n",
      "          2.7233e-02,  8.7731e-01,  1.6513e-01,  8.4857e-01,  2.5392e-02,\n",
      "          2.7825e-02,  1.6128e-02,  2.3795e-02,  8.0039e-02,  4.7037e-02,\n",
      "          8.6923e-01,  2.4950e-02, -1.9875e-02,  2.0179e-02,  5.3053e-01,\n",
      "          9.1266e-01,  7.4553e-01,  1.2062e-02,  1.7976e-01,  2.4071e-02,\n",
      "          4.9842e-02,  7.2838e-03,  8.9284e-01,  5.4505e-02,  9.1671e-01,\n",
      "          1.2872e-02,  5.1785e-02,  3.0285e-02,  2.8750e-02,  2.4411e-02,\n",
      "          1.8313e-03,  2.8259e-03,  3.3238e-02,  5.0661e-02,  7.9733e-01,\n",
      "          9.8161e-02,  2.5948e-02,  2.9685e-02,  4.7548e-02,  7.3413e-01,\n",
      "          4.6483e-04,  2.5989e-02,  7.1048e-02,  2.9612e-02,  4.6011e-02,\n",
      "          2.2524e-02,  8.0458e-01,  2.0590e-03,  6.8219e-01,  6.2949e-01,\n",
      "          1.0715e-02,  8.2940e-01,  4.3550e-02,  3.5036e-02,  9.9819e-01,\n",
      "          9.9897e-01,  1.3244e-01,  2.0848e-02,  1.1787e-01,  9.9578e-01,\n",
      "          5.2514e-01,  9.3141e-01,  3.0320e-02,  5.0943e-02,  1.6556e-02,\n",
      "          9.5228e-03,  7.1161e-03,  3.3841e-02,  3.0855e-02,  4.9882e-01,\n",
      "          6.8144e-02,  2.4498e-02,  3.3997e-01,  4.7863e-01,  2.5369e-01,\n",
      "          5.6738e-01,  2.0561e-02,  5.5156e-01,  2.2103e-02,  6.6414e-01,\n",
      "          9.5111e-01,  2.4650e-02,  8.4768e-01,  9.0318e-01,  2.2128e-02,\n",
      "          1.4919e-02,  4.8807e-01,  8.9339e-01,  7.9182e-01,  9.7842e-02,\n",
      "          9.3605e-01,  7.8876e-01,  9.1485e-01,  9.2230e-01,  9.6639e-01,\n",
      "          5.7456e-01,  2.5443e-02,  8.5296e-01,  9.7566e-01,  6.0726e-02,\n",
      "          2.9498e-02,  2.3811e-02,  7.2366e-01,  9.9886e-01,  7.2193e-01,\n",
      "          8.0819e-01,  6.7752e-01,  8.9693e-01,  1.2529e-02,  5.0814e-02,\n",
      "          2.9962e-02,  1.3721e-02,  7.0063e-01,  2.9499e-02,  3.9047e-02,\n",
      "          8.7295e-02,  6.7715e-01,  1.8442e-02,  8.4693e-03,  3.6774e-02,\n",
      "          3.4548e-02,  8.5663e-01,  4.7549e-02,  9.9353e-01,  9.0335e-01,\n",
      "          1.4264e-02,  1.1231e-01,  2.9905e-01,  9.6827e-01,  8.7144e-01,\n",
      "          5.3729e-02,  1.4012e-01,  8.7453e-01,  7.1794e-02,  3.2716e-02,\n",
      "          7.0296e-02,  2.6684e-03,  2.3677e-02,  2.1459e-02,  2.9104e-02,\n",
      "          1.1324e-01,  3.5477e-01,  2.6449e-02,  5.2603e-01,  1.1868e-02,\n",
      "          4.1158e-02,  9.9713e-02,  3.0286e-02,  7.1843e-01,  8.8322e-01,\n",
      "          4.8488e-02,  8.1912e-01,  5.6030e-01,  3.4163e-02,  1.9566e-02,\n",
      "          4.1473e-02,  1.2289e-01,  9.0098e-02,  6.4950e-01,  2.1843e-01,\n",
      "          8.7012e-01,  8.9915e-01,  2.0271e-02,  4.8327e-01,  3.0398e-02,\n",
      "          1.1576e-03,  3.5485e-02,  7.2571e-01,  8.7416e-01,  7.9107e-01,\n",
      "          3.5101e-02,  6.3821e-03,  4.5759e-01,  8.8005e-01,  2.6342e-02,\n",
      "          6.7772e-02,  8.4361e-01,  5.0678e-02,  1.0000e+00]], device='cuda:0')\n",
      "tensor([[-0.5113,  1.0000, -0.5497, -0.7929,  0.9473,  1.0000,  1.0000, -1.0000,\n",
      "         -0.9403,  1.0000,  0.9974,  1.0000, -0.0036, -0.8579,  0.7382,  0.4086,\n",
      "         -0.1107,  0.6897,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          0.9969,  1.0000,  1.0000,  0.3372,  1.0000,  1.0000,  0.3910, -1.0000,\n",
      "          1.0000,  1.0000,  0.2180,  0.9681,  1.0000,  1.0000,  1.0000,  0.1829,\n",
      "          1.0000,  0.7807,  1.0000, -0.5617,  1.0000,  1.0000,  0.5508,  1.0000,\n",
      "          0.0187,  1.0000,  1.0000,  1.0000, -0.5112, -1.0000,  0.9125, -0.4777,\n",
      "         -1.0000,  0.5912,  1.0000,  0.9979, -0.8276, -1.0000,  1.0000,  1.0000,\n",
      "         -0.9995, -1.0000,  1.0000,  0.1488,  0.0747,  0.2671,  1.0000, -0.0995,\n",
      "         -0.6811,  1.0000,  0.3676,  1.0000,  1.0000, -0.9997,  1.0000, -1.0000,\n",
      "          0.6229, -1.0000,  1.0000,  1.0000,  1.0000, -0.0179,  1.0000, -0.5832,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, -0.5065, -0.9361,  1.0000,  1.0000,\n",
      "          1.0000, -0.7304, -0.9027,  1.0000,  1.0000,  1.0000, -1.0000, -0.6874,\n",
      "          1.0000,  1.0000, -0.7228,  0.9991,  1.0000,  1.0000,  0.9790,  1.0000,\n",
      "         -0.4222, -0.1869,  1.0000,  0.1065,  1.0000,  0.3991, -0.8218,  1.0000,\n",
      "         -0.3811,  1.0000,  1.0000,  0.7704,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.0955,  1.0000,  0.9934,  0.0426,  0.0143,  0.9942,  0.1369,  0.2296,\n",
      "          0.0266,  0.3780,  0.0309,  0.7289,  0.2007,  0.1876,  0.4100,  0.3404,\n",
      "          0.3062,  0.2146,  0.1925,  0.9790,  0.1671,  0.3068,  0.9566,  0.9283,\n",
      "          0.2806,  0.0372,  0.0535,  0.0513,  0.0384,  0.0395,  0.0270,  0.7517,\n",
      "          0.0401,  0.0922,  0.9646,  0.2559,  0.0371,  0.0721,  0.8309,  0.0255,\n",
      "          0.0495,  0.0691,  0.0681,  0.7571,  0.0759,  0.0485,  0.0520,  0.1145,\n",
      "          0.7023,  0.0213,  0.0217,  0.1394,  0.5326,  0.7671,  0.0384,  0.0959,\n",
      "          0.7936,  0.1185,  0.8605,  0.0328,  0.1369,  0.0358,  0.7545,  0.6985,\n",
      "          0.7573,  0.2682,  0.0431,  0.6095,  0.7396,  0.6599,  0.0629,  0.0193,\n",
      "          0.8608,  0.1302,  0.0938,  0.0302,  0.9158,  0.0235,  0.8834,  0.9990,\n",
      "          0.9878,  0.8499,  0.0845,  0.0582,  0.9994,  0.0337,  0.5032,  0.0189,\n",
      "          0.8965,  1.0000,  0.8934,  0.7639,  0.5701,  0.8369,  0.0589,  0.7195,\n",
      "          0.0254,  0.7543,  0.0426,  0.1869,  0.0605,  0.1790,  0.0625,  0.1411,\n",
      "          0.0428,  0.0565,  0.0599,  0.9982,  0.0362,  0.9213,  0.8575,  0.0349,\n",
      "          0.0507,  0.8568,  0.0605,  0.0275,  0.1872,  0.0632,  0.0432,  0.0311,\n",
      "          0.1253,  0.9927,  0.5662,  0.1486,  0.0172,  0.0378,  0.0399,  0.4276,\n",
      "          0.9960,  0.9933,  0.3412,  0.8067,  0.2769,  0.3101,  0.8112,  0.9944,\n",
      "          0.4527,  0.4695,  0.9945,  0.7997,  0.6274,  0.8796,  0.0726,  0.8465,\n",
      "          0.0715,  0.0497,  0.0573,  0.7352,  0.0874,  0.1088,  0.0347,  0.8874,\n",
      "          0.0236,  0.0745,  0.1003,  0.0702,  0.9343,  0.7072,  0.0623,  0.1665,\n",
      "          0.0354,  0.0510,  0.0595,  0.0216,  0.8501,  0.1105,  0.0511,  0.8673,\n",
      "          0.0264,  0.1211,  0.8304,  0.9666,  0.0610,  0.9643,  0.0427,  0.0423,\n",
      "          0.8716,  0.0377,  0.9446,  0.9312,  0.8307,  0.0426,  0.0237,  0.0450,\n",
      "          0.3686,  0.3544,  0.0302,  0.0446,  0.0198,  0.0618,  0.0433,  0.0289,\n",
      "          0.0423,  0.8491,  0.1138,  0.8967,  0.0759,  0.0363,  0.0366,  0.0499,\n",
      "          0.0835,  0.0442,  0.8991,  0.0544,  0.1294,  0.0313,  0.4943,  0.8882,\n",
      "          0.9047,  0.0248,  0.1748,  0.0387,  0.0677,  0.0277,  0.7901,  0.0910,\n",
      "          0.9420,  0.0246,  0.0960,  0.0356,  0.0401,  0.0136,  0.0403,  0.0222,\n",
      "          0.0830,  0.0619,  0.8861,  0.1211,  0.0414,  0.0517,  0.0630,  0.8759,\n",
      "          0.0711,  0.0437,  0.0858,  0.0467,  0.0657,  0.0570,  0.7384,  0.0237,\n",
      "          0.8170,  0.7904,  0.0239,  0.7489,  0.0551,  0.0376,  0.9993,  0.9964,\n",
      "          0.0660,  0.0682,  0.1830,  0.9952,  0.7627,  0.9326,  0.0323,  0.0578,\n",
      "          0.0169,  0.0523,  0.0385,  0.0372,  0.0494,  0.6773,  0.0586,  0.0604,\n",
      "          0.4463,  0.5228,  0.1974,  0.8098,  0.0452,  0.8041,  0.0240,  0.8110,\n",
      "          0.9092,  0.0480,  0.8095,  0.6105,  0.0488,  0.0299,  0.3458,  0.8352,\n",
      "          0.7410,  0.1290,  0.8716,  0.6500,  0.8621,  0.9031,  0.8686,  0.6122,\n",
      "          0.0382,  0.7235,  0.9355,  0.0319,  0.0233,  0.0495,  0.8477,  0.9988,\n",
      "          0.8104,  0.4956,  0.8631,  0.7752,  0.0168,  0.0964,  0.0444,  0.0236,\n",
      "          0.8049,  0.1053,  0.0540,  0.2184,  0.8564,  0.0161,  0.0288,  0.0526,\n",
      "          0.0726,  0.9095,  0.0469,  0.9969,  0.8781,  0.0456,  0.0921,  0.2297,\n",
      "          0.9706,  0.8350,  0.0592,  0.1507,  0.6934,  0.1896,  0.0701,  0.0986,\n",
      "          0.0220,  0.0473,  0.0451,  0.0543,  0.1841,  0.2878,  0.0491,  0.7640,\n",
      "          0.0379,  0.0125,  0.0558,  0.1081,  0.4540,  0.8225,  0.0647,  0.8332,\n",
      "          0.3625,  0.0356,  0.0222,  0.0345,  0.0221,  0.1003,  0.8372,  0.2688,\n",
      "          0.7808,  0.8756,  0.0449,  0.7538,  0.0254,  0.2456,  0.0340,  0.8576,\n",
      "          0.8823,  0.9329,  0.0558,  0.0056,  0.7292,  0.8736,  0.0437,  0.0808,\n",
      "          0.9075,  0.0472,  1.0000]], device='cuda:0')\n",
      "tensor([[-3.4786e-01,  1.0000e+00,  1.0939e-01,  2.1072e-01,  5.4012e-01,\n",
      "          1.0000e+00, -8.9496e-01, -1.0000e+00,  1.0000e+00,  1.5329e-01,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -3.7966e-01, -3.0356e-01,  5.3271e-02,  1.0000e+00, -9.9995e-01,\n",
      "          1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00, -9.9647e-01,\n",
      "         -9.9999e-01,  1.0000e+00,  9.9991e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0212e-02,  1.0000e+00, -7.7458e-01,  1.0000e+00,  7.9966e-02,\n",
      "          9.9767e-01,  1.0000e+00,  9.9850e-01, -7.7338e-01,  8.7181e-01,\n",
      "          1.0000e+00,  6.8684e-02,  1.0000e+00,  1.4155e-02,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  8.2277e-01,  1.0000e+00,\n",
      "          2.1429e-01, -9.9998e-01,  6.6701e-01, -1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00, -1.0000e+00,  6.8342e-01,  1.0000e+00,  1.0000e+00,\n",
      "          4.6535e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.8749e-01,\n",
      "         -1.0000e+00, -3.2824e-01, -8.3288e-01,  8.1626e-02, -8.4425e-01,\n",
      "         -1.0000e+00,  5.1255e-01,  1.0000e+00,  1.0000e+00,  9.9552e-01,\n",
      "         -9.3222e-01,  9.9391e-01, -9.9993e-01, -2.0787e-01,  1.0000e+00,\n",
      "          8.8544e-02,  1.0000e+00,  1.0000e+00, -9.9395e-01,  1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -6.0049e-01,  1.0000e+00, -9.8775e-01,\n",
      "          1.0000e+00,  1.0000e+00, -3.2950e-01,  1.0000e+00,  1.0000e+00,\n",
      "         -9.9988e-01, -5.8033e-01,  1.0000e+00,  9.9998e-01,  1.0000e+00,\n",
      "         -4.0238e-01, -1.0000e+00, -1.0000e+00, -5.4812e-01,  1.0000e+00,\n",
      "          1.0000e+00,  2.3474e-01,  1.5704e-02,  1.0000e+00,  9.4033e-01,\n",
      "          9.0492e-01,  1.0000e+00,  1.0000e+00,  9.9992e-01,  1.0000e+00,\n",
      "          9.7445e-01,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -9.8608e-01,  1.0000e+00, -2.1929e-01,  8.9397e-02,  9.8973e-01,\n",
      "          1.0000e+00,  1.0000e+00, -7.5832e-01, -2.4719e-02, -6.7964e-01,\n",
      "          9.9833e-01,  1.9417e-02,  6.9041e-01,  6.4645e-02,  1.2690e-01,\n",
      "          5.3046e-01,  1.0414e-02,  7.5355e-01,  1.9526e-02,  8.7700e-01,\n",
      "          3.4132e-01,  2.7656e-01,  4.9000e-01,  3.7634e-01,  4.6744e-01,\n",
      "          3.0151e-01,  5.3919e-01,  2.9964e-01,  1.4261e-01,  4.6544e-01,\n",
      "          2.4712e-01,  4.5794e-01,  5.9735e-01,  1.7409e-02,  1.1149e-02,\n",
      "          7.4704e-01,  1.4025e-02,  5.7952e-03,  1.6861e-02,  8.9135e-01,\n",
      "          4.0452e-03,  5.4053e-02,  1.9547e-01,  9.6518e-01,  1.1024e-02,\n",
      "          8.8691e-01,  1.0138e-01,  8.1609e-01,  7.1198e-01,  3.7122e-02,\n",
      "          8.3910e-01,  9.1241e-01,  4.3065e-02,  2.9775e-02, -4.7107e-03,\n",
      "          8.3603e-01,  8.6873e-01, -3.0716e-04,  7.3512e-01,  8.9070e-01,\n",
      "          7.7394e-01,  8.8635e-01,  2.7124e-02,  8.1687e-01,  9.3010e-01,\n",
      "          1.0543e-01,  8.5730e-01,  2.7277e-02,  1.2958e-02, -5.9638e-04,\n",
      "          2.2662e-02,  7.0400e-02,  5.7064e-01,  1.7166e-01,  3.6069e-02,\n",
      "          8.6034e-01,  3.9174e-02,  9.9613e-02,  7.9930e-03, -2.2430e-02,\n",
      "          5.3767e-02,  2.6897e-02,  8.2890e-01,  2.5933e-02,  9.6344e-01,\n",
      "          4.0823e-02,  8.0552e-01,  9.9897e-01,  9.8774e-01,  9.0062e-01,\n",
      "          7.7510e-01,  6.3469e-02,  8.2906e-01,  1.3855e-02,  5.6974e-01,\n",
      "          7.4195e-01,  8.3490e-01,  9.3895e-01,  9.9985e-01,  8.9755e-01,\n",
      "          7.1389e-01,  6.4454e-02,  1.3627e-02,  1.1793e-01,  3.2778e-03,\n",
      "          2.2222e-01,  2.4984e-02,  2.0522e-01,  6.9570e-02,  8.9992e-01,\n",
      "          8.3321e-01,  9.3779e-01,  1.8992e-02,  2.7268e-02,  4.5034e-02,\n",
      "          8.9013e-01,  2.2819e-02,  2.0481e-01,  9.3687e-02,  2.8104e-02,\n",
      "          1.9684e-02,  7.3002e-01,  4.9317e-02,  2.5932e-03,  9.1546e-01,\n",
      "          3.7986e-02,  1.8498e-02,  1.3431e-02,  6.4998e-02,  6.9175e-01,\n",
      "          4.4755e-02, -1.6258e-02,  4.9161e-03,  2.1403e-02,  1.4991e-02,\n",
      "          9.8632e-01, -5.2154e-02,  2.4620e-01,  3.3691e-01,  3.3840e-01,\n",
      "          3.7484e-01,  9.7982e-01,  5.1078e-01,  3.1431e-01,  9.9063e-01,\n",
      "          3.1091e-01,  1.7992e-01,  3.5561e-01,  3.8998e-01,  6.5562e-02,\n",
      "          3.8805e-01,  7.7193e-02,  4.4478e-02,  4.2797e-02,  8.0308e-01,\n",
      "          2.8512e-01,  5.5559e-02,  4.4807e-02,  3.5734e-02,  8.5623e-01,\n",
      "          1.6770e-02,  7.1941e-01,  8.1074e-02,  7.7607e-02,  1.7247e-01,\n",
      "          1.7435e-01,  6.3210e-01,  1.2959e-01,  1.2112e-02,  3.5469e-02,\n",
      "          5.1605e-02,  1.5959e-02,  8.7279e-01,  2.6000e-01,  7.9191e-01,\n",
      "          9.2416e-02,  5.8916e-02,  9.2226e-01,  7.8770e-02,  2.5187e-01,\n",
      "          4.1663e-02,  1.1967e-01,  4.4069e-02,  1.4968e-02,  9.0645e-01,\n",
      "          7.0554e-01,  1.7379e-01,  6.5846e-02,  1.8260e-01,  2.8306e-02,\n",
      "          1.8947e-02,  1.0730e-02,  3.5400e-01,  3.3321e-01,  2.1463e-02,\n",
      "          7.1506e-01,  6.7625e-01,  7.3125e-01, -1.1510e-02,  2.3516e-02,\n",
      "          3.0717e-02, -6.1414e-02,  7.8420e-01,  9.0837e-01,  4.3420e-02,\n",
      "         -1.3355e-02,  1.3390e-02,  2.3317e-02,  1.0634e-01,  7.2926e-01,\n",
      "          7.9235e-02,  3.1389e-02,  8.6450e-01,  3.8841e-02,  5.2667e-01,\n",
      "          8.9355e-01,  1.2459e-03,  7.7065e-03,  3.1848e-01,  9.3050e-03,\n",
      "          3.8782e-02,  9.7649e-03,  9.2403e-01,  3.7931e-02,  7.9666e-02,\n",
      "         -2.7984e-03,  5.2394e-02,  1.9136e-02,  6.8274e-01, -1.0094e-02,\n",
      "         -1.0543e-02,  1.3800e-02,  2.0181e-02,  4.6536e-02,  1.0023e-01,\n",
      "          1.4079e-01,  1.4178e-03,  3.8470e-02,  4.0346e-02,  1.5290e-01,\n",
      "          7.8322e-01,  4.2330e-02,  4.3559e-02,  2.5537e-02,  3.2482e-02,\n",
      "          5.7674e-01,  8.6069e-01,  4.8376e-03,  2.0208e-02,  2.7672e-02,\n",
      "          1.3333e-02,  8.3283e-01,  8.3120e-01,  3.4414e-02,  9.1325e-01,\n",
      "          9.9863e-01,  4.5967e-02,  7.8401e-03,  2.9479e-01,  9.9795e-01,\n",
      "         -4.0755e-02,  2.9059e-02,  6.9309e-01,  7.9021e-02,  1.2056e-02,\n",
      "          8.5955e-03,  5.5442e-01,  4.1611e-02,  2.8252e-02,  4.6780e-02,\n",
      "          2.8356e-02,  1.5402e-02,  9.6756e-01,  5.7365e-01,  9.2257e-01,\n",
      "          1.3688e-01,  2.2032e-02,  7.3559e-02,  2.2477e-02,  7.2059e-02,\n",
      "          9.3792e-01,  8.1244e-01,  8.3037e-01,  5.9268e-01,  8.5440e-03,\n",
      "          2.3546e-02,  6.1132e-01,  8.7837e-01,  8.8101e-01,  1.0691e-01,\n",
      "          9.9872e-01,  6.5156e-01,  9.2549e-01,  9.2513e-01,  9.6852e-01,\n",
      "          7.3674e-01,  2.4042e-02,  9.8223e-01,  9.9962e-01,  7.0829e-01,\n",
      "          1.0879e-02,  8.3198e-03,  8.3055e-01,  8.0471e-01,  1.7194e-01,\n",
      "          8.0902e-01,  6.6789e-02,  9.1030e-01,  1.5240e-02,  1.4723e-02,\n",
      "          9.7887e-03,  7.2307e-03,  5.5255e-02,  7.3622e-01,  1.7783e-03,\n",
      "          3.5540e-02,  5.4156e-02,  1.4220e-02,  3.4123e-03,  3.3781e-02,\n",
      "          8.5799e-01,  6.2612e-02,  3.8760e-02,  7.5056e-01,  9.5367e-01,\n",
      "          2.3371e-02,  1.5310e-02,  9.2197e-01,  9.8317e-01,  9.1283e-01,\n",
      "          6.4101e-01,  1.8019e-01,  9.1210e-01,  9.4868e-01,  3.5997e-02,\n",
      "          5.7666e-02,  7.2290e-03,  8.3286e-01,  1.2118e-02,  3.6985e-02,\n",
      "          1.4912e-01,  4.0142e-01,  5.9116e-01,  6.8676e-02, -1.1547e-03,\n",
      "          6.5717e-01,  6.9973e-01,  3.1562e-02,  9.9327e-01,  9.7753e-01,\n",
      "          6.0440e-03,  8.3562e-01,  6.0300e-01,  3.2895e-02,  1.5612e-02,\n",
      "          6.3031e-01,  9.6768e-01,  4.7595e-01,  1.0819e-01,  6.4510e-01,\n",
      "          8.7552e-01,  9.2593e-01,  2.1627e-02,  5.5757e-02,  1.5702e-02,\n",
      "          2.7001e-01,  3.7200e-02,  1.6072e-01,  8.1949e-01,  3.1365e-02,\n",
      "          3.4339e-02, -2.1465e-04,  8.8380e-02,  9.1354e-01,  3.1346e-02,\n",
      "          8.3073e-02,  1.9867e-01,  5.5319e-02,  1.0000e+00]], device='cuda:0')\n",
      "tensor([[-0.9873,  1.0000, -0.4476,  0.3028, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "         -0.2504,  1.0000,  0.9998,  1.0000,  0.2525, -0.1811,  0.0299, -0.5682,\n",
      "         -0.3389, -0.8405,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -0.9637,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.1811, -1.0000,\n",
      "          1.0000,  1.0000,  0.0940,  0.9991,  1.0000,  1.0000,  1.0000, -0.2474,\n",
      "          1.0000,  0.9537,  1.0000, -0.9952,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -0.5186,  1.0000,  1.0000,  1.0000, -0.6566, -1.0000,  0.5502, -0.5369,\n",
      "         -1.0000,  0.7571,  1.0000,  1.0000, -0.8895, -1.0000,  1.0000,  1.0000,\n",
      "          0.7802, -1.0000,  1.0000, -0.9016, -0.0055,  0.9050,  1.0000,  0.2127,\n",
      "         -0.9992,  1.0000, -0.9924,  1.0000,  0.9997, -1.0000,  1.0000, -1.0000,\n",
      "          0.5220, -1.0000,  1.0000,  1.0000,  1.0000,  0.9664,  1.0000, -0.6340,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, -0.9433, -1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -0.2686, -0.9644,  1.0000,  1.0000,  1.0000, -1.0000, -0.9485,\n",
      "          1.0000,  1.0000, -0.9749,  0.9999,  1.0000,  1.0000,  0.8224,  1.0000,\n",
      "         -0.0916,  0.7576,  1.0000,  0.6818,  1.0000,  0.1363, -0.9322,  1.0000,\n",
      "         -0.4879,  1.0000,  1.0000,  0.1746,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.1074,  1.0000, -0.0568,  0.0561,  0.0126,  0.9318,  0.1402,  0.5189,\n",
      "          0.0292,  0.6955,  0.0690,  0.8662,  0.3652,  0.6521,  0.3919,  0.4963,\n",
      "          0.2241,  0.3072,  0.6707,  0.9803,  0.3326,  0.2250,  0.9826,  0.9811,\n",
      "          0.0695,  0.0536,  0.0548,  0.1408,  0.0640,  0.0443,  0.0631,  0.8766,\n",
      "          0.0507,  0.1081,  0.9759,  0.3500,  0.0338,  0.1919,  0.9192,  0.1017,\n",
      "          0.1204,  0.0626,  0.1491,  0.8635,  0.0803,  0.1037,  0.0578,  0.1640,\n",
      "          0.9272,  0.0159,  0.0996,  0.3834,  0.9474,  0.8980,  0.0563,  0.1634,\n",
      "          0.6396,  0.1372,  0.8381,  0.0645,  0.0985,  0.0179,  0.7627,  0.8060,\n",
      "          0.7011,  0.5251,  0.0648,  0.7303,  0.8111,  0.7215,  0.0819,  0.0807,\n",
      "          0.9656,  0.1271,  0.1809,  0.0641,  0.8644,  0.0332,  0.9438,  0.9997,\n",
      "          0.9914,  0.9247,  0.0835,  0.0519,  0.9999,  0.0378,  0.4511,  0.0628,\n",
      "          0.9111,  1.0000,  0.9447,  0.7334,  0.5925,  0.8875,  0.0673,  0.8376,\n",
      "          0.0239,  0.8341,  0.1137,  0.2748,  0.0985,  0.2740,  0.1061,  0.2837,\n",
      "          0.1080,  0.0967,  0.0948,  0.9989,  0.0210,  0.9419,  0.9090,  0.0589,\n",
      "          0.1125,  0.8450,  0.0579,  0.0358,  0.2919,  0.0733,  0.0673,  0.0313,\n",
      "          0.1067,  0.9990,  0.7654,  0.1425,  0.0494,  0.0553,  0.0365,  0.5238,\n",
      "          0.9912,  0.9941,  0.5080,  0.8155,  0.5637,  0.2637,  0.8158,  0.9961,\n",
      "          0.4929,  0.6595,  0.9958,  0.7933,  0.6477,  0.8939,  0.5941,  0.8573,\n",
      "          0.0756,  0.0735,  0.1591,  0.8507,  0.1606,  0.1365,  0.0309,  0.9279,\n",
      "          0.0483,  0.2310,  0.1143,  0.1028,  0.9478,  0.8404,  0.0572,  0.1907,\n",
      "          0.0411,  0.0917,  0.0600,  0.0320,  0.9411,  0.1702,  0.0865,  0.8946,\n",
      "          0.1220,  0.2117,  0.9089,  0.9943,  0.1026,  0.9623,  0.0596,  0.0952,\n",
      "          0.8987,  0.0766,  0.6421,  0.9490,  0.8765,  0.0431,  0.0173,  0.0546,\n",
      "          0.3249,  0.1980,  0.0720,  0.0591,  0.0302,  0.1254,  0.1920,  0.0309,\n",
      "          0.0153,  0.9794,  0.1540,  0.8681,  0.1597,  0.0523,  0.0535,  0.0533,\n",
      "          0.1517,  0.0636,  0.9176,  0.0881,  0.3972,  0.0437,  0.6320,  0.9273,\n",
      "          0.8987,  0.0287,  0.6763,  0.0398,  0.0913,  0.0396,  0.8979,  0.1722,\n",
      "          0.9606,  0.0637,  0.1196,  0.0430,  0.0679,  0.0304,  0.0864,  0.0406,\n",
      "          0.0422,  0.0541,  0.9291,  0.0060,  0.0394,  0.0749,  0.0910,  0.9260,\n",
      "          0.1356,  0.0538,  0.0989,  0.0898,  0.0154,  0.0426,  0.8294,  0.0204,\n",
      "          0.8756,  0.8279,  0.0743,  0.8911,  0.1005,  0.1250,  0.9998,  0.9999,\n",
      "          0.1564,  0.0930,  0.3410,  0.9968,  0.7822,  0.9453,  0.0129,  0.0828,\n",
      "          0.0142,  0.0764,  0.0170,  0.0930,  0.0645,  0.7654,  0.0919,  0.0428,\n",
      "          0.3619,  0.5233,  0.4636,  0.8848,  0.0603,  0.7911,  0.0331,  0.8897,\n",
      "          0.8416,  0.0519,  0.7408,  0.3732,  0.0643,  0.0339,  0.8647,  0.8193,\n",
      "          0.6891,  0.1209,  0.6965,  0.4019,  0.8680,  0.8759,  0.8193,  0.7223,\n",
      "          0.0724,  0.9020,  0.9399,  0.0531,  0.0503,  0.0609,  0.9275,  1.0000,\n",
      "          0.8680,  0.7566,  0.8664,  0.9118,  0.0660,  0.1314,  0.0883,  0.0087,\n",
      "          0.8388,  0.1247,  0.1014,  0.2637,  0.8625,  0.0141,  0.0113,  0.0953,\n",
      "          0.1328,  0.9371,  0.0278,  1.0000,  0.9140,  0.0759,  0.1665,  0.6210,\n",
      "          0.9701,  0.8613,  0.0112,  0.2985,  0.8190,  0.3466,  0.0584,  0.1142,\n",
      "          0.0147,  0.1179,  0.0523,  0.0743,  0.2436,  0.3203,  0.1267,  0.8318,\n",
      "          0.0509,  0.0430,  0.0970,  0.1371,  0.8813,  0.8666,  0.0927,  0.8905,\n",
      "          0.7418,  0.0804,  0.0309,  0.0328,  0.7768,  0.7586,  0.8839,  0.7305,\n",
      "          0.9098,  0.8799,  0.0600,  0.7942,  0.0444,  0.3245,  0.0745,  0.8915,\n",
      "          0.9252,  0.9158,  0.0896, -0.0040,  0.7497,  0.8802,  0.0682,  0.1084,\n",
      "          0.8689,  0.0934,  1.0000]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo SVC\u001b[39;00m\n\u001b[1;32m     37\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Predicciones\u001b[39;00m\n\u001b[1;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m~/PyEnv/PB/lib/python3.11/site-packages/sklearn/svm/_base.py:747\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    750\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "vae = VAE(combined_data.shape[1], 256, 100).to(device)\n",
    "vae.load_state_dict(torch.load('vae_model.pth'))\n",
    "vae.eval()\n",
    "\n",
    "# Función para reparametrización (para generar muestras del espacio latente)\n",
    "\n",
    "# Generar datos de expresión sintética\n",
    "expressions_list = []\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        # Puedes elegir mu y logvar como quieras, pero una opción común es usar 0 y 1\n",
    "        mu = torch.zeros(1, 100).to(device)\n",
    "        logvar = torch.ones(1, 100).to(device)\n",
    "        \n",
    "        z = vae.reparameterize(mu, logvar).to(device)\n",
    "        synthetic_expression = vae.decoder(z).to(device)\n",
    "        print(synthetic_expression)\n",
    "        expressions_list.append(synthetic_expression.detach().cpu().numpy().squeeze())\n",
    "\n",
    "X = pd.DataFrame(expressions_list)\n",
    "#X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = X.iloc[:, -1].values.astype(int)  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "X = X.iloc[:, :-1]\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54d7865-fa40-496c-944d-a230c3a87b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      2370\n",
      "           2       1.00      1.00      1.00       630\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprLOW.csv')\n",
    "meth_data = pd.read_csv('methylLOW.csv')\n",
    "assig_data = pd.read_csv('assignLOW.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = pd.concat([expr_data, meth_data], axis=1)\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4684f20f-50aa-4729-b7f8-fa924b20592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     23934\n",
      "           2       1.00      1.00      1.00      6066\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos los datos\n",
    "expr_data = pd.read_csv('exprMID.csv')\n",
    "meth_data = pd.read_csv('methylMID.csv')\n",
    "assig_data = pd.read_csv('assignMID.csv')\n",
    "expr_data = expr_data.iloc[:, 1:]\n",
    "meth_data = meth_data.iloc[:, 1:]\n",
    "assig_data = assig_data.iloc[:, 2:]\n",
    "\n",
    "# Convertir todas las columnas a tipo float\n",
    "expr_data = expr_data.apply(pd.to_numeric, errors='coerce')\n",
    "meth_data = meth_data.apply(pd.to_numeric, errors='coerce')\n",
    "assig_data = assig_data.apply(pd.to_numeric, errors='coerce')\n",
    "# Lidiar con valores NaN (si los hay). Pone 0(CAMBIAR)\n",
    "expr_data.fillna(0, inplace=True)\n",
    "meth_data.fillna(0, inplace=True)\n",
    "assig_data.fillna(0, inplace=True)\n",
    "# Aseguramos que las dimensiones coincidan\n",
    "assert expr_data.shape[0] == meth_data.shape[0] == assig_data.shape[0], \"Las dimensiones de los archivos no coinciden\"\n",
    "\n",
    "# Combinamos los datos de expresión génica y metilación\n",
    "X = meth_data\n",
    "y = assig_data.iloc[:, 0].values  # Suponiendo que la asignación está en la primera columna\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8b7b5-c373-4a86-9b69-eb5ddebd998a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
